apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pl04-webdl-dp-fs-classif-decisiontree-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.11, pipelines.kubeflow.org/pipeline_compilation_time: '2022-03-17T16:09:29.891037',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Pipeline: Download data,
      data processing", "inputs": [{"name": "url"}], "name": "Pl04 webdl dp fs classif
      decisiontree"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.11}
spec:
  entrypoint: pl04-webdl-dp-fs-classif-decisiontree
  templates:
  - name: create-model-inputs
    container:
      args: [--file, /tmp/inputs/file/data, --output-json, /tmp/outputs/output_json/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef create_model_inputs(file_path,\n                            output_json):\n\
        \    \"\"\"Impute unknown values (nan).\n        Input: CSV.\n        Output:\
        \ CSV.\n\n    Args:\n        file_path: A string containing path to input\
        \ data.        \n    \"\"\"\n\n    import json \n    from json import JSONEncoder\n\
        \    import numpy as np\n    import pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n\
        \        def default(self, obj):\n            if isinstance(obj, np.ndarray):\n\
        \                return obj.tolist()\n            return JSONEncoder.default(self,\
        \ obj)\n\n    # Read in CSV\n    features_to_include = [\"age_'30-59'\", \"\
        age_'<30'\", \"age_'>=60'\", 'sex_?', 'sex_female',\n       'sex_male', 'histologic-type_?',\
        \ 'histologic-type_adeno',\n       'histologic-type_anaplastic', 'histologic-type_epidermoid',\n\
        \       'degree-of-diffe_?', 'degree-of-diffe_fairly', 'degree-of-diffe_poorly',\n\
        \       'degree-of-diffe_well', 'bone_no', 'bone_yes', 'bone-marrow_no',\n\
        \       'bone-marrow_yes', 'lung_no', 'lung_yes', 'pleura_no', 'pleura_yes',\n\
        \       'peritoneum_no', 'peritoneum_yes', 'liver_no', 'liver_yes', 'brain_no',\n\
        \       'brain_yes', 'skin_?', 'skin_no', 'skin_yes', 'neck_no', 'neck_yes',\n\
        \       'supraclavicular_no', 'supraclavicular_yes', 'axillar_?', 'axillar_no',\n\
        \       'axillar_yes', 'mediastinum_no', 'mediastinum_yes', 'abdominal_no',\n\
        \       'abdominal_yes']\n    column_name_labels = 'class_breast'\n\n    df\
        \ = pd.read_csv(filepath_or_buffer=file_path)\n    df_feature_matrix = df[features_to_include]\n\
        \    df_X = df_feature_matrix\n    df_y = df[column_name_labels]\n\n    ##\
        \ output: JSON\n    d_output = {\"X\": np.array(df_X),\n                \"\
        y\": np.array(df_y),\n                \"feature_names\": np.array(df_feature_matrix.columns)}\n\
        \n    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n\
        \    ### write json file\n    with open(output_json, 'w') as outfile:\n  \
        \      outfile.write(json_string_output)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Create\
        \ model inputs', description='Impute unknown values (nan).')\n_parser.add_argument(\"\
        --file\", dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-json\", dest=\"output_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = create_model_inputs(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: scale-df-output_csv, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: create-model-inputs-output_json, path: /tmp/outputs/output_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-json", {"outputPath": "output_json"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef create_model_inputs(file_path,\n                            output_json):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.        \n    \"\"\"\n\n    import
          json \n    from json import JSONEncoder\n    import numpy as np\n    import
          pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n        def default(self,
          obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return JSONEncoder.default(self, obj)\n\n    #
          Read in CSV\n    features_to_include = [\"age_''30-59''\", \"age_''<30''\",
          \"age_''>=60''\", ''sex_?'', ''sex_female'',\n       ''sex_male'', ''histologic-type_?'',
          ''histologic-type_adeno'',\n       ''histologic-type_anaplastic'', ''histologic-type_epidermoid'',\n       ''degree-of-diffe_?'',
          ''degree-of-diffe_fairly'', ''degree-of-diffe_poorly'',\n       ''degree-of-diffe_well'',
          ''bone_no'', ''bone_yes'', ''bone-marrow_no'',\n       ''bone-marrow_yes'',
          ''lung_no'', ''lung_yes'', ''pleura_no'', ''pleura_yes'',\n       ''peritoneum_no'',
          ''peritoneum_yes'', ''liver_no'', ''liver_yes'', ''brain_no'',\n       ''brain_yes'',
          ''skin_?'', ''skin_no'', ''skin_yes'', ''neck_no'', ''neck_yes'',\n       ''supraclavicular_no'',
          ''supraclavicular_yes'', ''axillar_?'', ''axillar_no'',\n       ''axillar_yes'',
          ''mediastinum_no'', ''mediastinum_yes'', ''abdominal_no'',\n       ''abdominal_yes'']\n    column_name_labels
          = ''class_breast''\n\n    df = pd.read_csv(filepath_or_buffer=file_path)\n    df_feature_matrix
          = df[features_to_include]\n    df_X = df_feature_matrix\n    df_y = df[column_name_labels]\n\n    ##
          output: JSON\n    d_output = {\"X\": np.array(df_X),\n                \"y\":
          np.array(df_y),\n                \"feature_names\": np.array(df_feature_matrix.columns)}\n\n    json_string_output
          = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ### write json file\n    with
          open(output_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Create model inputs'',
          description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\",
          dest=\"output_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = create_model_inputs(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"description": "A string containing path to input data.        ", "name":
          "file", "type": "CSV"}], "name": "Create model inputs", "outputs": [{"name":
          "output_json", "type": "JSON"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: decision-tree
    container:
      args: [--file, /tmp/inputs/file/data, --output-json, /tmp/outputs/output_json/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef decision_tree(file_path,\n                            output_json):\n\
        \    \"\"\"Impute unknown values (nan).\n        Input: CSV.\n        Output:\
        \ CSV.\n\n    Args:\n        file_path: A string containing path to input\
        \ data.        \n    \"\"\"\n\n    import json \n    from json import JSONEncoder\n\
        \    import numpy as np\n    import pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n\
        \        def default(self, obj):\n            if isinstance(obj, np.ndarray):\n\
        \                return obj.tolist()\n            return JSONEncoder.default(self,\
        \ obj)\n\n    # read in JSON\n    with open(file_path, \"r\") as read_file:\n\
        \        json_input_data = json.load(read_file)\n\n    X = np.array(json_input_data['X'])\n\
        \    y = np.array(json_input_data['y'])\n    feature_names = json_input_data['feature_names']\n\
        \n    # output: JSON\n    d_output = {\"X\": X,\n                \"y\": y,\n\
        \                \"feature_names\": feature_names}\n\n    json_string_output\
        \ = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json file\n\
        \    with open(output_json, 'w') as outfile:\n        outfile.write(json_string_output)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Decision tree',\
        \ description='Impute unknown values (nan).')\n_parser.add_argument(\"--file\"\
        , dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-json\", dest=\"output_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = decision_tree(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: feature-selection-output_json, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: decision-tree-output_json, path: /tmp/outputs/output_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-json", {"outputPath": "output_json"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef decision_tree(file_path,\n                            output_json):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.        \n    \"\"\"\n\n    import
          json \n    from json import JSONEncoder\n    import numpy as np\n    import
          pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n        def default(self,
          obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return JSONEncoder.default(self, obj)\n\n    #
          read in JSON\n    with open(file_path, \"r\") as read_file:\n        json_input_data
          = json.load(read_file)\n\n    X = np.array(json_input_data[''X''])\n    y
          = np.array(json_input_data[''y''])\n    feature_names = json_input_data[''feature_names'']\n\n    #
          output: JSON\n    d_output = {\"X\": X,\n                \"y\": y,\n                \"feature_names\":
          feature_names}\n\n    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ##
          write json file\n    with open(output_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Decision tree'', description=''Impute
          unknown values (nan).'')\n_parser.add_argument(\"--file\", dest=\"file_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\",
          dest=\"output_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = decision_tree(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"description": "A string containing path to input data.        ", "name":
          "file", "type": "JSON"}], "name": "Decision tree", "outputs": [{"name":
          "output_json", "type": "JSON"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: download-data-kfp-sdk-v2
    container:
      args: []
      command:
      - sh
      - -exc
      - |
        url="$0"
        output_path="$1"
        curl_options="$2"
        mkdir -p "$(dirname "$output_path")"
        curl --get "$url" --output "$output_path" $curl_options
      - https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff
      - /tmp/outputs/Data/data
      - --location
      image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
    outputs:
      artifacts:
      - {name: download-data-kfp-sdk-v2-Data, path: /tmp/outputs/Data/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml',
        pipelines.kubeflow.org/component_spec: '{"description": "Downloads data from
          the specified URL. (Updated for KFP SDK v2.)", "implementation": {"container":
          {"command": ["sh", "-exc", "url=\"$0\"\noutput_path=\"$1\"\ncurl_options=\"$2\"\nmkdir
          -p \"$(dirname \"$output_path\")\"\ncurl --get \"$url\" --output \"$output_path\"
          $curl_options\n", {"inputValue": "Url"}, {"outputPath": "Data"}, {"inputValue":
          "curl options"}], "image": "byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342"}},
          "inputs": [{"name": "Url", "type": "String"}, {"default": "--location",
          "description": "Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html",
          "name": "curl options", "type": "String"}], "metadata": {"annotations":
          {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>", "canonical_location":
          "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml"}},
          "name": "Download data (KFP SDK v2)", "outputs": [{"name": "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "3eafccb9f368ff7282d1670d655c2e1b3ee8fdd241dd6b3d0e1d9de4b6da7c9b", "url":
          "./component-sdk-v2.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Url":
          "https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff",
          "curl options": "--location"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: feature-selection
    container:
      args: [--file, /tmp/inputs/file/data, --output-json, /tmp/outputs/output_json/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef feature_selection(file_path,\n                            output_json):\n\
        \    \"\"\"Impute unknown values (nan).\n        Input: CSV.\n        Output:\
        \ CSV.\n\n    Args:\n        file_path: A string containing path to input\
        \ data.        \n    \"\"\"\n\n    import json \n    from json import JSONEncoder\n\
        \    import numpy as np\n    import pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n\
        \        def default(self, obj):\n            if isinstance(obj, np.ndarray):\n\
        \                return obj.tolist()\n            return JSONEncoder.default(self,\
        \ obj)\n\n    # read in JSON\n    with open(file_path, \"r\") as read_file:\n\
        \        json_input_data = json.load(read_file)\n\n    X = np.array(json_input_data['X'])\n\
        \    y = np.array(json_input_data['y'])\n    feature_names = json_input_data['feature_names']\n\
        \n    # output: JSON\n    d_output = {\"X\": X,\n                \"y\": y,\n\
        \                \"feature_names\": feature_names}\n\n    json_string_output\
        \ = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json file\n\
        \    with open(output_json, 'w') as outfile:\n        outfile.write(json_string_output)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Feature selection',\
        \ description='Impute unknown values (nan).')\n_parser.add_argument(\"--file\"\
        , dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-json\", dest=\"output_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = feature_selection(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: create-model-inputs-output_json, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: feature-selection-output_json, path: /tmp/outputs/output_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-json", {"outputPath": "output_json"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef feature_selection(file_path,\n                            output_json):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.        \n    \"\"\"\n\n    import
          json \n    from json import JSONEncoder\n    import numpy as np\n    import
          pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n        def default(self,
          obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return JSONEncoder.default(self, obj)\n\n    #
          read in JSON\n    with open(file_path, \"r\") as read_file:\n        json_input_data
          = json.load(read_file)\n\n    X = np.array(json_input_data[''X''])\n    y
          = np.array(json_input_data[''y''])\n    feature_names = json_input_data[''feature_names'']\n\n    #
          output: JSON\n    d_output = {\"X\": X,\n                \"y\": y,\n                \"feature_names\":
          feature_names}\n\n    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ##
          write json file\n    with open(output_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Feature selection'',
          description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\",
          dest=\"output_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = feature_selection(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"description": "A string containing path to input data.        ", "name":
          "file", "type": "JSON"}], "name": "Feature selection", "outputs": [{"name":
          "output_json", "type": "JSON"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: get-dummies
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def get_dummies(file_path,
                        output_csv):
            """Distribute categorical features into separate features.
                Input: CSV with categorical (and numeric) features
                Output: CSV with categorical features separated into dummies.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import glob
            import pandas as pd

            df = pd.read_csv(filepath_or_buffer=file_path)
            df_dummies = pd.get_dummies(df)
            df_dummies.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Get dummies', description='Distribute categorical features into separate features.')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = get_dummies(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: download-data-kfp-sdk-v2-Data, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: get-dummies-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Distribute
          categorical features into separate features.", "implementation": {"container":
          {"args": ["--file", {"inputPath": "file"}, "--output-csv", {"outputPath":
          "output_csv"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas==1.1.4'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef get_dummies(file_path,\n                output_csv):\n    \"\"\"Distribute
          categorical features into separate features.\n        Input: CSV with categorical
          (and numeric) features\n        Output: CSV with categorical features separated
          into dummies.\n\n    Args:\n        file_path: A string containing path
          to input data.\n        output_csv: A string containing path to processed
          data.\n    \"\"\"\n    import glob\n    import pandas as pd\n\n    df =
          pd.read_csv(filepath_or_buffer=file_path)\n    df_dummies = pd.get_dummies(df)\n    df_dummies.to_csv(output_csv,
          index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Get
          dummies'', description=''Distribute categorical features into separate features.'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = get_dummies(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"description":
          "A string containing path to input data.", "name": "file", "type": "CSV"}],
          "name": "Get dummies", "outputs": [{"description": "A string containing
          path to processed data.", "name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: impute-unknown
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def impute_unknown(file_path,
                        output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import pandas as pd

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # Output to CSV
            df.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Impute unknown', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = impute_unknown(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: get-dummies-output_csv, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: impute-unknown-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef impute_unknown(file_path,\n                output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import pandas as pd\n\n    # Read
          in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    # Output
          to CSV\n    df.to_csv(output_csv, index = False, header = True)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Impute unknown'', description=''Impute
          unknown values (nan).'')\n_parser.add_argument(\"--file\", dest=\"file_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = impute_unknown(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"description": "A string containing path to input data.", "name": "file",
          "type": "CSV"}], "name": "Impute unknown", "outputs": [{"description": "A
          string containing path to processed data.", "name": "output_csv", "type":
          "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: pl04-webdl-dp-fs-classif-decisiontree
    dag:
      tasks:
      - name: create-model-inputs
        template: create-model-inputs
        dependencies: [scale-df]
        arguments:
          artifacts:
          - {name: scale-df-output_csv, from: '{{tasks.scale-df.outputs.artifacts.scale-df-output_csv}}'}
      - name: decision-tree
        template: decision-tree
        dependencies: [feature-selection]
        arguments:
          artifacts:
          - {name: feature-selection-output_json, from: '{{tasks.feature-selection.outputs.artifacts.feature-selection-output_json}}'}
      - {name: download-data-kfp-sdk-v2, template: download-data-kfp-sdk-v2}
      - name: feature-selection
        template: feature-selection
        dependencies: [create-model-inputs]
        arguments:
          artifacts:
          - {name: create-model-inputs-output_json, from: '{{tasks.create-model-inputs.outputs.artifacts.create-model-inputs-output_json}}'}
      - name: get-dummies
        template: get-dummies
        dependencies: [download-data-kfp-sdk-v2]
        arguments:
          artifacts:
          - {name: download-data-kfp-sdk-v2-Data, from: '{{tasks.download-data-kfp-sdk-v2.outputs.artifacts.download-data-kfp-sdk-v2-Data}}'}
      - name: impute-unknown
        template: impute-unknown
        dependencies: [get-dummies]
        arguments:
          artifacts:
          - {name: get-dummies-output_csv, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv}}'}
      - name: scale-df
        template: scale-df
        dependencies: [impute-unknown]
        arguments:
          artifacts:
          - {name: impute-unknown-output_csv, from: '{{tasks.impute-unknown.outputs.artifacts.impute-unknown-output_csv}}'}
  - name: scale-df
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def scale_df(file_path,
                        output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import pandas as pd

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # Output to CSV
            df.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Scale df', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = scale_df(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: impute-unknown-output_csv, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: scale-df-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef scale_df(file_path,\n                output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import pandas as pd\n\n    # Read
          in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    # Output
          to CSV\n    df.to_csv(output_csv, index = False, header = True)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Scale df'', description=''Impute
          unknown values (nan).'')\n_parser.add_argument(\"--file\", dest=\"file_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = scale_df(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"description":
          "A string containing path to input data.", "name": "file", "type": "CSV"}],
          "name": "Scale df", "outputs": [{"description": "A string containing path
          to processed data.", "name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters:
    - {name: url}
  serviceAccountName: pipeline-runner
