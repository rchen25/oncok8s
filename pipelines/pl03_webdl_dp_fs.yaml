apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pl03-webdl-dp-fs-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.11, pipelines.kubeflow.org/pipeline_compilation_time: '2022-03-18T02:09:56.053481',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Pipeline: Download data,
      data processing", "inputs": [{"name": "url"}], "name": "Pl03 webdl dp fs"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.11}
spec:
  entrypoint: pl03-webdl-dp-fs
  templates:
  - name: create-model-inputs
    container:
      args: [--file-path-features, /tmp/inputs/file_path_features/data, --file-path-target,
        /tmp/inputs/file_path_target/data, --output-json, /tmp/outputs/output_json/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.1.4' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef create_model_inputs(file_path_features,\n                        file_path_target,\n\
        \                            output_json):\n    \"\"\"Impute unknown values\
        \ (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n       \
        \ file_path: A string containing path to input data.        \n    \"\"\"\n\
        \n    import json \n    from json import JSONEncoder\n    import numpy as\
        \ np\n    import pandas as pd\n    from sklearn import preprocessing\n   \
        \ from sklearn.model_selection import train_test_split\n\n    class NumpyArrayEncoder(JSONEncoder):\n\
        \        def default(self, obj):\n            if isinstance(obj, np.ndarray):\n\
        \                return obj.tolist()\n            return JSONEncoder.default(self,\
        \ obj)\n\n    # Read in feature df and make X\n\n    df_feature_matrix = pd.read_csv(filepath_or_buffer=file_path_features)\n\
        \    df_X = df_feature_matrix\n    X = np.array(df_X)\n    feature_names =\
        \ np.array(df_feature_matrix.columns)\n\n    # Read in target df and make\
        \ y\n    df_target = pd.read_csv(filepath_or_buffer=file_path_target) # one-column\
        \ df\n    df_y = df_target # one-column df\n    y = np.array(df_y)\n\n   \
        \ # If needed, binarize one-vs-all\n    ## https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n\
        \    lb = preprocessing.LabelBinarizer()\n    lb.fit(y)\n    target_class_names\
        \ = lb.classes_\n    y_binarized = lb.transform(y) # can be multi-dimensional\
        \ array if there are more than 2 classes\n\n    # Train and test split - with\
        \ labels for y\n    X_train, X_test, y_train_labels, y_test_labels = train_test_split(\n\
        \                                            X, y, test_size=0.25, random_state=42)\
        \ \n\n    # Train and test split - one-hot encoded y\n    X_train, X_test,\
        \ y_train_one_hot, y_test_one_hot = train_test_split(\n                  \
        \                          X, y_binarized, test_size=0.25, random_state=42)\
        \ \n\n    ## output: JSON\n    d_output = {\"X\": np.array(X),\n         \
        \       \"y\": y,\n                \"X_train\": X_train,\n               \
        \ \"X_test\": X_test,\n                \"y_train_labels\": y_train_labels,\n\
        \                \"y_test_labels\": y_test_labels,\n                \"y_train_one_hot\"\
        : y_train_one_hot,\n                \"y_test_one_hot\": y_test_one_hot,\n\
        \                \"X_feature_names\": feature_names,\n                \"y_target_class_names\"\
        : target_class_names}\n\n    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n\
        \n    ### write json file\n    with open(output_json, 'w') as outfile:\n \
        \       outfile.write(json_string_output)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Create\
        \ model inputs', description='Impute unknown values (nan).')\n_parser.add_argument(\"\
        --file-path-features\", dest=\"file_path_features\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-target\"\
        , dest=\"file_path_target\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-json\", dest=\"output_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = create_model_inputs(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: scale-df-output_csv, path: /tmp/inputs/file_path_features/data}
      - {name: get-dummies-output_csv_target, path: /tmp/inputs/file_path_target/data}
    outputs:
      artifacts:
      - {name: create-model-inputs-output_json, path: /tmp/outputs/output_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file-path-features",
          {"inputPath": "file_path_features"}, "--file-path-target", {"inputPath":
          "file_path_target"}, "--output-json", {"outputPath": "output_json"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.1.4'' ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef create_model_inputs(file_path_features,\n                        file_path_target,\n                            output_json):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.        \n    \"\"\"\n\n    import
          json \n    from json import JSONEncoder\n    import numpy as np\n    import
          pandas as pd\n    from sklearn import preprocessing\n    from sklearn.model_selection
          import train_test_split\n\n    class NumpyArrayEncoder(JSONEncoder):\n        def
          default(self, obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return JSONEncoder.default(self, obj)\n\n    #
          Read in feature df and make X\n\n    df_feature_matrix = pd.read_csv(filepath_or_buffer=file_path_features)\n    df_X
          = df_feature_matrix\n    X = np.array(df_X)\n    feature_names = np.array(df_feature_matrix.columns)\n\n    #
          Read in target df and make y\n    df_target = pd.read_csv(filepath_or_buffer=file_path_target)
          # one-column df\n    df_y = df_target # one-column df\n    y = np.array(df_y)\n\n    #
          If needed, binarize one-vs-all\n    ## https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n    lb
          = preprocessing.LabelBinarizer()\n    lb.fit(y)\n    target_class_names
          = lb.classes_\n    y_binarized = lb.transform(y) # can be multi-dimensional
          array if there are more than 2 classes\n\n    # Train and test split - with
          labels for y\n    X_train, X_test, y_train_labels, y_test_labels = train_test_split(\n                                            X,
          y, test_size=0.25, random_state=42) \n\n    # Train and test split - one-hot
          encoded y\n    X_train, X_test, y_train_one_hot, y_test_one_hot = train_test_split(\n                                            X,
          y_binarized, test_size=0.25, random_state=42) \n\n    ## output: JSON\n    d_output
          = {\"X\": np.array(X),\n                \"y\": y,\n                \"X_train\":
          X_train,\n                \"X_test\": X_test,\n                \"y_train_labels\":
          y_train_labels,\n                \"y_test_labels\": y_test_labels,\n                \"y_train_one_hot\":
          y_train_one_hot,\n                \"y_test_one_hot\": y_test_one_hot,\n                \"X_feature_names\":
          feature_names,\n                \"y_target_class_names\": target_class_names}\n\n    json_string_output
          = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ### write json file\n    with
          open(output_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Create model inputs'',
          description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file-path-features\",
          dest=\"file_path_features\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-target\",
          dest=\"file_path_target\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\",
          dest=\"output_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = create_model_inputs(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "file_path_features", "type": "CSV"}, {"name": "file_path_target",
          "type": "CSV"}], "name": "Create model inputs", "outputs": [{"name": "output_json",
          "type": "JSON"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: download-data-kfp-sdk-v2
    container:
      args: []
      command:
      - sh
      - -exc
      - |
        url="$0"
        output_path="$1"
        curl_options="$2"
        mkdir -p "$(dirname "$output_path")"
        curl --get "$url" --output "$output_path" $curl_options
      - https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff
      - /tmp/outputs/Data/data
      - --location
      image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
    outputs:
      artifacts:
      - {name: download-data-kfp-sdk-v2-Data, path: /tmp/outputs/Data/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml',
        pipelines.kubeflow.org/component_spec: '{"description": "Downloads data from
          the specified URL. (Updated for KFP SDK v2.)", "implementation": {"container":
          {"command": ["sh", "-exc", "url=\"$0\"\noutput_path=\"$1\"\ncurl_options=\"$2\"\nmkdir
          -p \"$(dirname \"$output_path\")\"\ncurl --get \"$url\" --output \"$output_path\"
          $curl_options\n", {"inputValue": "Url"}, {"outputPath": "Data"}, {"inputValue":
          "curl options"}], "image": "byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342"}},
          "inputs": [{"name": "Url", "type": "String"}, {"default": "--location",
          "description": "Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html",
          "name": "curl options", "type": "String"}], "metadata": {"annotations":
          {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>", "canonical_location":
          "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml"}},
          "name": "Download data (KFP SDK v2)", "outputs": [{"name": "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "3eafccb9f368ff7282d1670d655c2e1b3ee8fdd241dd6b3d0e1d9de4b6da7c9b", "url":
          "./component-sdk-v2.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Url":
          "https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff",
          "curl options": "--location"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: feature-selection
    container:
      args: [--file, /tmp/inputs/file/data, --output-json, /tmp/outputs/output_json/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.1.4' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef feature_selection(file_path,\n                            output_json):\n\
        \    \"\"\"Impute unknown values (nan).\n        Input: CSV.\n        Output:\
        \ CSV.\n\n    Args:\n        file_path: A string containing path to input\
        \ data.        \n    \"\"\"\n\n    import json \n    from json import JSONEncoder\n\
        \    import numpy as np\n    import pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n\
        \        def default(self, obj):\n            if isinstance(obj, np.ndarray):\n\
        \                return obj.tolist()\n            return JSONEncoder.default(self,\
        \ obj)\n\n    # read in JSON\n    with open(file_path, \"r\") as read_file:\n\
        \        json_input_data = json.load(read_file)\n\n    X = np.array(json_input_data['X'])\n\
        \    y = np.array(json_input_data['y'])\n    X_feature_names = json_input_data['X_feature_names']\n\
        \    y_target_class_names = json_input_data['y_target_class_names']\n\n  \
        \  # output: JSON\n    d_output = json_input_data\n\n    json_string_output\
        \ = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json file\n\
        \    with open(output_json, 'w') as outfile:\n        outfile.write(json_string_output)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Feature selection',\
        \ description='Impute unknown values (nan).')\n_parser.add_argument(\"--file\"\
        , dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-json\", dest=\"output_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = feature_selection(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: create-model-inputs-output_json, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: feature-selection-output_json, path: /tmp/outputs/output_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-json", {"outputPath": "output_json"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.1.4'' ''scikit-learn==1.0.2''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas==1.1.4'' ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef feature_selection(file_path,\n                            output_json):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.        \n    \"\"\"\n\n    import
          json \n    from json import JSONEncoder\n    import numpy as np\n    import
          pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n        def default(self,
          obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return JSONEncoder.default(self, obj)\n\n    #
          read in JSON\n    with open(file_path, \"r\") as read_file:\n        json_input_data
          = json.load(read_file)\n\n    X = np.array(json_input_data[''X''])\n    y
          = np.array(json_input_data[''y''])\n    X_feature_names = json_input_data[''X_feature_names'']\n    y_target_class_names
          = json_input_data[''y_target_class_names'']\n\n    # output: JSON\n    d_output
          = json_input_data\n\n    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ##
          write json file\n    with open(output_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Feature selection'',
          description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\",
          dest=\"output_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = feature_selection(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"description": "A string containing path to input data.        ", "name":
          "file", "type": "JSON"}], "name": "Feature selection", "outputs": [{"name":
          "output_json", "type": "JSON"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: get-dummies
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv-features, /tmp/outputs/output_csv_features/data,
        --output-csv-target, /tmp/outputs/output_csv_target/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def get_dummies(file_path,
                        output_csv_features,
                        output_csv_target):
            """Distribute categorical features into separate features.
                Input: CSV with categorical (and numeric) features
                Output: CSV with categorical features separated into dummies.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import glob
            import pandas as pd

            df = pd.read_csv(filepath_or_buffer=file_path)
            l_col_names = list(df.columns)

            # assume last col is target
            df_target = df[l_col_names[-1]]

            # create dummies for every col except last
            df_features = df[l_col_names[:len(l_col_names)-1]]
            df_features_dummies = pd.get_dummies(df_features)

            # write outputs
            df_features_dummies.to_csv(output_csv_features, index = False, header = True)
            df_target.to_csv(output_csv_target, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Get dummies', description='Distribute categorical features into separate features.')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv-features", dest="output_csv_features", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv-target", dest="output_csv_target", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = get_dummies(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: download-data-kfp-sdk-v2-Data, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: get-dummies-output_csv_features, path: /tmp/outputs/output_csv_features/data}
      - {name: get-dummies-output_csv_target, path: /tmp/outputs/output_csv_target/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Distribute
          categorical features into separate features.", "implementation": {"container":
          {"args": ["--file", {"inputPath": "file"}, "--output-csv-features", {"outputPath":
          "output_csv_features"}, "--output-csv-target", {"outputPath": "output_csv_target"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef get_dummies(file_path,\n                output_csv_features,\n                output_csv_target):\n    \"\"\"Distribute
          categorical features into separate features.\n        Input: CSV with categorical
          (and numeric) features\n        Output: CSV with categorical features separated
          into dummies.\n\n    Args:\n        file_path: A string containing path
          to input data.\n        output_csv: A string containing path to processed
          data.\n    \"\"\"\n    import glob\n    import pandas as pd\n\n    df =
          pd.read_csv(filepath_or_buffer=file_path)\n    l_col_names = list(df.columns)\n\n    #
          assume last col is target\n    df_target = df[l_col_names[-1]]\n\n    #
          create dummies for every col except last\n    df_features = df[l_col_names[:len(l_col_names)-1]]\n    df_features_dummies
          = pd.get_dummies(df_features)\n\n    # write outputs\n    df_features_dummies.to_csv(output_csv_features,
          index = False, header = True)\n    df_target.to_csv(output_csv_target, index
          = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Get
          dummies'', description=''Distribute categorical features into separate features.'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-features\",
          dest=\"output_csv_features\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-target\",
          dest=\"output_csv_target\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = get_dummies(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"description":
          "A string containing path to input data.", "name": "file", "type": "CSV"}],
          "name": "Get dummies", "outputs": [{"name": "output_csv_features", "type":
          "CSV"}, {"name": "output_csv_target", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: impute-unknown
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def impute_unknown(file_path,
                        output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import pandas as pd

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # Output to CSV
            df.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Impute unknown', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = impute_unknown(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: get-dummies-output_csv_features, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: impute-unknown-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef impute_unknown(file_path,\n                output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import pandas as pd\n\n    # Read
          in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    # Output
          to CSV\n    df.to_csv(output_csv, index = False, header = True)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Impute unknown'', description=''Impute
          unknown values (nan).'')\n_parser.add_argument(\"--file\", dest=\"file_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = impute_unknown(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"description": "A string containing path to input data.", "name": "file",
          "type": "CSV"}], "name": "Impute unknown", "outputs": [{"description": "A
          string containing path to processed data.", "name": "output_csv", "type":
          "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: pl03-webdl-dp-fs
    dag:
      tasks:
      - name: create-model-inputs
        template: create-model-inputs
        dependencies: [get-dummies, scale-df]
        arguments:
          artifacts:
          - {name: get-dummies-output_csv_target, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_target}}'}
          - {name: scale-df-output_csv, from: '{{tasks.scale-df.outputs.artifacts.scale-df-output_csv}}'}
      - {name: download-data-kfp-sdk-v2, template: download-data-kfp-sdk-v2}
      - name: feature-selection
        template: feature-selection
        dependencies: [create-model-inputs]
        arguments:
          artifacts:
          - {name: create-model-inputs-output_json, from: '{{tasks.create-model-inputs.outputs.artifacts.create-model-inputs-output_json}}'}
      - name: get-dummies
        template: get-dummies
        dependencies: [download-data-kfp-sdk-v2]
        arguments:
          artifacts:
          - {name: download-data-kfp-sdk-v2-Data, from: '{{tasks.download-data-kfp-sdk-v2.outputs.artifacts.download-data-kfp-sdk-v2-Data}}'}
      - name: impute-unknown
        template: impute-unknown
        dependencies: [get-dummies]
        arguments:
          artifacts:
          - {name: get-dummies-output_csv_features, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_features}}'}
      - name: scale-df
        template: scale-df
        dependencies: [impute-unknown]
        arguments:
          artifacts:
          - {name: impute-unknown-output_csv, from: '{{tasks.impute-unknown.outputs.artifacts.impute-unknown-output_csv}}'}
  - name: scale-df
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def scale_df(file_path,
                        output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import pandas as pd

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # Output to CSV
            df.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Scale df', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = scale_df(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: impute-unknown-output_csv, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: scale-df-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef scale_df(file_path,\n                output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import pandas as pd\n\n    # Read
          in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    # Output
          to CSV\n    df.to_csv(output_csv, index = False, header = True)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Scale df'', description=''Impute
          unknown values (nan).'')\n_parser.add_argument(\"--file\", dest=\"file_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = scale_df(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"description":
          "A string containing path to input data.", "name": "file", "type": "CSV"}],
          "name": "Scale df", "outputs": [{"description": "A string containing path
          to processed data.", "name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters:
    - {name: url}
  serviceAccountName: pipeline-runner
