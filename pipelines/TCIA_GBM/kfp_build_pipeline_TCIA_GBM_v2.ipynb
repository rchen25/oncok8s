{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5001e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c450717",
   "metadata": {},
   "source": [
    "## 1 Data acquisition and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45e8baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load component from yaml\n",
    "component_download_file = kfp.components.load_component_from_file(\"./component-sdk-v2.yaml\")\n",
    "\n",
    "def process_data_tarball(file_path: comp.InputPath('Tarball'),\n",
    "                         output_patient_id_list: comp.OutputPath('CSV'),\n",
    "                         output_master_df: comp.OutputPath('CSV')):\n",
    "    \"\"\"Specific data processing of tarball downloaded.\n",
    "        - Hard code tarball content names. \n",
    "        - Assume there is MANIFEST.txt\n",
    "        - Output: \n",
    "            - output_MANIFEST: MANIFEST.txt as df\n",
    "            - output_DATA: tables in dict (keys: name as in MANIFEST.txt, values: table)\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the tarball.\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    import glob\n",
    "    import json \n",
    "    from json import JSONEncoder\n",
    "    import numpy as np\n",
    "    import tarfile\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "        \n",
    "    tarfile.open(name=file_path, mode=\"r|gz\").extractall('data_extracted')\n",
    "    l_tarball_contents = tarfile.open(name=file_path, mode=\"r|gz\").getnames()\n",
    "\n",
    "    # all dataframes\n",
    "    d_df_data = {}\n",
    "    \n",
    "    for name in l_tarball_contents:\n",
    "        archive_filename = 'data_extracted/' + name\n",
    "        if name == 'MANIFEST.txt':\n",
    "            df_manifest = pd.read_csv(glob.glob(archive_filename)[0], sep = \"\\t\")\n",
    "        else:\n",
    "            df = pd.read_csv(glob.glob(archive_filename)[0], sep = \"\\t\")\n",
    "            df.to_csv(index=False, header=True)\n",
    "            d_df_data[name] = df    \n",
    "\n",
    "    filename_patient_table = '57683e22-a8ea-4eca-bfcf-f708cf459546/nationwidechildrens.org_clinical_patient_gbm.txt'\n",
    "    filename_follow_up_table = 'c9cdbc76-105d-429b-9fce-f000819716f9/nationwidechildrens.org_clinical_follow_up_v1.0_gbm.txt'\n",
    "            \n",
    "    # 1 remove un-needed header rows for tables\n",
    "    df_patient_raw = d_df_data[filename_patient_table]\n",
    "    df_follow_up_raw = d_df_data[filename_follow_up_table]\n",
    "    \n",
    "    ## use the 2nd row as the column name (in raw data, first 3 rows are column labels)\n",
    "    ### patient table\n",
    "    df_patient = df_patient_raw.drop([1])\n",
    "    df_patient = df_patient.tail(len(df_patient) -1 )\n",
    "    df_patient = df_patient.reset_index().drop(['index'], axis = 1)\n",
    "    ### follow_up table\n",
    "    df_follow_up = df_follow_up_raw.drop([1])\n",
    "    df_follow_up = df_follow_up.tail(len(df_follow_up) -1 )\n",
    "    df_follow_up = df_follow_up.reset_index().drop(['index'], axis = 1)\n",
    "    \n",
    "    \n",
    "    # 2 data processing\n",
    "    ## 2a: data processing: clnical data\n",
    "    \n",
    "    ### patient table\n",
    "    missing_value_flags = ['[Not Available]',\n",
    "                       '[Discrepancy]',\n",
    "                       '[Unknown]',\n",
    "                       '[Not Applicable]',\n",
    "                       '[Not Evaluated]'\n",
    "                      ]\n",
    "    #### replace flags with np.nan\n",
    "    for flag in missing_value_flags:\n",
    "        df_patient = df_patient.replace(flag, np.nan)\n",
    "    \n",
    "    #### drop un-needed columns - identifiers\n",
    "    df_patient = df_patient.drop(['bcr_patient_uuid', 'form_completion_date', 'patient_id'], axis=1)\n",
    "    \n",
    "    #### parse numerical columns w/ dates\n",
    "    l_numerical_cols = [\n",
    "        'last_contact_days_to',\n",
    "        'death_days_to',\n",
    "        'age_at_initial_pathologic_diagnosis'\n",
    "    ]\n",
    "    for col in l_numerical_cols:\n",
    "        df_patient[col] = [float(x) for x in df_patient[col]]\n",
    "\n",
    "    df_patient['survival_time_yrs'] = np.abs(df_patient['death_days_to']) / 365\n",
    "    df_patient['days_since_last_contact'] = np.abs(df_patient['last_contact_days_to']) / 365\n",
    "    \n",
    "    #### drop columns parsed / not needed anymore\n",
    "    l_columns_not_needed = [\n",
    "        'last_contact_days_to',\n",
    "        'birth_days_to',\n",
    "        'death_days_to',\n",
    "\n",
    "        # uninformative cols (all missing/not evaluated etc):\n",
    "        'anatomic_neoplasm_subdivision',\n",
    "        'disease_code',\n",
    "        'project_code',\n",
    "        'days_to_initial_pathologic_diagnosis',\n",
    "        'icd_10',\n",
    "        'icd_o_3_histology',\n",
    "        'icd_o_3_site',\n",
    "        'informed_consent_verified',\n",
    "        'initial_pathologic_dx_year'\n",
    "\n",
    "    ]\n",
    "    df_patient = df_patient.drop(l_columns_not_needed, axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### follow_up table\n",
    "    #### replace missing value flags with np.nan\n",
    "    missing_value_flags = ['[Not Available]',\n",
    "                           '[Discrepancy]',\n",
    "                           '[Unknown]',\n",
    "                           '[Not Applicable]',\n",
    "                           '[Not Evaluated]'\n",
    "                          ]\n",
    "    for flag in missing_value_flags:\n",
    "        df_follow_up = df_follow_up.replace(flag, np.nan)\n",
    "\n",
    "    #### drop un-needed columns - identifiers\n",
    "    df_follow_up = df_follow_up.drop(['bcr_patient_uuid', 'bcr_followup_uuid', 'form_completion_date',\n",
    "                                     'followup_reason', 'followup_lost_to'], axis=1)\n",
    "\n",
    "    #### parse numerical columns w/ dates\n",
    "    l_numerical_cols = [\n",
    "        'last_contact_days_to',\n",
    "        'death_days_to'\n",
    "    ]\n",
    "    for col in l_numerical_cols:\n",
    "        df_follow_up[col] = [float(x) for x in df_follow_up[col]]\n",
    "\n",
    "    #### isolate the LAST followup (sorted by barcode) --> get one row per patient\n",
    "    df_follow_up = df_follow_up.groupby(['bcr_patient_barcode']).tail(1)\n",
    "\n",
    "    #### drop columns parsed / not needed anymore\n",
    "    l_columns_not_needed = [\n",
    "        'last_contact_days_to',\n",
    "        'death_days_to',\n",
    "        'bcr_followup_barcode'\n",
    "    ]\n",
    "    df_follow_up = df_follow_up.drop(l_columns_not_needed, axis = 1)\n",
    "    \n",
    "    \n",
    "    # 3 Merge tables by patient identifier (create master table)\n",
    "\n",
    "    ## merge patient and followup table\n",
    "    ### find common cols: \n",
    "    l_cols_table_1 = set(df_patient.columns)\n",
    "    l_cols_table_2 = set(df_follow_up.columns)\n",
    "    intersect_cols = l_cols_table_1.intersection(l_cols_table_2)\n",
    "    l_cols_to_remove_table_1 = [x for x in intersect_cols if x != 'bcr_patient_barcode']\n",
    "\n",
    "    ### drop common cols from left table (`patient`) before join\n",
    "    df_patient = df_patient.drop(l_cols_to_remove_table_1, axis = 1)\n",
    "    ### join\n",
    "    df_master = df_patient.merge(df_follow_up, \n",
    "                                 left_on = 'bcr_patient_barcode', \n",
    "                                 right_on = 'bcr_patient_barcode', \n",
    "                                 how = 'inner')\n",
    "    ### drop patient ID identifier from master table (used as identifier, not treated as a feature)\n",
    "    df_patient_id_list = df_master[['bcr_patient_barcode']]\n",
    "    df_master = df_master.drop(['bcr_patient_barcode'], axis = 1)    \n",
    "        \n",
    "    # pl output\n",
    "    df_patient_id_list.to_csv(output_patient_id_list, index=False, header=True)\n",
    "    df_master.to_csv(output_master_df, index=False, header=True)\n",
    "    \n",
    "        \n",
    "    \n",
    "create_step_process_data_tarball = kfp.components.create_component_from_func(\n",
    "    func=process_data_tarball,\n",
    "    output_component_file='component_process_data_tarball.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf862f",
   "metadata": {},
   "source": [
    "## 2 Data processing\n",
    "\n",
    "Further process master table\n",
    "\n",
    "- select column to be used as class label; \n",
    "- for other columns, get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "142e1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies_for_features(file: comp.InputPath('CSV'),\n",
    "                                file_path_patient_list: comp.InputPath('CSV'),\n",
    "                                class_label_colname: str,               \n",
    "                                s_colnames_to_exclude: str,\n",
    "                                param_test_set_size: str,\n",
    "                                param_random_seed: str,\n",
    "                                output_csv_features: comp.OutputPath('CSV'),\n",
    "                                output_csv_target: comp.OutputPath('CSV'),\n",
    "                                output_csv_target_class_labels: comp.OutputPath('CSV'),\n",
    "                                output_csv_feature_list: comp.OutputPath('CSV'),\n",
    "                                output_csv_patient_list_filtered: comp.OutputPath('CSV'),\n",
    "                                output_csv_train_indexes: comp.OutputPath('CSV'),\n",
    "                                output_csv_test_indexes: comp.OutputPath('CSV')):\n",
    "    \n",
    "    \"\"\"Distribute categorical features into separate features.\n",
    "        Input: CSV with categorical (and numeric) features. Assume last \n",
    "            feature is target label. \n",
    "        Output: CSV with categorical features separated into dummies.\n",
    "    \n",
    "    Args:\n",
    "        file: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    import glob\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df = pd.read_csv(filepath_or_buffer=file)\n",
    "    l_col_names = list(df.columns)\n",
    "\n",
    "    # isolate column for target class label; if null, use last column\n",
    "    target_class_label = class_label_colname if str(class_label_colname) != '' else l_col_names[-1]\n",
    "    ## remove all rows where class label col is NaN    \n",
    "    df = df.dropna(subset=[target_class_label])  \n",
    "    ## remove patients from master list whose data got removed because class label col was NaN\n",
    "    df_patient_list = pd.read_csv(file_path_patient_list)\n",
    "    df_patient_list_filtered = df_patient_list.loc[df.index]\n",
    "\n",
    "    ## extract target class column\n",
    "    df_target_raw = df[target_class_label] # from input parameter\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(df_target_raw.astype(str)) # fit to data for target class to find all classes\n",
    "    target_class_label_names = lb.classes_ # store array of all the classes\n",
    "    df_target_class_labels = pd.DataFrame({'class': target_class_label_names})\n",
    "    d_target_class_label_idx = dict(zip(list(df_target_class_labels['class']), list(df_target_class_labels.index)))\n",
    "    l_target_column = [d_target_class_label_idx[x] for x in df[target_class_label]]\n",
    "    df_target = pd.DataFrame([])\n",
    "    df_target[target_class_label] = l_target_column# 1-column of multiple classes\n",
    "    df_target['bcr_patient_barcode'] = df_patient_list_filtered['bcr_patient_barcode']\n",
    "\n",
    "    # create dummies for every col except class label col\n",
    "    df_features = df[[x for x in l_col_names if x != class_label_colname]] # features are all colnames except target class\n",
    "\n",
    "    # exclude any other colnames (other than target class column), if specified\n",
    "    ## performed on feature matrix after dummies are removed\n",
    "    if str(s_colnames_to_exclude) != '':\n",
    "        l_colnames_to_exclude = s_colnames_to_exclude.split(',')\n",
    "        l_colnames_in_this_df_to_exclude = [x for x in l_colnames_to_exclude if x in df.columns]\n",
    "        df_features = df_features.drop(l_colnames_to_exclude, axis = 1).reset_index() # reset_index becauase need to get indexes for train/test later on data with rows filtered out\n",
    "    # get dummies\n",
    "    df_features_dummies = pd.get_dummies(df_features)\n",
    "    # feature list\n",
    "    l_features = df_features_dummies.columns\n",
    "    df_feature_list = pd.DataFrame({'feature': l_features})\n",
    "\n",
    "    # create train test and store indexes, using these clinical features\n",
    "    param_test_set_size_float = np.float64(param_test_set_size)\n",
    "    ## set a valid value of 0.25 for test set size if needed   \n",
    "    if param_test_set_size_float <= 0 or param_test_set_size_float >= 1:\n",
    "        param_test_set_size_float = 0.25    \n",
    "\n",
    "    param_random_seed_int = int(np.float64(param_random_seed))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features_dummies, df_target,\n",
    "                                test_size=param_test_set_size_float,random_state=param_random_seed_int) \n",
    "\n",
    "    # IMPT: set random seed\n",
    "    df_idx_train = X_train.index.to_frame().rename(columns={0: \"index\"})\n",
    "    df_idx_train['bcr_patient_barcode'] = df_patient_list_filtered.iloc[X_train.index]\n",
    "    df_idx_test = X_test.index.to_frame().rename(columns={0: \"index\"})\n",
    "    df_idx_test['bcr_patient_barcode'] = df_patient_list_filtered.iloc[X_test.index]\n",
    "\n",
    "    # write outputs\n",
    "    del df_features_dummies['index']\n",
    "    df_features_dummies.to_csv(output_csv_features, index = False, header = True)\n",
    "    df_target.to_csv(output_csv_target, index = False, header = True)\n",
    "    df_target_class_labels.to_csv(output_csv_target_class_labels, index = True, header = False)\n",
    "    df_feature_list.to_csv(output_csv_feature_list, index = True, header = False)\n",
    "\n",
    "    ### save new filtered patient list\n",
    "    df_patient_list_filtered.to_csv(output_csv_patient_list_filtered, header=True, index=False)\n",
    "    ### save training and testing splits - to use with all fused feature matrices from now on    \n",
    "    df_idx_train.to_csv(output_csv_train_indexes, index = False, header = True)\n",
    "    df_idx_test.to_csv(output_csv_test_indexes, index = False, header = True)\n",
    "\n",
    "\n",
    "create_step_dp_get_dummies = kfp.components.create_component_from_func(\n",
    "    func=get_dummies_for_features,\n",
    "    output_component_file='component_dp_get_dummies.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                        'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae32e8f",
   "metadata": {},
   "source": [
    "## download images - from DIGITAL SLIDE ARCHIVE\n",
    "\n",
    "Acquire images given `bcr_patient_barcode` - fits in pipeline as shown in image below\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11c160df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(file_path_patient_list: comp.InputPath('CSV'),\n",
    "                    outfile_images_json: comp.OutputPath('JSON')):\n",
    "    \"\"\"Acquisition of images associated with patients listed\n",
    "        in master `patient` table. \n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the tarball.\n",
    "    \"\"\"\n",
    "    import json \n",
    "    from json import JSONEncoder\n",
    "    import cv2\n",
    "    import girder_client\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    class NumpyArrayEncoder(JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return JSONEncoder.default(self, obj)\n",
    "        \n",
    "    gc = girder_client.GirderClient(apiUrl=\"https://api.digitalslidearchive.org/api/v1\")\n",
    "\n",
    "    # list of patients; header col name = \"bcr_patient_barcode\"\n",
    "    df_patient_list = pd.read_csv(file_path_patient_list)\n",
    "\n",
    "    ## FYI: image name format:\n",
    "    ##    <patient_id>-01Z-00-DX1.<string>.svs\n",
    "    ##    TCGA-02-0038-01Z-00-DX1.5E369837-371E-4845-AD78-84BB48E1A082.svs\n",
    "\n",
    "    \n",
    "    # retrieve images by ptID in list\n",
    "    l_imagelist_ptID = []\n",
    "    l_imagelist_imgID = []\n",
    "    l_imagelist_imgContent = []\n",
    "    for ptID in df_patient_list['bcr_patient_barcode']:\n",
    "\n",
    "        # get case metadata for this patient\n",
    "        caseMetadata = gc.get('tcga/case/label/%s' % ptID)\n",
    "        # get caseId for this patient\n",
    "        caseId =  caseMetadata['tcga']['caseId']\n",
    "\n",
    "\n",
    "        # Get images for this case ID..\n",
    "        imageData = gc.get(\"/tcga/case/%s/images\" % caseId)\n",
    "\n",
    "        for i in imageData['data']:\n",
    "            if i['name'].split('.')[0].split('-')[-1] == 'DX1':\n",
    "                print(i['name'],i['_id'])\n",
    "                l_imagelist_ptID.append(ptID)\n",
    "                l_imagelist_imgID.append(i['_id'])\n",
    "\n",
    "    for i in range(len(l_imagelist_imgID)):\n",
    "        # retrieve image content with image ID\n",
    "        imgID = l_imagelist_imgID[i]\n",
    "        imageThumb = gc.get(\"item/%s/tiles/thumbnail\" % imgID,jsonResp=False)\n",
    "        img_array = np.frombuffer(imageThumb.content, dtype=np.uint8)\n",
    "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "        l_imagelist_imgContent.append(img)\n",
    "\n",
    "\n",
    "    # image shape\n",
    "    img_shape = l_imagelist_imgContent[0].shape # assume all images are same shape\n",
    "    df_img_shape = pd.DataFrame([])\n",
    "    df_img_shape['img_shape'] = img_shape\n",
    "                    \n",
    "    # JSON of all images\n",
    "    d_images = dict()\n",
    "    for i in range(len(l_imagelist_ptID)):\n",
    "        ptID = l_imagelist_ptID[i]\n",
    "        if ptID not in d_images:\n",
    "            d_images[ptID] = []\n",
    "        d_images[ptID].append(l_imagelist_imgContent[i])\n",
    "    json_string_output = json.dumps(d_images, cls=NumpyArrayEncoder)\n",
    "\n",
    "    # write outputs\n",
    "    ## write json file\n",
    "    with open(outfile_images_json, 'w') as outfile:\n",
    "        outfile.write(json_string_output)\n",
    "    \n",
    "    \n",
    "create_step_download_images = kfp.components.create_component_from_func(\n",
    "    func=download_images,\n",
    "    output_component_file='component_download_images.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['girder_client==3.1.8',\n",
    "                         'numpy==1.21.2',\n",
    "                         'opencv-python-headless==4.5.5.62',\n",
    "                         'pandas==1.1.4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052b639",
   "metadata": {},
   "source": [
    "## feature construction - from images\n",
    "\n",
    "Construct features from images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e912b78",
   "metadata": {},
   "source": [
    "### feature construction from images: skimage / openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "125314f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features_image_skimage_opencv(file_path_images: comp.InputPath('JSON'),\n",
    "                                file_path_full_patient_id_list: comp.InputPath('CSV'),\n",
    "                                file_path_full_patient_class_labels: comp.InputPath('CSV'),\n",
    "                                outfile_features_df: comp.OutputPath('CSV'),\n",
    "                                outfile_patient_id_list: comp.OutputPath('CSV'),\n",
    "                                outfile_class_labels_pts_with_images: comp.OutputPath('CSV')):\n",
    "    \"\"\"Feature construction from images. \n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to images (3-channel) in JSON obj.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    import json\n",
    "    from collections import OrderedDict\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from skimage.filters import prewitt_h,prewitt_v\n",
    "    \n",
    "\n",
    "    # openCV features\n",
    "    input_filestream_json = \"./output/json_10_images.json\"\n",
    "\n",
    "    with open(file_path_images, \"r\") as read_file:\n",
    "        d_images_from_pl = json.load(read_file)\n",
    "\n",
    "\n",
    "    d_features = OrderedDict()\n",
    "    d_features[\"mean_weight_raw_img\"] = []\n",
    "    d_features[\"mean_edge_weight_horizontal\"] = []\n",
    "    d_features[\"mean_edge_weight_vertical\"] = []\n",
    "\n",
    "    l_ptID = np.sort(list(d_images_from_pl.keys()))\n",
    "    for ptID in l_ptID:\n",
    "        img = np.array(\n",
    "                d_images_from_pl[\n",
    "                    ptID]\n",
    "                        [0] # use the first image for the pt\n",
    "                )\n",
    "        # feature extraction\n",
    "\n",
    "        ## calculating horizontal edges using prewitt kernel\n",
    "        edges_prewitt_horizontal = prewitt_h(img[:,:,0])\n",
    "        ## calculating vertical edges using prewitt kernel\n",
    "        edges_prewitt_vertical = prewitt_v(img[:,:,0])\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_weight_raw_img = np.mean(img)\n",
    "        d_features[\"mean_weight_raw_img\"].append(mean_weight_raw_img)\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_edge_weight_horizontal = np.mean(edges_prewitt_horizontal)\n",
    "        d_features[\"mean_edge_weight_horizontal\"].append(mean_edge_weight_horizontal)\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_edge_weight_vertical = np.mean(edges_prewitt_vertical)\n",
    "        d_features[\"mean_edge_weight_vertical\"].append(mean_edge_weight_vertical)\n",
    "\n",
    "    df_features_combined = pd.DataFrame([])\n",
    "    df_features_combined['bcr_patient_barcode'] = l_ptID\n",
    "    for feature_name in d_features.keys():\n",
    "        df_features_combined[feature_name] = d_features[feature_name]\n",
    "\n",
    "    # output patient ID list\n",
    "    df_patient_id_list = df_features_combined[['bcr_patient_barcode']]\n",
    "    df_patient_id_list.to_csv(outfile_patient_id_list, header = True, index = False)\n",
    "    \n",
    "    # output features\n",
    "    del df_features_combined['bcr_patient_barcode']\n",
    "    df_features_combined.to_csv(outfile_features_df, header = True, index = False)\n",
    "\n",
    "    # output target class labels for this cohort of patients (not all patients in full\n",
    "    #  cohort will have images)\n",
    "    df_full_patient_id_list = pd.read_csv(file_path_full_patient_id_list)\n",
    "    df_full_patient_class_labels = pd.read_csv(file_path_full_patient_class_labels)\n",
    "    ## isolate patients that have images\n",
    "    df_slice_full_patient_id_list_have_images = df_full_patient_id_list[df_full_patient_id_list['bcr_patient_barcode'].isin(df_patient_id_list['bcr_patient_barcode'])]\n",
    "    idx_in_full_patient_id_list_have_images = df_slice_full_patient_id_list_have_images.index\n",
    "    df_class_labels_pts_with_images = df_full_patient_class_labels.loc[idx_in_full_patient_id_list_have_images]\n",
    "    \n",
    "    df_class_labels_pts_with_images.to_csv(outfile_class_labels_pts_with_images, header=True, index=False)\n",
    "    df_slice_full_patient_id_list_have_images.to_csv(outfile_patient_id_list, header=True, index=False)\n",
    "    \n",
    "    \n",
    "create_step_image_feature_construction_image_fx = kfp.components.create_component_from_func(\n",
    "    func=construct_features_image_skimage_opencv,\n",
    "    output_component_file='component_image_feature_skimage_opencv.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['scikit-image==0.19.1',\n",
    "                         'numpy==1.21.2',\n",
    "                         'opencv-python-headless==4.5.5.62',\n",
    "                         'pandas==1.1.4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf71ea5",
   "metadata": {},
   "source": [
    "### feature construction: pathML\n",
    "\n",
    "* leverage pathML package for feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88b60c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features_images_pathml(file_path_images: comp.InputPath('JSON'),\n",
    "                                file_path_full_patient_id_list: comp.InputPath('CSV'),\n",
    "                                file_path_full_patient_class_labels: comp.InputPath('CSV'),\n",
    "                                outfile_features_df: comp.OutputPath('CSV'),\n",
    "                                outfile_patient_id_list: comp.OutputPath('CSV'),\n",
    "                                outfile_class_labels_pts_with_images: comp.OutputPath('CSV')):\n",
    "    \"\"\"Feature construction from images. \n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to images (3-channel) in JSON obj.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # install \n",
    "    import pip\n",
    "    \n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "    def uninstall(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package])        \n",
    "        \n",
    "    # install cytoolz, numpy - needed for installation of pathml\n",
    "#    install(\"cytoolz==0.11.2\")\n",
    "#     uninstall(\"numpy\")\n",
    "#     install(\"numpy\")\n",
    "    \n",
    "    import conda.cli\n",
    "    # install openjdk - needed for pathml\n",
    "    conda.cli.main('conda', 'config',  '--add', 'channels', 'conda-forge') # ! conda config --add channels conda-forge\n",
    "    conda.cli.main('conda', 'install', '-c', 'conda-forge', '-y', 'openjdk')\n",
    "    \n",
    "    # install python-javabridge==4.0.3\n",
    "    conda.cli.main('conda', 'install', '-c', 'conda-forge', '-y', 'python-javabridge')\n",
    "    \n",
    "    # install pathml\n",
    "    install(\"pathml==2.1.0\")\n",
    "    \n",
    "    \n",
    "#     import os\n",
    "#     print(\"JAVA_HOME: \", os.environ[\"JAVA_HOME\"])\n",
    "#     print(\"HOME: \", os.environ[\"HOME\"])\n",
    "    \n",
    "    \n",
    "#     from pathml.core import HESlide\n",
    "#     from pathml.preprocessing import StainNormalizationHE\n",
    "\n",
    "    import json\n",
    "    from collections import OrderedDict\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from skimage.filters import prewitt_h,prewitt_v\n",
    "    \n",
    "\n",
    "\n",
    "    # open images\n",
    "    \n",
    "    with open(file_path_images, \"r\") as read_file:\n",
    "        d_images_from_pl = json.load(read_file)\n",
    "\n",
    "    # compute pathml features\n",
    "    d_normed_images = dict()\n",
    "    for i, method in enumerate([\"macenko\", \"vahadane\"]):\n",
    "        d_normed_images[method] = dict()\n",
    "    for i, method in enumerate([\"macenko\", \"vahadane\"]):\n",
    "        for j, target in enumerate([\"normalize\", \"hematoxylin\", \"eosin\"]):\n",
    "            d_normed_images[method][target] = dict()\n",
    "\n",
    "    l_ptID = np.sort(list(d_images_from_pl.keys()))\n",
    "    for ptID in l_ptID:\n",
    "        img = np.array(\n",
    "                d_images_from_pl[\n",
    "                    ptID]\n",
    "                        [0] # use the first image for the pt\n",
    "                )\n",
    "        # feature extraction\n",
    "\n",
    "        ## stain normalization\n",
    "        for i, method in enumerate([\"macenko\", \"vahadane\"]):\n",
    "            for j, target in enumerate([\"normalize\", \"hematoxylin\", \"eosin\"]):\n",
    "                # initialize stain normalization object\n",
    "                normalizer = StainNormalizationHE(target = target, stain_estimation_method = method)\n",
    "                # apply on example image\n",
    "                im = normalizer.F(img)  ## apply normalizer to the image `img`\n",
    "                d_normed_images[\"method\"][\"target\"][ptID] = im\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    ##########################################\n",
    "    \n",
    "    # compute openCV features\n",
    "\n",
    "    d_features = OrderedDict()\n",
    "    d_features[\"mean_weight_raw_img\"] = []\n",
    "    d_features[\"mean_edge_weight_horizontal\"] = []\n",
    "    d_features[\"mean_edge_weight_vertical\"] = []\n",
    "\n",
    "    l_ptID = np.sort(list(d_images_from_pl.keys()))\n",
    "    for ptID in l_ptID:\n",
    "        img = np.array(\n",
    "                d_images_from_pl[\n",
    "                    ptID]\n",
    "                        [0] # use the first image for the pt\n",
    "                )\n",
    "        # feature extraction\n",
    "\n",
    "        ## calculating horizontal edges using prewitt kernel\n",
    "        edges_prewitt_horizontal = prewitt_h(img[:,:,0])\n",
    "        ## calculating vertical edges using prewitt kernel\n",
    "        edges_prewitt_vertical = prewitt_v(img[:,:,0])\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_weight_raw_img = np.mean(img)\n",
    "        d_features[\"mean_weight_raw_img\"].append(mean_weight_raw_img)\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_edge_weight_horizontal = np.mean(edges_prewitt_horizontal)\n",
    "        d_features[\"mean_edge_weight_horizontal\"].append(mean_edge_weight_horizontal)\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_edge_weight_vertical = np.mean(edges_prewitt_vertical)\n",
    "        d_features[\"mean_edge_weight_vertical\"].append(mean_edge_weight_vertical)\n",
    "\n",
    "    df_features_combined = pd.DataFrame([])\n",
    "    df_features_combined['bcr_patient_barcode'] = l_ptID\n",
    "    for feature_name in d_features.keys():\n",
    "        df_features_combined[feature_name] = d_features[feature_name]\n",
    "\n",
    "    # output patient ID list\n",
    "    df_patient_id_list = df_features_combined[['bcr_patient_barcode']]\n",
    "    df_patient_id_list.to_csv(outfile_patient_id_list, header = True, index = False)\n",
    "    \n",
    "    # output features\n",
    "    del df_features_combined['bcr_patient_barcode']\n",
    "    df_features_combined.to_csv(outfile_features_df, header = True, index = False)\n",
    "\n",
    "    # output target class labels for this cohort of patients (not all patients in full\n",
    "    #  cohort will have images)\n",
    "    df_full_patient_id_list = pd.read_csv(file_path_full_patient_id_list)\n",
    "    df_full_patient_class_labels = pd.read_csv(file_path_full_patient_class_labels)\n",
    "    ## isolate patients that have images\n",
    "    df_slice_full_patient_id_list_have_images = df_full_patient_id_list[df_full_patient_id_list['bcr_patient_barcode'].isin(df_patient_id_list['bcr_patient_barcode'])]\n",
    "    idx_in_full_patient_id_list_have_images = df_slice_full_patient_id_list_have_images.index\n",
    "    df_class_labels_pts_with_images = df_full_patient_class_labels.loc[idx_in_full_patient_id_list_have_images]\n",
    "    \n",
    "    df_class_labels_pts_with_images.to_csv(outfile_class_labels_pts_with_images, header=True, index=False)\n",
    "    df_slice_full_patient_id_list_have_images.to_csv(outfile_patient_id_list, header=True, index=False)\n",
    "    \n",
    "    \n",
    "create_step_image_feature_construction_pathml = kfp.components.create_component_from_func(\n",
    "    func=construct_features_images_pathml,\n",
    "    output_component_file='component_image_feature_construction_pathml.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['conda==4.3.16',\n",
    "                         \"cytoolz==0.11.2\",\n",
    "                         'numpy==1.21.2',\n",
    "                         'opencv-python-headless==4.5.5.62',\n",
    "                         'pandas==1.1.4',\n",
    "#                         'pathml==2.1.0',\n",
    "                         'scikit-image==0.19.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c1e23",
   "metadata": {},
   "source": [
    "### feature construction: HistomicsTK\n",
    "\n",
    "Methodologies using\n",
    "    \n",
    "* Positive pixel count: https://digitalslidearchive.github.io/HistomicsTK/examples/positive_pixel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7c905fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features_images_histomicstk(file_path_images: comp.InputPath('JSON'),\n",
    "                                file_path_full_patient_id_list: comp.InputPath('CSV'),\n",
    "                                file_path_full_patient_class_labels: comp.InputPath('CSV'),\n",
    "                                user_input_class_label_column_name: str,\n",
    "                                param_test_set_size: str,\n",
    "                                param_random_seed: str,          \n",
    "                                outfile_features_df: comp.OutputPath('CSV'),\n",
    "                                outfile_patient_id_list: comp.OutputPath('CSV'),\n",
    "                                outfile_class_labels_pts_with_images: comp.OutputPath('CSV'), \n",
    "                                output_csv_train_indexes: comp.OutputPath('CSV'),\n",
    "                                output_csv_test_indexes: comp.OutputPath('CSV')):\n",
    "    \"\"\"Feature construction from images. \n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to images (3-channel) in JSON obj.\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    #####\n",
    "    import os\n",
    "    import pip\n",
    "    import subprocess\n",
    "    import sys\n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    def uninstall(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package])            \n",
    "\n",
    "    ## install gdal, lapack, blas(needed for histomicstk)\n",
    "    import conda.cli\n",
    "#    conda.cli.main('conda', 'config',  '--add', 'channels', 'conda-forge') # ! conda config --add channels conda-forge\n",
    "#    conda.cli.main('conda', 'install', '-c', 'conda-forge', '-y', 'libgdal')\n",
    "#    conda.cli.main('conda', 'install', '-c', 'conda-forge', '-y', 'lapack')\n",
    "#    conda.cli.main('conda', 'install', '-c', 'conda-forge', '-y', 'blas')\n",
    "#    conda.cli.main('conda', 'install', '-c', 'conda-forge', '-y', 'scipy')\n",
    "#    conda.cli.main('conda', 'install', '-c', 'conda-forge', '-y', 'mrterry', 'mapnik') # mapnik not found on pypi?\n",
    "#    conda.cli.main('conda', 'install', '-c', 'conda-forge', '-y', 'large-image')\n",
    "    \n",
    "    \n",
    "    \n",
    "#    install(\"histomicstk==1.2.0\") ## histomicstk\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"histomicstk==1.2.0\",\n",
    "                           \"--find-links\", \"https://girder.github.io/large_image_wheels\"]) ## install with find-links\n",
    "    \n",
    "    ##################################################################################\n",
    "    #\n",
    "    # IMPT NOTE: need global import for packages installed programatically\n",
    "    # https://stackoverflow.com/questions/59308781/python-module-not-importing-after-installing-programmatically\n",
    "    #\n",
    "    #          import importlib\n",
    "    #          globals()[package] = importlib.import_module(package)\n",
    "    # Note: python does not add it to path automatically, so need to add sys.path.append(site.getusersitepackages())\n",
    "    #    https://kubeflow-pipelines.readthedocs.io/en/stable/_modules/kfp/components/_python_op.html\n",
    "    #   \n",
    "    ##################################################################################\n",
    "    ### import histomicstk.segmentation.positive_pixel_count as ppc # instead of this, use below\n",
    "    #\n",
    "#     import importlib\n",
    "#     globals()[\"histomicstk\"] = importlib.import_module(\"histomicstk\")\n",
    "#     globals()[\"ppc\"] = importlib.import_module(\"histomicstk.segmentation.positive_pixel_count\")\n",
    "    #\n",
    "    ##################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "#    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'histomicstk', \"--find-links\", \"https://girder.github.io/large_image_wheels\"]) ## install with find-links\n",
    "    import site\n",
    "    sys.path.append(site.getusersitepackages())\n",
    "    sys.path.append('/root/.local/lib/python3.10/site-packages')\n",
    "    sys.path.append('/usr/local/lib/python3.10/site-packages')\n",
    "    sys.path.append('/root/.local/lib/python3.9/site-packages')\n",
    "    sys.path.append('/usr/local/lib/python3.9/site-packages')\n",
    "    sys.path.append('/root/.local/lib/python3.8/site-packages')\n",
    "    sys.path.append('/usr/local/lib/python3.8/site-packages')\n",
    "    sys.path.append('/root/.local/lib/python3.7/site-packages')\n",
    "    sys.path.append('/usr/local/lib/python3.7/site-packages')\n",
    "    sys.path.append('/usr/local')\n",
    "    sys.path.append('/root/.local')\n",
    "    sys.path.append('/usr/local/bin')\n",
    "    sys.path.append('/root/.local/bin')\n",
    "    print(sys.path)\n",
    "    print(os.listdir('/usr/local/lib/python3.9/site-packages/histomicstk/segmentation/label/'))\n",
    "    import histomicstk.segmentation.positive_pixel_count as ppc\n",
    "\n",
    "    \n",
    "    import json\n",
    "    from collections import OrderedDict\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    import large_image\n",
    "    import skimage.io\n",
    "    from skimage.filters import prewitt_h,prewitt_v\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # open images\n",
    "    \n",
    "    with open(file_path_images, \"r\") as read_file:\n",
    "        d_images_from_pl = json.load(read_file)\n",
    "\n",
    "    # compute histomicstk features\n",
    "    ## set template params\n",
    "    template_params = ppc.Parameters(\n",
    "    hue_value=0.05,\n",
    "    hue_width=9999,\n",
    "    saturation_minimum=0.05,\n",
    "    intensity_upper_limit=0.95,\n",
    "    intensity_weak_threshold=0.65,\n",
    "    intensity_strong_threshold=0.35,\n",
    "    intensity_lower_limit=0.05)\n",
    "    \n",
    "    \n",
    "    image_url = ('https://data.kitware.com/api/v1/file/'\n",
    "                 '598b71ee8d777f7d33e9c1d4/download')  # DAB.png\n",
    "    img_input_sample = skimage.io.imread(image_url)\n",
    "\n",
    "    ## compute for each patient's image\n",
    "    l_ptID = np.sort(list(d_images_from_pl.keys()))\n",
    "    for ptID in l_ptID:\n",
    "        img = np.array(\n",
    "                d_images_from_pl[\n",
    "                    ptID]\n",
    "                        [0] # use the first image for the pt\n",
    "                )\n",
    "        # feature extraction  \n",
    "        try:\n",
    "            stats, label_image = ppc.count_image(img, template_params)\n",
    "        except:\n",
    "            stats, label_image = ppc.count_image(img_input_sample, template_params)\n",
    "\n",
    "        \n",
    "    ##########################################\n",
    "    \n",
    "    # compute features\n",
    "\n",
    "    d_features = OrderedDict()\n",
    "    d_features[\"mean_weight_raw_img\"] = []\n",
    "    d_features[\"mean_edge_weight_horizontal\"] = []\n",
    "    d_features[\"mean_edge_weight_vertical\"] = []\n",
    "\n",
    "    l_ptID = np.sort(list(d_images_from_pl.keys()))\n",
    "    for ptID in l_ptID:\n",
    "        img = np.array(\n",
    "                d_images_from_pl[\n",
    "                    ptID]\n",
    "                        [0] # use the first image for the pt\n",
    "                )\n",
    "        # feature extraction\n",
    "\n",
    "        ## calculating horizontal edges using prewitt kernel\n",
    "        edges_prewitt_horizontal = prewitt_h(img[:,:,0])\n",
    "        ## calculating vertical edges using prewitt kernel\n",
    "        edges_prewitt_vertical = prewitt_v(img[:,:,0])\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_weight_raw_img = np.mean(img)\n",
    "        d_features[\"mean_weight_raw_img\"].append(mean_weight_raw_img)\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_edge_weight_horizontal = np.mean(edges_prewitt_horizontal)\n",
    "        d_features[\"mean_edge_weight_horizontal\"].append(mean_edge_weight_horizontal)\n",
    "\n",
    "        ## Feature: mean values \n",
    "        mean_edge_weight_vertical = np.mean(edges_prewitt_vertical)\n",
    "        d_features[\"mean_edge_weight_vertical\"].append(mean_edge_weight_vertical)\n",
    "\n",
    "    df_features_combined = pd.DataFrame([])\n",
    "    df_features_combined['bcr_patient_barcode'] = l_ptID\n",
    "    for feature_name in d_features.keys():\n",
    "        df_features_combined[feature_name] = d_features[feature_name]\n",
    "\n",
    "    # output patient ID list\n",
    "    df_patient_id_list = df_features_combined[['bcr_patient_barcode']]\n",
    "    df_patient_id_list.to_csv(outfile_patient_id_list, header = True, index = False)\n",
    "\n",
    "    df_full_patient_id_list = pd.read_csv(file_path_full_patient_id_list)\n",
    "    df_full_patient_class_labels = pd.read_csv(file_path_full_patient_class_labels)\n",
    "    \n",
    "    ## isolate patients that have images\n",
    "    df_slice_full_patient_id_list_have_images = df_full_patient_id_list[df_full_patient_id_list['bcr_patient_barcode'].isin(df_patient_id_list['bcr_patient_barcode'])]\n",
    "    idx_in_full_patient_id_list_have_images = df_slice_full_patient_id_list_have_images.index\n",
    "\n",
    "    df_patient_id_class_labels_pts_with_images = df_full_patient_class_labels[df_full_patient_class_labels['bcr_patient_barcode'].isin(df_slice_full_patient_id_list_have_images['bcr_patient_barcode'])] # remove pts in master list that don't have images\n",
    "    df_patient_id_class_labels_pts_with_images = df_patient_id_class_labels_pts_with_images[df_patient_id_class_labels_pts_with_images['bcr_patient_barcode'].isin(df_full_patient_id_list['bcr_patient_barcode'])] # remove pts with images but not in master list\n",
    "    \n",
    "#    df_class_labels_pts_with_images = df_full_patient_class_labels.loc[idx_in_full_patient_id_list_have_images]\n",
    "    df_class_labels_pts_with_images = df_patient_id_class_labels_pts_with_images[user_input_class_label_column_name] #isolate class label col\n",
    "    df_pt_id_with_images = df_patient_id_class_labels_pts_with_images['bcr_patient_barcode'] #isolate pt id col\n",
    "    \n",
    "    # save class labels pts with images\n",
    "    df_class_labels_pts_with_images.to_csv(outfile_class_labels_pts_with_images, header=True, index=False)\n",
    "    df_patient_id_class_labels_pts_with_images.to_csv(outfile_patient_id_list, header=True, index=False)\n",
    "\n",
    "\n",
    "    ## filter out patients in df_features_combined that are not in the list of patients with images\n",
    "    df_features_combined = df_features_combined[df_features_combined['bcr_patient_barcode'].isin(df_patient_id_class_labels_pts_with_images['bcr_patient_barcode'])]\n",
    "    \n",
    "    # output target class labels for this cohort of patients (not all patients in full\n",
    "    #  cohort will have images)\n",
    "\n",
    "    del df_features_combined['bcr_patient_barcode']\n",
    "    df_features_combined.to_csv(outfile_features_df, header = True, index = False)\n",
    "\n",
    "    \n",
    "    ## create train test and store indexes, using these clinical features   \n",
    "    param_test_set_size_float = np.float64(param_test_set_size)\n",
    "    ### set a valid value of 0.25 for test set size if needed\n",
    "    if param_test_set_size_float <= 0 or param_test_set_size_float >= 1:        \n",
    "        param_test_set_size_float = 0.25\n",
    "    \n",
    "    # set random seed = passed down parameter\n",
    "    param_random_seed_int = int(np.float64(param_random_seed))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features_combined, df_class_labels_pts_with_images, \n",
    "                                                        test_size=param_test_set_size_float,\n",
    "                                                        random_state=param_random_seed_int) # IMPT: set random seed\\\\n \n",
    "                                                        \n",
    "    df_idx_train = pd.DataFrame([])\n",
    "    df_idx_train['bcr_patient_barcode'] = df_slice_full_patient_id_list_have_images.iloc[X_train.index]  \n",
    "    df_idx_test = X_test.index.to_frame().rename(columns={0: \"index\"})\n",
    "    df_idx_test['bcr_patient_barcode'] = df_slice_full_patient_id_list_have_images.iloc[X_test.index]   \n",
    "    ## save training and testing splits - to be used for instances where just these image data are used in expt\\\\n    \n",
    "    df_idx_train.to_csv(output_csv_train_indexes, index = False, header = True) \n",
    "    df_idx_test.to_csv(output_csv_test_indexes, index = False, header = True)\n",
    "    \n",
    "create_step_image_feature_construction_histomicstk = kfp.components.create_component_from_func(\n",
    "    func=construct_features_images_histomicstk,\n",
    "    output_component_file='component_image_feature_construction_pathml.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.9', # don't use 3.10 b/c conda has bug and doesn't work for 3.10 yet\n",
    "    packages_to_install=[\n",
    "                         #'gdal-utils',\n",
    "                         #'gdal',\n",
    "                         #'pygdal', # ERROR: Could not find a version that satisfies the requirement libgdal==3.4.1 (from versions: none)\n",
    "#                         \"https://anaconda.org/conda-forge/libgdal/3.4.2/download/linux-64/libgdal-3.4.2-hb785293_6.tar.bz2\",\n",
    "#                         'histomicstk --find-links https://girder.github.io/large_image_wheels',\n",
    "#                         'histomicstk',                         \n",
    "                         'cytoolz==0.11.2',\n",
    "                         'conda==4.3.16',\n",
    "                         'large_image==1.14.3',\n",
    "                         'numpy==1.22.3',\n",
    "                         'pandas==1.4.2',\n",
    "                         'scikit-image==0.19.2'   #,\n",
    "#                         'scipy==1.7.3'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6485e",
   "metadata": {},
   "source": [
    "## component: combine feature domains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29d2ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature_domains(file_path_image_fx: comp.InputPath('CSV'),\n",
    "                            file_path_clinical_fx: comp.InputPath('CSV'),\n",
    "                            file_path_image_patient_id: comp.InputPath('CSV'),\n",
    "                            file_path_clinical_patient_id: comp.InputPath('CSV'),\n",
    "                            param_test_set_size: str,\n",
    "                            param_random_seed: str,\n",
    "                            outfile_master_features_df: comp.OutputPath('CSV'),\n",
    "                            outfile_master_patient_id_list: comp.OutputPath('CSV'),\n",
    "                            output_csv_train_indexes: comp.OutputPath('CSV'),\n",
    "                            output_csv_test_indexes: comp.OutputPath('CSV'),\n",
    "                            output_csv_master_feature_list: comp.OutputPath('CSV')):\n",
    "    \n",
    "    \"\"\"Combination of features (concatenation) for early fusion model. \n",
    "    \n",
    "    Args:\n",
    "        file_path_image_fx: \n",
    "        file_path_clinical_fx:\n",
    "        file_path_image_patient_id:\n",
    "        file_path_clinical_patient_id: \n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df_image_fx = pd.read_csv(file_path_image_fx)\n",
    "    df_clinical_fx = pd.read_csv(file_path_clinical_fx)\n",
    "    df_image_patient_id = pd.read_csv(file_path_image_patient_id)\n",
    "    df_clinical_patient_id = pd.read_csv(file_path_clinical_patient_id)\n",
    "    \n",
    "    # slap on patient id column as first column\n",
    "    df_image_table_with_patient_id = pd.concat([df_image_patient_id, df_image_fx],\n",
    "                                               axis=1)\n",
    "    df_clinical_table_with_patient_id = pd.concat([df_clinical_patient_id, df_clinical_fx],\n",
    "                                                  axis=1)\n",
    "\n",
    "    # merge data domains\n",
    "    df_master_features = df_image_table_with_patient_id.merge(df_clinical_table_with_patient_id,\n",
    "                                                              how='outer',\n",
    "                                                              on='bcr_patient_barcode')\n",
    "\n",
    "    # create train test and store indexes, using these features\n",
    "    param_test_set_size_float = np.float64(param_test_set_size)\n",
    "    ## set a valid value of 0.25 for test set size if needed\n",
    "    if param_test_set_size_float <= 0 or param_test_set_size_float >= 1:        \n",
    "        param_test_set_size_float = 0.25\n",
    "    param_random_seed_int = int(np.float64(param_random_seed))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_master_features,\n",
    "                                                        df_master_features[df_master_features.columns[-1]],\n",
    "                                                        test_size=param_test_set_size_float,\n",
    "                                                        random_state=param_random_seed_int) \n",
    "    # IMPT: set random seed\\\\n    \n",
    "    df_idx_train = X_train.index.to_frame().rename(columns={0: \"index\"})    \n",
    "    df_idx_train['bcr_patient_barcode'] = df_master_features.iloc[X_train.index]['bcr_patient_barcode']\n",
    "    df_idx_test = X_test.index.to_frame().rename(columns={0: \"index\"})\n",
    "    df_idx_test['bcr_patient_barcode'] = df_master_features.iloc[X_test.index]['bcr_patient_barcode']\n",
    "\n",
    "\n",
    "    # write outputs\n",
    "    df_master_patient_id_list = df_master_features[['bcr_patient_barcode']]\n",
    "    del df_master_features['bcr_patient_barcode']\n",
    "    \n",
    "    ## feature matrix and patient id list\n",
    "    df_master_features.to_csv(outfile_master_features_df, header=True, index=False)\n",
    "    df_master_patient_id_list.to_csv(outfile_master_patient_id_list, header=True, index=False)\n",
    "    ## list of feature names\n",
    "    l_feature_names_images = list(set(df_image_fx.columns).intersection(df_master_features.columns))\n",
    "    df_feature_list = pd.DataFrame([])\n",
    "    df_feature_list['feature_name'] = l_feature_names_images\n",
    "    df_feature_list.to_csv(output_csv_master_feature_list, header=True, index=False)\n",
    "    ## save training and testing splits - to use with all fused feature matrices from now on\n",
    "    df_idx_train.to_csv(output_csv_train_indexes, index = False, header = True)\n",
    "    df_idx_test.to_csv(output_csv_test_indexes, index = False, header = True)\n",
    "    \n",
    "    \n",
    "create_step_combine_feature_domains = kfp.components.create_component_from_func(\n",
    "    func=combine_feature_domains,\n",
    "    output_component_file='component_combine_feature_domains.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2eeec",
   "metadata": {},
   "source": [
    "### Component: Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3360afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_unknown(file_path: comp.InputPath('CSV'),\n",
    "                   output_csv: comp.OutputPath('CSV')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    # Read in CSV\n",
    "    df = pd.read_csv(filepath_or_buffer=file_path)\n",
    "    \n",
    "    # Impute: most common\n",
    "    imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    imp_most_frequent.fit(df)\n",
    "    nparr_imputed = imp_most_frequent.transform(df)\n",
    "    df_imputed = pd.DataFrame(nparr_imputed)\n",
    "    df_imputed.columns = df.columns\n",
    "    \n",
    "    # Output to CSV\n",
    "    df_imputed.to_csv(output_csv, index = False, header = True)\n",
    "\n",
    "create_step_dp_impute_unknown = kfp.components.create_component_from_func(\n",
    "    func=impute_unknown,\n",
    "    output_component_file='component_dp_impute_unknown.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240804c",
   "metadata": {},
   "source": [
    "### Component: Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c88e9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df(file_path: comp.InputPath('CSV'),\n",
    "             output_csv: comp.OutputPath('CSV')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Read in CSV\n",
    "    df = pd.read_csv(filepath_or_buffer=file_path)\n",
    "    \n",
    "    # scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df)\n",
    "    nparr_scaled_data = scaler.transform(df)\n",
    "    \n",
    "    df_scaled = pd.DataFrame(nparr_scaled_data)\n",
    "    df_scaled.columns = df.columns\n",
    "    \n",
    "    # Output to CSV\n",
    "    df_scaled.to_csv(output_csv, index = False, header = True)\n",
    "\n",
    "create_step_dp_scale_df = kfp.components.create_component_from_func(\n",
    "    func=scale_df,\n",
    "    output_component_file='component_dp_scale_df.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                        'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7c0390",
   "metadata": {},
   "source": [
    "### Component: create model inputs: X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "779f7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(file_path_features: comp.InputPath('CSV'),\n",
    "                        file_path_class_labels: comp.InputPath('CSV'),\n",
    "                        test_set_size: str,\n",
    "                        param_random_seed: str,                     \n",
    "                        file_path_train_indexes: comp.InputPath('CSV'),                 \n",
    "                        file_path_test_indexes: comp.InputPath('CSV'),\n",
    "                        output_X_train: comp.OutputPath('CSV'),\n",
    "                        output_X_test: comp.OutputPath('CSV'),\n",
    "                        output_y_train: comp.OutputPath('CSV'),\n",
    "                        output_y_test: comp.OutputPath('CSV')):\n",
    "    \"\"\"Create train and test sets\n",
    "    \n",
    "    Args:\n",
    "        file_path_features: A string containing path / pipeline component output\n",
    "        file_path_class_labels: A string containing path to data / pipeline component output\n",
    "        test_set_size: [optional, default=0.25] proportion to use for the test set\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    \n",
    "    X = pd.read_csv(filepath_or_buffer=file_path_features)\n",
    "    y = pd.read_csv(filepath_or_buffer=file_path_class_labels)\n",
    "    # load train/test indexes - computed with combine feature domains task\n",
    "    df_train_indexes = pd.read_csv(file_path_train_indexes)\n",
    "    df_test_indexes = pd.read_csv(file_path_test_indexes)\n",
    "    nparr_train_indexes = np.array(df_train_indexes['index'])\n",
    "    nparr_test_indexes = np.array(df_test_indexes['index'])\n",
    "\n",
    "    X_train = X.iloc[nparr_train_indexes]\n",
    "    y_train = y.iloc[nparr_train_indexes]\n",
    "    X_test = X.iloc[nparr_test_indexes]\n",
    "    y_test = y.iloc[nparr_test_indexes]\n",
    "\n",
    "    X_train.to_csv(output_X_train, header=True, index=False)\n",
    "    X_test.to_csv(output_X_test, header=True, index=False)\n",
    "    y_train.to_csv(output_y_train, header=True, index=False)\n",
    "    y_test.to_csv(output_y_test, header=True, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "create_step_dp_create_train_test = kfp.components.create_component_from_func(\n",
    "    func=create_model_inputs,\n",
    "    output_component_file='component_dp_create_train_test.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db89b50",
   "metadata": {},
   "source": [
    "## 3 model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27642eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(file_path_x_train: comp.InputPath('CSV'), \n",
    "                file_path_y_train: comp.InputPath('CSV'),\n",
    "                param_random_seed: str,\n",
    "                feature_selection_method: str,\n",
    "                output_model: comp.OutputPath('Model'),\n",
    "                output_features_selected: comp.OutputPath('CSV')):\n",
    "\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import mlflow.sklearn    \n",
    "    import pandas as pd    \n",
    "    from sklearn.linear_model import LogisticRegression    \n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.feature_selection import chi2, RFE, SelectKBest, SelectPercentile, SequentialFeatureSelector, VarianceThreshold\n",
    "\n",
    "    # set the seed \n",
    "    np.random.seed(int(np.float64(param_random_seed)))    \n",
    "    X_train = pd.read_csv(file_path_x_train)\n",
    "    y_train_raw = pd.read_csv(file_path_y_train) \n",
    "    y_train=y_train_raw[y_train_raw.columns[0]]  # grab first column\n",
    "\n",
    "    # feature selection    \n",
    "    ## define feature selection method\n",
    "    if feature_selection_method == 'VarianceThreshold':\n",
    "        feature_selector = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "        X_train_after_fs = feature_selector.fit_transform(X_train)  \n",
    "    elif feature_selection_method == 'f_classif':\n",
    "        feature_selector = SelectPercentile(percentile=20)\n",
    "        X_train_after_fs = feature_selector.fit_transform(X_train, y_train)\n",
    "    elif feature_selection_method == 'chi2': \n",
    "        # NOTE: need to have non-negative features (e.g., binary, count)\n",
    "        feature_selector = SelectPercentile(chi2, percentile=20)   \n",
    "        X_train_after_fs = feature_selector.fit_transform(X_train, y_train)\n",
    "    elif feature_selection_method == 'SequentialFeatureSelector_backward':\n",
    "        feature_selector = SequentialFeatureSelector(estimator=LogisticRegression(),\n",
    "        n_features_to_select = 'auto',      \n",
    "        cv =10,                             \n",
    "        direction ='backward')\n",
    "        feature_selector.fit(X_train, y_train)\n",
    "\n",
    "        X_train.columns[feature_selector.get_support()]\n",
    "        X_train_after_fs = feature_selector.transform(X_train)\n",
    "    elif feature_selection_method == 'SequentialFeatureSelector_forward':\n",
    "        feature_selector = SequentialFeatureSelector(estimator=LogisticRegression(),\n",
    "        n_features_to_select = 'auto', cv =10, direction ='forward')\n",
    "\n",
    "        feature_selector.fit(X_train, y_train)       \n",
    "        X_train.columns[feature_selector.get_support()]\n",
    "        X_train_after_fs = feature_selector.transform(X_train)\n",
    "\n",
    "    else: # set default FS method (choose all fx) if none of the coded FS methods are used as input\n",
    "        feature_selector = SelectKBest(k='all') # by default, choose all features i.e., k='all'       \n",
    "        X_train_after_fs = feature_selector.fit_transform(X_train, y_train)\n",
    "\n",
    "    features_selected = feature_selector.get_feature_names_out(X_train.columns)\n",
    "\n",
    "\n",
    "    df_features_selected = pd.DataFrame(features_selected).rename(columns={0: \"feature_selected\"})\n",
    "    print(\"features selected:\", list(df_features_selected['feature_selected']))\n",
    "    # fit model\n",
    "    ## define model\n",
    "    model = LogisticRegression(verbose=1,penalty='l2',tol=1e-4,C=1.0,\n",
    "                               class_weight='balanced',solver='lbfgs',\n",
    "                               multi_class='ovr') # one vs rest\n",
    "    model.fit(X_train_after_fs, y_train)\n",
    "\n",
    "    # save model  \n",
    "    mlflow.sklearn.save_model(model, output_model,\n",
    "    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n",
    "\n",
    "    # log model\n",
    "    mlflow.sklearn.log_model(model, \"sklearn_models\")\n",
    "    # save features\n",
    "    df_features_selected.to_csv(output_features_selected, index = False, header = True)\n",
    "    \n",
    "\n",
    "create_step_train_model = kfp.components.create_component_from_func(\n",
    "    func=train_model,\n",
    "    output_component_file='component_train_model.yaml', \n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['joblib==1.1.0',\n",
    "                         'mlflow==1.24.0',\n",
    "                         'pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2',\n",
    "                         'protobuf==3.20.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a4be1",
   "metadata": {},
   "source": [
    "## 4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b4d142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(file_path_x_test: comp.InputPath('CSV'),\n",
    "               file_path_y_test: comp.InputPath('CSV'),\n",
    "               file_path_model: comp.InputPath('Model'),\n",
    "               file_path_features_selected: comp.InputPath('CSV'),\n",
    "               output_json: comp.OutputPath('JSON'),\n",
    "               output_csv:  comp.OutputPath('CSV')):\n",
    "    import joblib\n",
    "    import json    \n",
    "    import mlflow.sklearn\n",
    "    import numpy as np    \n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    \n",
    "    class NumpyArrayEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "        \n",
    "    # load features selected:\n",
    "    df_features_selected = pd.read_csv(file_path_features_selected)\n",
    "    # load trained model\n",
    "    trained_model = mlflow.sklearn.load_model(file_path_model)\n",
    "    # load test data\n",
    "    X_test = pd.read_csv(file_path_x_test)\n",
    "    y_test = np.array(pd.read_csv(file_path_y_test))\n",
    "    ## filter X_test to only include features selected for model\n",
    "    X_test = X_test[list(df_features_selected['feature_selected'])]\n",
    "    \n",
    "    # Get predictions    \n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    #############################################################\n",
    "    # Get accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Confusion matrix - use labels\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # AUC score\n",
    "    sum_y_test_axis_0 = np.sum(y_test, axis=0)\n",
    "    col_idx_y_test_nnz = np.where(sum_y_test_axis_0 > 0)[0]\n",
    "\n",
    "    # compute AUC\n",
    "    ## check if multi-class\n",
    "    num_classes = np.max([len(np.unique(y_pred)), len(np.unique(y_test))])\n",
    "\n",
    "    if num_classes > 2: # one-hot encode      \n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(np.concatenate((np.array(y_test).ravel(), np.array(y_pred).ravel())))\n",
    "        y_test_onehot = lb.fit_transform(y_test)\n",
    "        y_pred_onehot = lb.fit_transform(y_pred)\n",
    "    else:\n",
    "        y_test_onehot = y_test\n",
    "        y_pred_onehot = y_pred\n",
    "    \n",
    "    auc_test = roc_auc_score(y_test_onehot,                            \n",
    "                             y_pred_onehot,\n",
    "                             multi_class=trained_model.multi_class) # specify multi-class method\n",
    "    # output: JSON\n",
    "    d_output = {}\n",
    "\n",
    "    ## add in model results\n",
    "    d_output['model_results'] = {}\n",
    "    d_output['model_results']['model'] = trained_model.get_params()\n",
    "    d_output['model_results']['y_test'] = np.array(y_test).ravel()\n",
    "    d_output['model_results']['y_pred'] = y_pred\n",
    "    d_output['model_results']['accuracy'] = accuracy\n",
    "    d_output['model_results']['auc_test'] = auc_test\n",
    "    d_output['model_results']['cm'] = cm\n",
    "    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n",
    "\n",
    "    ## write json output: results\n",
    "    with open(output_json, 'w') as outfile:\n",
    "        outfile.write(json_string_output)\n",
    "\n",
    "    ## write csv output: results\n",
    "    df_output_csv = pd.DataFrame({'metric': d_output['model_results'].keys(),\n",
    "                                  'value':  d_output['model_results'].values()})\n",
    "\n",
    "    df_output_csv.to_csv(output_csv, index = False, header = True)\n",
    "\n",
    "\n",
    "    \n",
    "create_step_test_model = kfp.components.create_component_from_func(\n",
    "    func=test_model,\n",
    "    output_component_file='component_test_model.yaml', \n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['joblib==1.1.0',\n",
    "                         'mlflow==1.24.0',\n",
    "                         'pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2',\n",
    "                         'protobuf==3.20.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4fa816",
   "metadata": {},
   "source": [
    "## 5 Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6fd47",
   "metadata": {},
   "source": [
    "### Final Pipeline GBM v3: early fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dca14bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_GBM_v3_early_fusion_clinical_images(user_input_class_label_column_name,\n",
    "                                           s_colnames_to_exclude,\n",
    "                                           test_set_size,\n",
    "                                           param_random_seed,\n",
    "                                           feature_selection_method):\n",
    "    \"\"\"Pipeline: Download data, data processing\n",
    "        1. download data and make master table\n",
    "        2. data processing\n",
    "        3. train model\n",
    "    \n",
    "    Args:\n",
    "        user_input_class_label_column_name: name of column to use as class label (i.e., vital_status)\n",
    "        s_colnames_to_exclude: other columns to exclude, other than the ones automatically \n",
    "                               filtered out in make_dfs(); separate colnames by ','\n",
    "    \"\"\"\n",
    "    \n",
    "    # PL1: FULL PIPELINE - Clinical data only\n",
    "    url = 'https://wiki.cancerimagingarchive.net/download/attachments/1966258/gdc_download_clinical_gbm.tar.gz'\n",
    "    web_downloader_task = component_download_file(url)\n",
    "    make_dfs = create_step_process_data_tarball(file=web_downloader_task.outputs['data'])\n",
    "    dp_get_dummies_task = create_step_dp_get_dummies(file=make_dfs.outputs['output_master_df'],\n",
    "                                                     file_path_patient_list=make_dfs.outputs['output_patient_id_list'],\n",
    "                                                     class_label_colname=user_input_class_label_column_name, \n",
    "                                                     s_colnames_to_exclude=s_colnames_to_exclude,\n",
    "                                                     param_test_set_size=test_set_size,\n",
    "                                                     param_random_seed=param_random_seed)\n",
    "    dp_impute_task = create_step_dp_impute_unknown(file=dp_get_dummies_task.outputs['output_csv_features'])\n",
    "    dp_scale_task = create_step_dp_scale_df(file=dp_impute_task.outputs['output_csv'])\n",
    "    dp_create_train_test = create_step_dp_create_train_test(file_path_features=dp_scale_task.outputs['output_csv'], \n",
    "                                                            file_path_class_labels=dp_get_dummies_task.outputs['output_csv_target'],\n",
    "                                                            test_set_size=test_set_size,\n",
    "                                                            file_path_train_indexes=dp_get_dummies_task.outputs['output_csv_train_indexes'],\n",
    "                                                            file_path_test_indexes=dp_get_dummies_task.outputs['output_csv_test_indexes'],        \n",
    "                                                            param_random_seed=param_random_seed)\n",
    "    train_model_task = create_step_train_model(file_path_x_train=dp_create_train_test.outputs['output_X_train'],\n",
    "                                               file_path_y_train=dp_create_train_test.outputs['output_y_train'],\n",
    "                                               param_random_seed=param_random_seed,\n",
    "                                               feature_selection_method=feature_selection_method)\n",
    "    test_model_task = create_step_test_model(file_path_x_test=dp_create_train_test.outputs['output_X_test'],\n",
    "                                             file_path_y_test=dp_create_train_test.outputs['output_y_test'],\n",
    "                                             file_path_model=train_model_task.outputs['output_model'],\n",
    "                                             file_path_features_selected=train_model_task.outputs['output_features_selected'])\n",
    "    \n",
    "    \n",
    "    # DATA ACQUISITION: image features\n",
    "    get_images_task = create_step_download_images(file_path_patient_list=make_dfs.outputs['output_patient_id_list'])\n",
    "\n",
    "    # FEATURE CONSTRUCTION: images - skimage\n",
    "    create_image_features_task = create_step_image_feature_construction_image_fx(file_path_images=get_images_task.outputs['outfile_images_json'],\n",
    "                                                                                 file_path_full_patient_id_list=dp_get_dummies_task.outputs['output_csv_patient_list_filtered'],\n",
    "                                                                                 file_path_full_patient_class_labels=dp_get_dummies_task.outputs['output_csv_target'])\n",
    "    \n",
    "#     # FEATURE CONSTRUCTION: images - pathml\n",
    "#     create_image_features_pathml_task = create_step_image_feature_construction_pathml(file_path_images=get_images_task.outputs['outfile_images_json'],\n",
    "#                                                                                  file_path_full_patient_id_list=dp_get_dummies_task.outputs['output_csv_patient_list_filtered'],\n",
    "#                                                                                  file_path_full_patient_class_labels=dp_get_dummies_task.outputs['output_csv_target'])\n",
    "    \n",
    "    # FEATURE CONSTRUCTION: images - histomicstk\n",
    "    create_image_features_histomicstk_task = create_step_image_feature_construction_histomicstk(file_path_images=get_images_task.outputs['outfile_images_json'],\n",
    "                                                                                 file_path_full_patient_id_list=dp_get_dummies_task.outputs['output_csv_patient_list_filtered'],\n",
    "                                                                                 file_path_full_patient_class_labels=dp_get_dummies_task.outputs['output_csv_target'],\n",
    "                                                                                 user_input_class_label_column_name=user_input_class_label_column_name, \n",
    "                                                                                 param_test_set_size=test_set_size,\n",
    "                                                                                 param_random_seed=param_random_seed)\n",
    "    \n",
    "    # DATA PREPARATION: fusion image and clinical data feature domains\n",
    "    task_combine_feature_domains = create_step_combine_feature_domains(file_path_image_fx=create_image_features_task.outputs['outfile_features_df'],\n",
    "                                                                       file_path_clinical_fx=dp_get_dummies_task.outputs['output_csv_features'],\n",
    "                                                                       file_path_image_patient_id=create_image_features_task.outputs['outfile_patient_id_list'],\n",
    "                                                                       file_path_clinical_patient_id=dp_get_dummies_task.outputs['output_csv_patient_list_filtered'],\n",
    "                                                                       param_test_set_size=test_set_size,\n",
    "                                                                       param_random_seed=param_random_seed)\n",
    "    # PL2: MODELING PORTION OF PIPELINE: Image data only\n",
    "    task_images_only_impute = create_step_dp_impute_unknown(file=create_image_features_task.outputs['outfile_features_df'])\n",
    "    task_images_only_scale = create_step_dp_scale_df(file=task_images_only_impute.outputs['output_csv'])    \n",
    "    task_images_only_create_train_test = create_step_dp_create_train_test(file_path_features=task_images_only_scale.outputs['output_csv'], \n",
    "                                                                          file_path_class_labels=create_image_features_task.outputs['outfile_class_labels_pts_with_images'], #note: the image data \n",
    "                                                                             # acquisition component currently assumes only patients that\n",
    "                                                                             # have clinical data are searched for images; ie no patients \n",
    "                                                                             # that would have images but no clinical data\n",
    "                                                                          file_path_train_indexes=create_image_features_histomicstk_task.outputs['output_csv_train_indexes'],\n",
    "                                                                          file_path_test_indexes=create_image_features_histomicstk_task.outputs['output_csv_test_indexes'],\n",
    "                                                                          test_set_size=test_set_size,\n",
    "                                                                          param_random_seed=param_random_seed)\n",
    "    task_images_only_train_model = create_step_train_model(file_path_x_train=task_images_only_create_train_test.outputs['output_X_train'],\n",
    "                                                           file_path_y_train=task_images_only_create_train_test.outputs['output_y_train'],\n",
    "                                                           param_random_seed=param_random_seed,\n",
    "                                                           feature_selection_method=feature_selection_method)\n",
    "    task_images_only_test_model = create_step_test_model(file_path_x_test=task_images_only_create_train_test.outputs['output_X_test'],\n",
    "                                                          file_path_y_test=task_images_only_create_train_test.outputs['output_y_test'],\n",
    "                                                          file_path_model=task_images_only_train_model.outputs['output_model'],\n",
    "                                                          file_path_features_selected=task_images_only_train_model.outputs['output_features_selected'])\n",
    "\n",
    "\n",
    "    \n",
    "    # PL3: MODELING PORTION OF PIPELINE: FUSION (Image + Clinical data)\n",
    "    task_early_fusion_impute = create_step_dp_impute_unknown(file=task_combine_feature_domains.outputs['outfile_master_features_df'])\n",
    "    task_early_fusion_scale = create_step_dp_scale_df(file=task_early_fusion_impute.outputs['output_csv'])    \n",
    "    task_early_fusion_create_train_test = create_step_dp_create_train_test(file_path_features=task_early_fusion_scale.outputs['output_csv'], \n",
    "                                                                           file_path_class_labels=dp_get_dummies_task.outputs['output_csv_target'], #note: the image data \n",
    "                                                                             # acquisition component currently assumes only patients that\n",
    "                                                                             # have clinical data are searched for images; ie no patients \n",
    "                                                                             # that would have images but no clinical data\n",
    "                                                                          file_path_train_indexes=task_combine_feature_domains.outputs['output_csv_train_indexes'],\n",
    "                                                                          file_path_test_indexes=task_combine_feature_domains.outputs['output_csv_test_indexes'],                                                                           \n",
    "                                                                           test_set_size=test_set_size, \n",
    "                                                                           param_random_seed=param_random_seed)\n",
    "    task_early_fusion_train_model = create_step_train_model(file_path_x_train=task_early_fusion_create_train_test.outputs['output_X_train'],\n",
    "                                                            file_path_y_train=task_early_fusion_create_train_test.outputs['output_y_train'],\n",
    "                                                            param_random_seed=param_random_seed,\n",
    "                                                            feature_selection_method=feature_selection_method)\n",
    "    task_early_fusion_test_model = create_step_test_model(file_path_x_test=task_early_fusion_create_train_test.outputs['output_X_test'],\n",
    "                                                          file_path_y_test=task_early_fusion_create_train_test.outputs['output_y_test'],\n",
    "                                                          file_path_model=task_early_fusion_train_model.outputs['output_model'],\n",
    "                                                          file_path_features_selected=task_early_fusion_train_model.outputs['output_features_selected'])\n",
    "    \n",
    "    \n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl_GBM_v3_early_fusion_clinical_images,\n",
    "    package_path='pl_GBM_v3_early_fusion_clinical_images.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab54ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99f657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50d113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
