{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f93c81e",
   "metadata": {},
   "source": [
    "# build kubeflow pipeline with python SDK\n",
    "\n",
    "\n",
    "* Leverage python SDK \n",
    "* Create components and pipelines (YAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7127ec",
   "metadata": {},
   "source": [
    "## parameters - user defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85e5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "770af401",
   "metadata": {},
   "source": [
    "## required installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717b298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /Users/user1/miniconda3/lib/python3.9/site-packages (1.8.4)\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py<2,>=0.9 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.11.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (2.7.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.44.0)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (18.20.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.12.11)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.35.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.9.1)\n",
      "Collecting cloudpickle<3,>=2.0.0\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.8.1)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.2.13)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.1.10)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.13)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
      "  Downloading kfp_pipeline_spec-0.1.14-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (3.19.4)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (3.0.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.9.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: six in /Users/user1/miniconda3/lib/python3.9/site-packages (from absl-py<2,>=0.9->kfp) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/user1/miniconda3/lib/python3.9/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /Users/user1/miniconda3/lib/python3.9/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.55.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.27.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.20.4)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (58.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (4.2.4)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.2.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp) (3.0.6)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.1)\n",
      "Requirement already satisfied: certifi in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.15 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/user1/miniconda3/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.2.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/user1/miniconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/user1/miniconda3/lib/python3.9/site-packages (from pydantic<2,>=1.8.2->kfp) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user1/miniconda3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.0.10)\n",
      "Requirement already satisfied: wheel in /Users/user1/miniconda3/lib/python3.9/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.2.0)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=837578a7efdf21cc044031ce45450a87757f4520eced6aa4b26f1d2ad43e2fc3\n",
      "  Stored in directory: /Users/user1/Library/Caches/pip/wheels/e0/a3/8b/897768051e54a03d4ad9d8211e100fc4cc5439d20743d8ab25\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp-pipeline-spec, cloudpickle, kfp\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.1.13\n",
      "    Uninstalling kfp-pipeline-spec-0.1.13:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.1.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.8.4\n",
      "    Uninstalling kfp-1.8.4:\n",
      "      Successfully uninstalled kfp-1.8.4\n",
      "Successfully installed cloudpickle-2.0.0 kfp-1.8.12 kfp-pipeline-spec-0.1.14\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aad804",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87db5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbee1d",
   "metadata": {},
   "source": [
    "## 1 Data acquisition and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ef96475",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load component from yaml\n",
    "component_download_file = kfp.components.load_component_from_file(\"./component-sdk-v2.yaml\")\n",
    "\n",
    "def process_data_tarball(file_path: comp.InputPath('Tarball'),\n",
    "                         output_patient_id_list: comp.OutputPath('CSV'),\n",
    "                         output_master_df: comp.OutputPath('CSV')):\n",
    "    \"\"\"Specific data processing of tarball downloaded.\n",
    "        - Hard code tarball content names. \n",
    "        - Assume there is MANIFEST.txt\n",
    "        - Output: \n",
    "            - output_MANIFEST: MANIFEST.txt as df\n",
    "            - output_DATA: tables in dict (keys: name as in MANIFEST.txt, values: table)\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the tarball.\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    import glob\n",
    "    import json \n",
    "    from json import JSONEncoder\n",
    "    import numpy as np\n",
    "    import tarfile\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "        \n",
    "    tarfile.open(name=file_path, mode=\"r|gz\").extractall('data_extracted')\n",
    "    l_tarball_contents = tarfile.open(name=file_path, mode=\"r|gz\").getnames()\n",
    "\n",
    "    # all dataframes\n",
    "    d_df_data = {}\n",
    "    \n",
    "    for name in l_tarball_contents:\n",
    "        archive_filename = 'data_extracted/' + name\n",
    "        if name == 'MANIFEST.txt':\n",
    "            df_manifest = pd.read_csv(glob.glob(archive_filename)[0], sep = \"\\t\")\n",
    "        else:\n",
    "            df = pd.read_csv(glob.glob(archive_filename)[0], sep = \"\\t\")\n",
    "            df.to_csv(index=False, header=True)\n",
    "            d_df_data[name] = df    \n",
    "\n",
    "    filename_patient_table = '57683e22-a8ea-4eca-bfcf-f708cf459546/nationwidechildrens.org_clinical_patient_gbm.txt'\n",
    "    filename_follow_up_table = 'c9cdbc76-105d-429b-9fce-f000819716f9/nationwidechildrens.org_clinical_follow_up_v1.0_gbm.txt'\n",
    "            \n",
    "    # 1 remove un-needed header rows for tables\n",
    "    df_patient_raw = d_df_data[filename_patient_table]\n",
    "    df_follow_up_raw = d_df_data[filename_follow_up_table]\n",
    "    \n",
    "    ## use the 2nd row as the column name (in raw data, first 3 rows are column labels)\n",
    "    ### patient table\n",
    "    df_patient = df_patient_raw.drop([1])\n",
    "    df_patient = df_patient.tail(len(df_patient) -1 )\n",
    "    df_patient = df_patient.reset_index().drop(['index'], axis = 1)\n",
    "    ### follow_up table\n",
    "    df_follow_up = df_follow_up_raw.drop([1])\n",
    "    df_follow_up = df_follow_up.tail(len(df_follow_up) -1 )\n",
    "    df_follow_up = df_follow_up.reset_index().drop(['index'], axis = 1)\n",
    "    \n",
    "    \n",
    "    # 2 data processing\n",
    "    ## 2a: data processing: clnical data\n",
    "    \n",
    "    ### patient table\n",
    "    missing_value_flags = ['[Not Available]',\n",
    "                       '[Discrepancy]',\n",
    "                       '[Unknown]',\n",
    "                       '[Not Applicable]',\n",
    "                       '[Not Evaluated]'\n",
    "                      ]\n",
    "    #### replace flags with np.nan\n",
    "    for flag in missing_value_flags:\n",
    "        df_patient = df_patient.replace(flag, np.nan)\n",
    "    \n",
    "    #### drop un-needed columns - identifiers\n",
    "    df_patient = df_patient.drop(['bcr_patient_uuid', 'form_completion_date', 'patient_id'], axis=1)\n",
    "    \n",
    "    #### parse numerical columns w/ dates\n",
    "    l_numerical_cols = [\n",
    "        'last_contact_days_to',\n",
    "        'death_days_to',\n",
    "        'age_at_initial_pathologic_diagnosis'\n",
    "    ]\n",
    "    for col in l_numerical_cols:\n",
    "        df_patient[col] = [float(x) for x in df_patient[col]]\n",
    "\n",
    "    df_patient['survival_time_yrs'] = np.abs(df_patient['death_days_to']) / 365\n",
    "    df_patient['days_since_last_contact'] = np.abs(df_patient['last_contact_days_to']) / 365\n",
    "    \n",
    "    #### drop columns parsed / not needed anymore\n",
    "    l_columns_not_needed = [\n",
    "        'last_contact_days_to',\n",
    "        'birth_days_to',\n",
    "        'death_days_to',\n",
    "\n",
    "        # uninformative cols (all missing/not evaluated etc):\n",
    "        'anatomic_neoplasm_subdivision',\n",
    "        'disease_code',\n",
    "        'project_code',\n",
    "        'days_to_initial_pathologic_diagnosis',\n",
    "        'icd_10',\n",
    "        'icd_o_3_histology',\n",
    "        'icd_o_3_site',\n",
    "        'informed_consent_verified',\n",
    "        'initial_pathologic_dx_year'\n",
    "\n",
    "    ]\n",
    "    df_patient = df_patient.drop(l_columns_not_needed, axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### follow_up table\n",
    "    #### replace missing value flags with np.nan\n",
    "    missing_value_flags = ['[Not Available]',\n",
    "                           '[Discrepancy]',\n",
    "                           '[Unknown]',\n",
    "                           '[Not Applicable]',\n",
    "                           '[Not Evaluated]'\n",
    "                          ]\n",
    "    for flag in missing_value_flags:\n",
    "        df_follow_up = df_follow_up.replace(flag, np.nan)\n",
    "\n",
    "    #### drop un-needed columns - identifiers\n",
    "    df_follow_up = df_follow_up.drop(['bcr_patient_uuid', 'bcr_followup_uuid', 'form_completion_date',\n",
    "                                     'followup_reason', 'followup_lost_to'], axis=1)\n",
    "\n",
    "    #### parse numerical columns w/ dates\n",
    "    l_numerical_cols = [\n",
    "        'last_contact_days_to',\n",
    "        'death_days_to'\n",
    "    ]\n",
    "    for col in l_numerical_cols:\n",
    "        df_follow_up[col] = [float(x) for x in df_follow_up[col]]\n",
    "\n",
    "    #### isolate the LAST followup (sorted by barcode) --> get one row per patient\n",
    "    df_follow_up = df_follow_up.groupby(['bcr_patient_barcode']).tail(1)\n",
    "\n",
    "    #### drop columns parsed / not needed anymore\n",
    "    l_columns_not_needed = [\n",
    "        'last_contact_days_to',\n",
    "        'death_days_to',\n",
    "        'bcr_followup_barcode'\n",
    "    ]\n",
    "    df_follow_up = df_follow_up.drop(l_columns_not_needed, axis = 1)\n",
    "    \n",
    "    \n",
    "    # 3 Merge tables by patient identifier (create master table)\n",
    "\n",
    "    ## merge patient and followup table\n",
    "    ### find common cols: \n",
    "    l_cols_table_1 = set(df_patient.columns)\n",
    "    l_cols_table_2 = set(df_follow_up.columns)\n",
    "    intersect_cols = l_cols_table_1.intersection(l_cols_table_2)\n",
    "    l_cols_to_remove_table_1 = [x for x in intersect_cols if x != 'bcr_patient_barcode']\n",
    "\n",
    "    ### drop common cols from left table (`patient`) before join\n",
    "    df_patient = df_patient.drop(l_cols_to_remove_table_1, axis = 1)\n",
    "    ### join\n",
    "    df_master = df_patient.merge(df_follow_up, \n",
    "                                 left_on = 'bcr_patient_barcode', \n",
    "                                 right_on = 'bcr_patient_barcode', \n",
    "                                 how = 'inner')\n",
    "    ### drop patient ID identifier from master table (used as identifier, not treated as a feature)\n",
    "    df_patient_id_list = df_master[['bcr_patient_barcode']]\n",
    "    df_master = df_master.drop(['bcr_patient_barcode'], axis = 1)    \n",
    "        \n",
    "    # pl output\n",
    "    df_patient_id_list.to_csv(output_patient_id_list, index=False, header=True)\n",
    "    df_master.to_csv(output_master_df, index=False, header=True)\n",
    "    \n",
    "        \n",
    "    \n",
    "create_step_process_data_tarball = kfp.components.create_component_from_func(\n",
    "    func=process_data_tarball,\n",
    "    output_component_file='component_process_data_tarball.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db23e0",
   "metadata": {},
   "source": [
    "## 2 Data processing\n",
    "\n",
    "Further process master table\n",
    "\n",
    "- select column to be used as class label; \n",
    "- for other columns, get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "09bd59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(file_path: comp.InputPath('CSV'),\n",
    "                class_label_colname: str,\n",
    "                s_colnames_to_exclude: str,\n",
    "                output_csv_features: comp.OutputPath('CSV'),\n",
    "                output_csv_target: comp.OutputPath('CSV'),\n",
    "                output_csv_target_class_labels: comp.OutputPath('CSV'),\n",
    "                output_csv_feature_list: comp.OutputPath('CSV')):\n",
    "    \"\"\"Distribute categorical features into separate features.\n",
    "        Input: CSV with categorical (and numeric) features. Assume last \n",
    "            feature is target label. \n",
    "        Output: CSV with categorical features separated into dummies.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    df = pd.read_csv(filepath_or_buffer=file_path)\n",
    "    l_col_names = list(df.columns)\n",
    "    \n",
    "    # isolate column for target class label; if null, use last column\n",
    "    target_class_label = class_label_colname if str(class_label_colname) != '' else l_col_names[-1]\n",
    "    ## remove all rows where class label col is NaN\n",
    "    df = df.dropna(subset=[target_class_label])\n",
    "    \n",
    "    ## extract target class column\n",
    "    df_target_raw = df[target_class_label] # from input parameter\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(df_target_raw.astype(str)) # fit to data for target class to find all classes\n",
    "    target_class_label_names = lb.classes_ # store array of all the classes\n",
    "#    df_target = pd.DataFrame(np.array(lb.fit_transform(df_target_raw)))\n",
    "\n",
    "    df_target_class_labels = pd.DataFrame({'class': target_class_label_names})\n",
    "    d_target_class_label_idx = dict(zip(list(df_target_class_labels['class']), list(df_target_class_labels.index)))\n",
    "    \n",
    "    l_target_column = [d_target_class_label_idx[x] for x in df[target_class_label]]\n",
    "    df_target = pd.DataFrame([])\n",
    "    df_target[target_class_label] = l_target_column# 1-column of multiple classes\n",
    "    \n",
    "\n",
    "    \n",
    "    # create dummies for every col except class label col\n",
    "    df_features = df[[x for x in l_col_names if x != class_label_colname]] # features are all colnames except target class\n",
    "    \n",
    "    # exclude any other colnames (other than target class column), if specified\n",
    "    if str(s_colnames_to_exclude) != '':\n",
    "        l_colnames_to_exclude = s_colnames_to_exclude.split(',')\n",
    "        l_colnames_in_this_df_to_exclude = [x for x in l_colnames_to_exclude if x in df.columns]\n",
    "        df.drop(l_colnames_to_exclude, axis = 1)\n",
    "    \n",
    "    # get dummies\n",
    "    df_features_dummies = pd.get_dummies(df_features)\n",
    "    \n",
    "    # feature list\n",
    "    l_features = df_features_dummies.columns\n",
    "    df_feature_list = pd.DataFrame({'feature': l_features})\n",
    "        \n",
    "    # write outputs\n",
    "    df_features_dummies.to_csv(output_csv_features, index = False, header = True)\n",
    "    df_target.to_csv(output_csv_target, index = False, header = True)\n",
    "    df_target_class_labels.to_csv(output_csv_target_class_labels, index = True, header = False)\n",
    "    df_feature_list.to_csv(output_csv_feature_list, index = True, header = False)\n",
    "\n",
    "create_step_dp_get_dummies = kfp.components.create_component_from_func(\n",
    "    func=get_dummies,\n",
    "    output_component_file='component_dp_get_dummies.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                        'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e68fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ccc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c128af2c",
   "metadata": {},
   "source": [
    "### Component: Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "378d29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_unknown(file_path: comp.InputPath('CSV'),\n",
    "                   output_csv: comp.OutputPath('CSV')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    # Read in CSV\n",
    "    df = pd.read_csv(filepath_or_buffer=file_path)\n",
    "    \n",
    "    # Impute: most common\n",
    "    imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    imp_most_frequent.fit(df)\n",
    "    nparr_imputed = imp_most_frequent.transform(df)\n",
    "    df_imputed = pd.DataFrame(nparr_imputed)\n",
    "    df_imputed.columns = df.columns\n",
    "    \n",
    "    # Output to CSV\n",
    "    df_imputed.to_csv(output_csv, index = False, header = True)\n",
    "\n",
    "create_step_dp_impute_unknown = kfp.components.create_component_from_func(\n",
    "    func=impute_unknown,\n",
    "    output_component_file='component_dp_impute_unknown.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789df51",
   "metadata": {},
   "source": [
    "### Component: Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8a378d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df(file_path: comp.InputPath('CSV'),\n",
    "             output_csv: comp.OutputPath('CSV')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Read in CSV\n",
    "    df = pd.read_csv(filepath_or_buffer=file_path)\n",
    "    \n",
    "    # scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df)\n",
    "    nparr_scaled_data = scaler.transform(df)\n",
    "    \n",
    "    df_scaled = pd.DataFrame(nparr_scaled_data)\n",
    "    df_scaled.columns = df.columns\n",
    "    \n",
    "    # Output to CSV\n",
    "    df_scaled.to_csv(output_csv, index = False, header = True)\n",
    "\n",
    "create_step_dp_scale_df = kfp.components.create_component_from_func(\n",
    "    func=scale_df,\n",
    "    output_component_file='component_dp_scale_df.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                        'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65ebc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fba03b2",
   "metadata": {},
   "source": [
    "### Component: create model inputs: X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "eefda9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(file_path_features: comp.InputPath('CSV'),\n",
    "                        file_path_class_labels: comp.InputPath('CSV'),\n",
    "                        test_set_size: str,\n",
    "                        output_X_train: comp.OutputPath('CSV'),\n",
    "                        output_X_test: comp.OutputPath('CSV'),\n",
    "                        output_y_train: comp.OutputPath('CSV'),\n",
    "                        output_y_test: comp.OutputPath('CSV')):\n",
    "    \"\"\"Create train and test sets\n",
    "    \n",
    "    Args:\n",
    "        file_path_features: A string containing path / pipeline component output\n",
    "        file_path_class_labels: A string containing path to data / pipeline component output\n",
    "        test_set_size: [optional, default=0.25] proportion to use for the test set\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = pd.read_csv(filepath_or_buffer=file_path_features)\n",
    "    y = pd.read_csv(filepath_or_buffer=file_path_class_labels)\n",
    "    \n",
    "    try:\n",
    "        test_set_size_for_model_training = float(test_set_size)\n",
    "    except:\n",
    "        test_set_size_for_model_training = 0.25\n",
    "    if test_set_size_for_model_training <= 0 or test_set_size_for_model_training >= 1:\n",
    "        test_set_size_for_model_training = 0.25\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set_size_for_model_training)\n",
    "    \n",
    "    X_train.to_csv(output_X_train, header=True, index=False)\n",
    "    X_test.to_csv(output_X_test, header=True, index=False)\n",
    "    y_train.to_csv(output_y_train, header=True, index=False)\n",
    "    y_test.to_csv(output_y_test, header=True, index=False)\n",
    "\n",
    "    \n",
    "create_step_dp_create_train_test = kfp.components.create_component_from_func(\n",
    "    func=create_model_inputs,\n",
    "    output_component_file='component_dp_create_train_test.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f805a",
   "metadata": {},
   "source": [
    "## 3 model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "eaa1a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(file_path_x_train: comp.InputPath('CSV'), \n",
    "                file_path_y_train: comp.InputPath('CSV'),\n",
    "                output_model: comp.OutputPath('Model')):\n",
    "    import joblib\n",
    "    import mlflow.sklearn\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    \n",
    "    X_train = pd.read_csv(file_path_x_train)\n",
    "    y_train_raw = pd.read_csv(file_path_y_train)\n",
    "    \n",
    "    \n",
    "#     lb = preprocessing.LabelBinarizer()\n",
    "#     lb.fit_transform(y_train_raw.astype(str))\n",
    "#     y_class_labels = lb.classes_ #\n",
    "#     print('y_class_labels: ', y_class_labels) #\n",
    "#     y_train = lb.fit_transform(y_train_raw.astype(str))    \n",
    "    \n",
    "    y_train=y_train_raw\n",
    "    \n",
    "    model = LogisticRegression(verbose=1,\n",
    "                               penalty='l2',\n",
    "                               tol=1e-4,\n",
    "                               C=1.0,\n",
    "                               class_weight='balanced',\n",
    "                               solver='lbfgs',\n",
    "                               multi_class='ovr')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    mlflow.sklearn.save_model(model, output_model,\n",
    "                          serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n",
    "\n",
    "    # log model\n",
    "    mlflow.sklearn.log_model(model, \"sklearn_models\")\n",
    "\n",
    "create_step_train_model = kfp.components.create_component_from_func(\n",
    "    func=train_model,\n",
    "    output_component_file='component_train_model.yaml', \n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['joblib==1.1.0',\n",
    "                         'mlflow==1.24.0',\n",
    "                         'pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566923a6",
   "metadata": {},
   "source": [
    "## 4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "304469e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(file_path_x_test: comp.InputPath('CSV'),\n",
    "               file_path_y_test: comp.InputPath('CSV'),\n",
    "               file_path_model: comp.InputPath('Model'),\n",
    "               output_json: comp.OutputPath('JSON'),\n",
    "               output_csv:  comp.OutputPath('CSV')):\n",
    "    import joblib\n",
    "    import json\n",
    "    import mlflow.sklearn\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "    class NumpyArrayEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "        \n",
    "    trained_model = mlflow.sklearn.load_model(file_path_model)\n",
    "    \n",
    "    X_test = pd.read_csv(file_path_x_test)\n",
    "    y_test = np.array(pd.read_csv(file_path_y_test))\n",
    "    \n",
    "    # Get predictions \n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "    \n",
    "    \n",
    "    # Get accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Confusion matrix - use labels\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # AUC score\n",
    "    sum_y_test_axis_0 = np.sum(y_test, axis=0)\n",
    "    col_idx_y_test_nnz = np.where(sum_y_test_axis_0 > 0)[0]\n",
    "    \n",
    "    # compute AUC \n",
    "\n",
    "    ## check if multi-class\n",
    "    num_classes = np.max([len(np.unique(y_pred)), len(np.unique(y_test))])\n",
    "    if num_classes > 2: # one-hot encode\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(np.concatenate((np.array(y_test).ravel(), np.array(y_pred).ravel())))\n",
    "        y_test_onehot = lb.fit_transform(y_test)\n",
    "        y_pred_onehot = lb.fit_transform(y_pred)\n",
    "    else:\n",
    "        y_test_onehot = y_test\n",
    "        y_pred_onehot = y_pred\n",
    "        \n",
    "    auc_test = roc_auc_score(y_test_onehot,\n",
    "                             y_pred_onehot,\n",
    "                             multi_class=trained_model.multi_class) # specify multi-class method\n",
    "\n",
    "    # output: JSON\n",
    "    d_output = {}\n",
    "    ## add in model results\n",
    "    d_output['model_results'] = {}\n",
    "    d_output['model_results']['model'] = trained_model.get_params()\n",
    "    d_output['model_results']['y_test'] = np.array(y_test).ravel()\n",
    "    d_output['model_results']['y_pred'] = y_pred\n",
    "    d_output['model_results']['accuracy'] = accuracy\n",
    "    d_output['model_results']['auc_test'] = auc_test\n",
    "    d_output['model_results']['cm'] = cm\n",
    "\n",
    "    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n",
    "    \n",
    "    ## write json output: results\n",
    "    with open(output_json, 'w') as outfile:\n",
    "        outfile.write(json_string_output)\n",
    "        \n",
    "    ## write csv output: results\n",
    "    df_output_csv = pd.DataFrame({'metric': d_output['model_results'].keys(),\n",
    "                                  'value':  d_output['model_results'].values()})\n",
    "    df_output_csv.to_csv(output_csv, index = False, header = True)\n",
    "    \n",
    "create_step_test_model = kfp.components.create_component_from_func(\n",
    "    func=test_model,\n",
    "    output_component_file='component_test_model.yaml', \n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['joblib==1.1.0',\n",
    "                         'mlflow==1.24.0',\n",
    "                         'pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62242342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61000a21",
   "metadata": {},
   "source": [
    "## 5 Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b36ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbaee5e1",
   "metadata": {},
   "source": [
    "### pl04 Final Pipeline: data process, train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "396f980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_GBM_04_webdl_dp_train(user_input_class_label_column_name,\n",
    "                            s_colnames_to_exclude,\n",
    "                            test_set_size):\n",
    "    \"\"\"Pipeline: Download data, data processing\n",
    "        1. download data and make master table\n",
    "        2. data processing\n",
    "        3. train model\n",
    "    \n",
    "    Args:\n",
    "        user_input_class_label_column_name: name of column to use as class label (i.e., vital_status)\n",
    "        s_colnames_to_exclude: other columns to exclude, other than the ones automatically \n",
    "                               filtered out in make_dfs(); separate colnames by ','\n",
    "    \"\"\"\n",
    "    url = 'https://wiki.cancerimagingarchive.net/download/attachments/1966258/gdc_download_clinical_gbm.tar.gz'\n",
    "    web_downloader_task = component_download_file(url)\n",
    "    make_dfs = create_step_process_data_tarball(file=web_downloader_task.outputs['data'])\n",
    "    dp_get_dummies_task = create_step_dp_get_dummies(file=make_dfs.outputs['output_master_df'],\n",
    "                                                     class_label_colname=user_input_class_label_column_name, \n",
    "                                                     s_colnames_to_exclude=s_colnames_to_exclude)\n",
    "    dp_impute_task = create_step_dp_impute_unknown(file=dp_get_dummies_task.outputs['output_csv_features'])\n",
    "    dp_scale_task = create_step_dp_scale_df(file=dp_impute_task.outputs['output_csv'])\n",
    "    dp_create_train_test = create_step_dp_create_train_test(file_path_features=dp_scale_task.outputs['output_csv'], \n",
    "                                                            file_path_class_labels=dp_get_dummies_task.outputs['output_csv_target'],\n",
    "                                                            test_set_size=test_set_size)\n",
    "    train_model_task = create_step_train_model(file_path_x_train=dp_create_train_test.outputs['output_X_train'],\n",
    "                                               file_path_y_train=dp_create_train_test.outputs['output_y_train'])\n",
    "    test_model_task = create_step_test_model(file_path_x_test=dp_create_train_test.outputs['output_X_test'],\n",
    "                                             file_path_y_test=dp_create_train_test.outputs['output_y_test'],\n",
    "                                             file_path_model=train_model_task.outputs['output_model'])\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl_GBM_04_webdl_dp_train,\n",
    "    package_path='pl_GBM_04_webdl_dp_train_test.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac192c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
