apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pl-gbm-v3-early-fusion-clinical-images-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12, pipelines.kubeflow.org/pipeline_compilation_time: '2022-05-28T17:02:01.857712',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Pipeline: Download data,
      data processing", "inputs": [{"description": "name of column to use as class
      label (i.e., vital_status)", "name": "user_input_class_label_column_name"},
      {"description": "other columns to exclude, other than the ones automatically
      \nfiltered out in make_dfs(); separate colnames by '',''", "name": "s_colnames_to_exclude"},
      {"name": "test_set_size"}, {"name": "param_random_seed"}], "name": "Pl GBM v3
      early fusion clinical images"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12}
spec:
  entrypoint: pl-gbm-v3-early-fusion-clinical-images
  templates:
  - name: combine-feature-domains
    container:
      args: [--file-path-image-fx, /tmp/inputs/file_path_image_fx/data, --file-path-clinical-fx,
        /tmp/inputs/file_path_clinical_fx/data, --file-path-image-patient-id, /tmp/inputs/file_path_image_patient_id/data,
        --file-path-clinical-patient-id, /tmp/inputs/file_path_clinical_patient_id/data,
        --param-test-set-size, '{{inputs.parameters.test_set_size}}', --param-random-seed,
        '{{inputs.parameters.param_random_seed}}', --outfile-master-features-df, /tmp/outputs/outfile_master_features_df/data,
        --outfile-master-patient-id-list, /tmp/outputs/outfile_master_patient_id_list/data,
        --output-csv-train-indexes, /tmp/outputs/output_csv_train_indexes/data, --output-csv-test-indexes,
        /tmp/outputs/output_csv_test_indexes/data, --output-csv-master-feature-list,
        /tmp/outputs/output_csv_master_feature_list/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef combine_feature_domains(file_path_image_fx,\n                      \
        \      file_path_clinical_fx,\n                            file_path_image_patient_id,\n\
        \                            file_path_clinical_patient_id,\n            \
        \                param_test_set_size,\n                            param_random_seed,\n\
        \                            outfile_master_features_df,\n               \
        \             outfile_master_patient_id_list,\n                          \
        \  output_csv_train_indexes,\n                            output_csv_test_indexes,\n\
        \                            output_csv_master_feature_list):\n\n    \"\"\"\
        Combination of features (concatenation) for early fusion model. \n\n    Args:\n\
        \        file_path_image_fx: \n        file_path_clinical_fx:\n        file_path_image_patient_id:\n\
        \        file_path_clinical_patient_id: \n    \"\"\"\n    import numpy as\
        \ np\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\
        \n    df_image_fx = pd.read_csv(file_path_image_fx)\n    df_clinical_fx =\
        \ pd.read_csv(file_path_clinical_fx)\n    df_image_patient_id = pd.read_csv(file_path_image_patient_id)\n\
        \    df_clinical_patient_id = pd.read_csv(file_path_clinical_patient_id)\n\
        \n    # slap on patient id column as first column\n    df_image_table_with_patient_id\
        \ = pd.concat([df_image_patient_id, df_image_fx],\n                      \
        \                         axis=1)\n    df_clinical_table_with_patient_id =\
        \ pd.concat([df_clinical_patient_id, df_clinical_fx],\n                  \
        \                                axis=1)\n\n    # merge data domains\n   \
        \ df_master_features = df_image_table_with_patient_id.merge(df_clinical_table_with_patient_id,\n\
        \                                                              how='outer',\n\
        \                                                              on='bcr_patient_barcode')\n\
        \n    # create train test and store indexes, using these features\n    param_test_set_size_float\
        \ = np.float64(param_test_set_size)\n\n    ## set a valid value of 0.25 for\
        \ test set size if needed\n    if param_test_set_size_float <= 0 or param_test_set_size_float\
        \ >= 1:\n        param_test_set_size_float = 0.25    \n\n    param_random_seed_int\
        \ = int(np.float64(param_random_seed))\n    X_train, X_test, y_train, y_test\
        \ = train_test_split(df_master_features, \n                              \
        \                          df_master_features[df_master_features.columns[-1]],\
        \ \n                                                    test_size=param_test_set_size_float,\
        \ \n                                                    random_state=param_random_seed_int)\
        \ # IMPT: set random seed\n    df_idx_train = X_train.index.to_frame().rename(columns={0:\
        \ \"index\"})\n    df_idx_train['bcr_patient_barcode'] = df_master_features.iloc[X_train.index]['bcr_patient_barcode']\n\
        \    df_idx_test = X_test.index.to_frame().rename(columns={0: \"index\"})\n\
        \    df_idx_test['bcr_patient_barcode'] = df_master_features.iloc[X_test.index]['bcr_patient_barcode']\n\
        \n    # write outputs\n    df_master_patient_id_list = df_master_features[['bcr_patient_barcode']]\n\
        \    del df_master_features['bcr_patient_barcode']\n\n    ## feature matrix\
        \ and patient id list\n    df_master_features.to_csv(outfile_master_features_df,\
        \ header=True, index=False)\n    df_master_patient_id_list.to_csv(outfile_master_patient_id_list,\
        \ header=True, index=False)\n\n    ## list of feature names\n    l_feature_names_images\
        \ = list(set(df_image_fx.columns).intersection(df_master_features.columns))\
        \ \n    df_feature_list = pd.DataFrame([])\n    df_feature_list['feature_name']\
        \ = l_feature_names_images\n    df_feature_list.to_csv(output_csv_master_feature_list,\
        \ header=True, index=False)\n\n    ## save training and testing splits - to\
        \ use with all fused feature matrices from now on\n    df_idx_train.to_csv(output_csv_train_indexes,\
        \ index = False, header = True)\n    df_idx_test.to_csv(output_csv_test_indexes,\
        \ index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Combine\
        \ feature domains', description='Combination of features (concatenation) for\
        \ early fusion model.')\n_parser.add_argument(\"--file-path-image-fx\", dest=\"\
        file_path_image_fx\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--file-path-clinical-fx\", dest=\"file_path_clinical_fx\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --file-path-image-patient-id\", dest=\"file_path_image_patient_id\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-clinical-patient-id\"\
        , dest=\"file_path_clinical_patient_id\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--param-test-set-size\", dest=\"param_test_set_size\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --param-random-seed\", dest=\"param_random_seed\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--outfile-master-features-df\"\
        , dest=\"outfile_master_features_df\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--outfile-master-patient-id-list\"\
        , dest=\"outfile_master_patient_id_list\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-train-indexes\"\
        , dest=\"output_csv_train_indexes\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-test-indexes\"\
        , dest=\"output_csv_test_indexes\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-master-feature-list\"\
        , dest=\"output_csv_master_feature_list\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = combine_feature_domains(**_parsed_args)\n"
      image: python:3.9
    inputs:
      parameters:
      - {name: param_random_seed}
      - {name: test_set_size}
      artifacts:
      - {name: get-dummies-output_csv_features, path: /tmp/inputs/file_path_clinical_fx/data}
      - {name: get-dummies-output_csv_patient_list_filtered, path: /tmp/inputs/file_path_clinical_patient_id/data}
      - {name: construct-features-images-histomicstk-outfile_features_df, path: /tmp/inputs/file_path_image_fx/data}
      - {name: construct-features-images-histomicstk-outfile_patient_id_list, path: /tmp/inputs/file_path_image_patient_id/data}
    outputs:
      artifacts:
      - {name: combine-feature-domains-outfile_master_features_df, path: /tmp/outputs/outfile_master_features_df/data}
      - {name: combine-feature-domains-outfile_master_patient_id_list, path: /tmp/outputs/outfile_master_patient_id_list/data}
      - {name: combine-feature-domains-output_csv_master_feature_list, path: /tmp/outputs/output_csv_master_feature_list/data}
      - {name: combine-feature-domains-output_csv_test_indexes, path: /tmp/outputs/output_csv_test_indexes/data}
      - {name: combine-feature-domains-output_csv_train_indexes, path: /tmp/outputs/output_csv_train_indexes/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Combination
          of features (concatenation) for early fusion model.", "implementation":
          {"container": {"args": ["--file-path-image-fx", {"inputPath": "file_path_image_fx"},
          "--file-path-clinical-fx", {"inputPath": "file_path_clinical_fx"}, "--file-path-image-patient-id",
          {"inputPath": "file_path_image_patient_id"}, "--file-path-clinical-patient-id",
          {"inputPath": "file_path_clinical_patient_id"}, "--param-test-set-size",
          {"inputValue": "param_test_set_size"}, "--param-random-seed", {"inputValue":
          "param_random_seed"}, "--outfile-master-features-df", {"outputPath": "outfile_master_features_df"},
          "--outfile-master-patient-id-list", {"outputPath": "outfile_master_patient_id_list"},
          "--output-csv-train-indexes", {"outputPath": "output_csv_train_indexes"},
          "--output-csv-test-indexes", {"outputPath": "output_csv_test_indexes"},
          "--output-csv-master-feature-list", {"outputPath": "output_csv_master_feature_list"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas==1.4.2'' ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef combine_feature_domains(file_path_image_fx,\n                            file_path_clinical_fx,\n                            file_path_image_patient_id,\n                            file_path_clinical_patient_id,\n                            param_test_set_size,\n                            param_random_seed,\n                            outfile_master_features_df,\n                            outfile_master_patient_id_list,\n                            output_csv_train_indexes,\n                            output_csv_test_indexes,\n                            output_csv_master_feature_list):\n\n    \"\"\"Combination
          of features (concatenation) for early fusion model. \n\n    Args:\n        file_path_image_fx:
          \n        file_path_clinical_fx:\n        file_path_image_patient_id:\n        file_path_clinical_patient_id:
          \n    \"\"\"\n    import numpy as np\n    import pandas as pd\n    from
          sklearn.model_selection import train_test_split\n\n    df_image_fx = pd.read_csv(file_path_image_fx)\n    df_clinical_fx
          = pd.read_csv(file_path_clinical_fx)\n    df_image_patient_id = pd.read_csv(file_path_image_patient_id)\n    df_clinical_patient_id
          = pd.read_csv(file_path_clinical_patient_id)\n\n    # slap on patient id
          column as first column\n    df_image_table_with_patient_id = pd.concat([df_image_patient_id,
          df_image_fx],\n                                               axis=1)\n    df_clinical_table_with_patient_id
          = pd.concat([df_clinical_patient_id, df_clinical_fx],\n                                                  axis=1)\n\n    #
          merge data domains\n    df_master_features = df_image_table_with_patient_id.merge(df_clinical_table_with_patient_id,\n                                                              how=''outer'',\n                                                              on=''bcr_patient_barcode'')\n\n    #
          create train test and store indexes, using these features\n    param_test_set_size_float
          = np.float64(param_test_set_size)\n\n    ## set a valid value of 0.25 for
          test set size if needed\n    if param_test_set_size_float <= 0 or param_test_set_size_float
          >= 1:\n        param_test_set_size_float = 0.25    \n\n    param_random_seed_int
          = int(np.float64(param_random_seed))\n    X_train, X_test, y_train, y_test
          = train_test_split(df_master_features, \n                                                        df_master_features[df_master_features.columns[-1]],
          \n                                                    test_size=param_test_set_size_float,
          \n                                                    random_state=param_random_seed_int)
          # IMPT: set random seed\n    df_idx_train = X_train.index.to_frame().rename(columns={0:
          \"index\"})\n    df_idx_train[''bcr_patient_barcode''] = df_master_features.iloc[X_train.index][''bcr_patient_barcode'']\n    df_idx_test
          = X_test.index.to_frame().rename(columns={0: \"index\"})\n    df_idx_test[''bcr_patient_barcode'']
          = df_master_features.iloc[X_test.index][''bcr_patient_barcode'']\n\n    #
          write outputs\n    df_master_patient_id_list = df_master_features[[''bcr_patient_barcode'']]\n    del
          df_master_features[''bcr_patient_barcode'']\n\n    ## feature matrix and
          patient id list\n    df_master_features.to_csv(outfile_master_features_df,
          header=True, index=False)\n    df_master_patient_id_list.to_csv(outfile_master_patient_id_list,
          header=True, index=False)\n\n    ## list of feature names\n    l_feature_names_images
          = list(set(df_image_fx.columns).intersection(df_master_features.columns))
          \n    df_feature_list = pd.DataFrame([])\n    df_feature_list[''feature_name'']
          = l_feature_names_images\n    df_feature_list.to_csv(output_csv_master_feature_list,
          header=True, index=False)\n\n    ## save training and testing splits - to
          use with all fused feature matrices from now on\n    df_idx_train.to_csv(output_csv_train_indexes,
          index = False, header = True)\n    df_idx_test.to_csv(output_csv_test_indexes,
          index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Combine
          feature domains'', description=''Combination of features (concatenation)
          for early fusion model.'')\n_parser.add_argument(\"--file-path-image-fx\",
          dest=\"file_path_image_fx\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-clinical-fx\",
          dest=\"file_path_clinical_fx\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-image-patient-id\",
          dest=\"file_path_image_patient_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-clinical-patient-id\",
          dest=\"file_path_clinical_patient_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-test-set-size\",
          dest=\"param_test_set_size\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--outfile-master-features-df\",
          dest=\"outfile_master_features_df\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--outfile-master-patient-id-list\",
          dest=\"outfile_master_patient_id_list\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-train-indexes\",
          dest=\"output_csv_train_indexes\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-test-indexes\",
          dest=\"output_csv_test_indexes\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-master-feature-list\",
          dest=\"output_csv_master_feature_list\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = combine_feature_domains(**_parsed_args)\n"], "image": "python:3.9"}},
          "inputs": [{"description": "", "name": "file_path_image_fx", "type": "CSV"},
          {"description": "", "name": "file_path_clinical_fx", "type": "CSV"}, {"description":
          "", "name": "file_path_image_patient_id", "type": "CSV"}, {"description":
          "", "name": "file_path_clinical_patient_id", "type": "CSV"}, {"name": "param_test_set_size",
          "type": "String"}, {"name": "param_random_seed", "type": "String"}], "name":
          "Combine feature domains", "outputs": [{"name": "outfile_master_features_df",
          "type": "CSV"}, {"name": "outfile_master_patient_id_list", "type": "CSV"},
          {"name": "output_csv_train_indexes", "type": "CSV"}, {"name": "output_csv_test_indexes",
          "type": "CSV"}, {"name": "output_csv_master_feature_list", "type": "CSV"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"param_random_seed":
          "{{inputs.parameters.param_random_seed}}", "param_test_set_size": "{{inputs.parameters.test_set_size}}"}'}
  - name: construct-features-images-histomicstk
    container:
      args: [--file-path-images, /tmp/inputs/file_path_images/data, --file-path-full-patient-id-list,
        /tmp/inputs/file_path_full_patient_id_list/data, --file-path-full-patient-class-labels,
        /tmp/inputs/file_path_full_patient_class_labels/data, --param-test-set-size,
        '{{inputs.parameters.test_set_size}}', --param-random-seed, '{{inputs.parameters.param_random_seed}}',
        --outfile-features-df, /tmp/outputs/outfile_features_df/data, --outfile-patient-id-list,
        /tmp/outputs/outfile_patient_id_list/data, --outfile-class-labels-pts-with-images,
        /tmp/outputs/outfile_class_labels_pts_with_images/data, --output-csv-train-indexes,
        /tmp/outputs/output_csv_train_indexes/data, --output-csv-test-indexes, /tmp/outputs/output_csv_test_indexes/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'cytoolz==0.11.2' 'conda==4.3.16' 'large_image==1.14.3' 'numpy==1.22.3' 'pandas==1.4.2'
        'scikit-image==0.19.2' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'cytoolz==0.11.2'
        'conda==4.3.16' 'large_image==1.14.3' 'numpy==1.22.3' 'pandas==1.4.2' 'scikit-image==0.19.2'
        'scikit-learn==1.0.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef construct_features_images_histomicstk(file_path_images,\n          \
        \                      file_path_full_patient_id_list,\n                 \
        \               file_path_full_patient_class_labels,\n                   \
        \             param_test_set_size,\n                                param_random_seed,\n\
        \                                outfile_features_df,\n                  \
        \              outfile_patient_id_list,\n                                outfile_class_labels_pts_with_images,\n\
        \                                output_csv_train_indexes,\n             \
        \                   output_csv_test_indexes):\n    \"\"\"Feature construction\
        \ from images. \n\n    Args:\n        file_path: A string containing path\
        \ to images (3-channel) in JSON obj.\n    \"\"\"\n\n    #####\n    import\
        \ os\n    import pip\n    import subprocess\n    import sys\n    def install(package):\n\
        \        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"\
        , package])\n    def uninstall(package):\n        subprocess.check_call([sys.executable,\
        \ \"-m\", \"pip\", \"uninstall\", \"-y\", package])            \n\n    import\
        \ conda.cli\n\n    subprocess.check_call([sys.executable, \"-m\", \"pip\"\
        , \"install\", \"histomicstk==1.2.0\",\n                           \"--find-links\"\
        , \"https://girder.github.io/large_image_wheels\"]) ## install with find-links\n\
        \n    ##################################################################################\n\
        \    #\n    # IMPT NOTE: need global import for packages installed programatically\n\
        \    # https://stackoverflow.com/questions/59308781/python-module-not-importing-after-installing-programmatically\n\
        \    #\n    #          import importlib\n    #          globals()[package]\
        \ = importlib.import_module(package)\n    # Note: python does not add it to\
        \ path automatically, so need to add sys.path.append(site.getusersitepackages())\n\
        \    #    https://kubeflow-pipelines.readthedocs.io/en/stable/_modules/kfp/components/_python_op.html\n\
        \    #   \n    ##################################################################################\n\
        \n    import site\n    sys.path.append(site.getusersitepackages())\n    sys.path.append('/root/.local/lib/python3.10/site-packages')\n\
        \    sys.path.append('/usr/local/lib/python3.10/site-packages')\n    sys.path.append('/root/.local/lib/python3.9/site-packages')\n\
        \    sys.path.append('/usr/local/lib/python3.9/site-packages')\n    sys.path.append('/root/.local/lib/python3.8/site-packages')\n\
        \    sys.path.append('/usr/local/lib/python3.8/site-packages')\n    sys.path.append('/root/.local/lib/python3.7/site-packages')\n\
        \    sys.path.append('/usr/local/lib/python3.7/site-packages')\n    sys.path.append('/usr/local')\n\
        \    sys.path.append('/root/.local')\n    sys.path.append('/usr/local/bin')\n\
        \    sys.path.append('/root/.local/bin')\n    print(sys.path)\n    print(os.listdir('/usr/local/lib/python3.9/site-packages/histomicstk/segmentation/label/'))\n\
        \    import histomicstk.segmentation.positive_pixel_count as ppc\n\n    import\
        \ json\n    from collections import OrderedDict\n    import numpy as np\n\
        \    import pandas as pd\n\n    import large_image\n    import skimage.io\n\
        \    from skimage.filters import prewitt_h,prewitt_v\n    from sklearn.model_selection\
        \ import train_test_split\n\n    # open images\n\n    with open(file_path_images,\
        \ \"r\") as read_file:\n        d_images_from_pl = json.load(read_file)\n\n\
        \    # compute histomicstk features\n\n    # make d_features and add all ppc-related\
        \ features\n    d_features = OrderedDict()\n    d_features['IntensitySumWeakPositive']\
        \ = []\n    d_features['IntensitySumPositive'] = []\n    d_features['IntensitySumStrongPositive']\
        \ = []\n    d_features['IntensityAverage'] = []\n    d_features['RatioStrongToTotal']\
        \ = []\n    d_features['IntensityAverageWeakAndPositive'] = []\n\n    ## set\
        \ template params\n    template_params = ppc.Parameters(\n    hue_value=0.05,\n\
        \    hue_width=9999,\n    saturation_minimum=0.05,\n    intensity_upper_limit=0.95,\n\
        \    intensity_weak_threshold=0.65,\n    intensity_strong_threshold=0.35,\n\
        \    intensity_lower_limit=0.05)\n\n    image_url = ('https://data.kitware.com/api/v1/file/'\n\
        \                 '598b71ee8d777f7d33e9c1d4/download')  # DAB.png\n    img_input_sample\
        \ = skimage.io.imread(image_url)\n\n    ## compute for each patient's image\n\
        \    l_ptID = np.sort(list(d_images_from_pl.keys()))\n    for ptID in l_ptID:\n\
        \        img = np.array(\n                d_images_from_pl[\n            \
        \        ptID]\n                        [0] # use the first image for the\
        \ pt\n                )\n        # feature extraction  \n        try:\n  \
        \          stats, label_image = ppc.count_image(img, template_params)\n  \
        \      except: ## Placeholder - use sample image\n            stats, label_image\
        \ = ppc.count_image(img_input_sample, template_params)\n\n        d_features['IntensitySumWeakPositive'].append(stats.IntensitySumWeakPositive)\n\
        \        d_features['IntensitySumPositive'].append(stats.IntensitySumPositive)\n\
        \        d_features['IntensitySumStrongPositive'].append(stats.IntensitySumStrongPositive)\n\
        \        d_features['IntensityAverage'].append(stats.IntensityAverage)\n \
        \       d_features['RatioStrongToTotal'].append(stats.RatioStrongToTotal)\n\
        \        d_features['IntensityAverageWeakAndPositive'].append(stats.IntensityAverageWeakAndPositive)\n\
        \n    ##########################################\n\n    # compute features\
        \ and append to d_features - skimage\n    d_features[\"mean_weight_raw_img\"\
        ] = []\n    d_features[\"mean_edge_weight_horizontal\"] = []\n    d_features[\"\
        mean_edge_weight_vertical\"] = []\n\n    l_ptID = np.sort(list(d_images_from_pl.keys()))\n\
        \    for ptID in l_ptID:\n        img = np.array(\n                d_images_from_pl[\n\
        \                    ptID]\n                        [0] # use the first image\
        \ for the pt\n                )\n        # feature extraction\n\n        ##\
        \ calculating horizontal edges using prewitt kernel\n        edges_prewitt_horizontal\
        \ = prewitt_h(img[:,:,0])\n        ## calculating vertical edges using prewitt\
        \ kernel\n        edges_prewitt_vertical = prewitt_v(img[:,:,0])\n\n     \
        \   ## Feature: mean values \n        mean_weight_raw_img = np.mean(img)\n\
        \        d_features[\"mean_weight_raw_img\"].append(mean_weight_raw_img)\n\
        \n        ## Feature: mean values \n        mean_edge_weight_horizontal =\
        \ np.mean(edges_prewitt_horizontal)\n        d_features[\"mean_edge_weight_horizontal\"\
        ].append(mean_edge_weight_horizontal)\n\n        ## Feature: mean values \n\
        \        mean_edge_weight_vertical = np.mean(edges_prewitt_vertical)\n   \
        \     d_features[\"mean_edge_weight_vertical\"].append(mean_edge_weight_vertical)\n\
        \n    df_features_combined = pd.DataFrame([])\n    df_features_combined['bcr_patient_barcode']\
        \ = l_ptID\n    for feature_name in d_features.keys():\n        df_features_combined[feature_name]\
        \ = d_features[feature_name]\n\n    # output patient ID list\n    df_patient_id_list\
        \ = df_features_combined[['bcr_patient_barcode']]\n    df_patient_id_list.to_csv(outfile_patient_id_list,\
        \ header = True, index = False)\n\n    # output features\n    del df_features_combined['bcr_patient_barcode']\n\
        \    df_features_combined.to_csv(outfile_features_df, header = True, index\
        \ = False)\n\n    # output target class labels for this cohort of patients\
        \ (not all patients in full\n    #  cohort will have images)\n    df_full_patient_id_list\
        \ = pd.read_csv(file_path_full_patient_id_list)\n    df_full_patient_class_labels\
        \ = pd.read_csv(file_path_full_patient_class_labels)\n    ## isolate patients\
        \ that have images\n    df_slice_full_patient_id_list_have_images = df_full_patient_id_list[df_full_patient_id_list['bcr_patient_barcode'].isin(df_patient_id_list['bcr_patient_barcode'])]\n\
        \    idx_in_full_patient_id_list_have_images = df_slice_full_patient_id_list_have_images.index\n\
        \    df_class_labels_pts_with_images = df_full_patient_class_labels.loc[idx_in_full_patient_id_list_have_images]\n\
        \    ## write outputs for class labels (y) and patient id list (just for these\
        \ pts that have images)\n    df_class_labels_pts_with_images.to_csv(outfile_class_labels_pts_with_images,\
        \ header=True, index=False)\n    df_slice_full_patient_id_list_have_images.to_csv(outfile_patient_id_list,\
        \ header=True, index=False)\n\n    ## create train test and store indexes,\
        \ using these clinical features\n    param_test_set_size_float = np.float64(param_test_set_size)\n\
        \    ### set a valid value of 0.25 for test set size if needed\n    if param_test_set_size_float\
        \ < 0 or param_test_set_size_float > 1:\n        param_test_set_size_float\
        \ = 0.25    \n\n    param_random_seed_int = int(np.float64(param_random_seed))\n\
        \    X_train, X_test, y_train, y_test = train_test_split(df_features_combined,\
        \ df_class_labels_pts_with_images, \n                                    \
        \                test_size=param_test_set_size_float, \n                 \
        \                                   random_state=param_random_seed_int) #\
        \ IMPT: set random seed\n    df_idx_train = X_train.index.to_frame().rename(columns={0:\
        \ \"index\"})\n    df_idx_train['bcr_patient_barcode'] = df_slice_full_patient_id_list_have_images.iloc[X_train.index]\n\
        \    df_idx_test = X_test.index.to_frame().rename(columns={0: \"index\"})\n\
        \    df_idx_test['bcr_patient_barcode'] = df_slice_full_patient_id_list_have_images.iloc[X_test.index]\n\
        \    ## save training and testing splits - to be used for instances where\
        \ just these image data are used in expt\n    df_idx_train.to_csv(output_csv_train_indexes,\
        \ index = False, header = True)\n    df_idx_test.to_csv(output_csv_test_indexes,\
        \ index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Construct\
        \ features images histomicstk', description='Feature construction from images.')\n\
        _parser.add_argument(\"--file-path-images\", dest=\"file_path_images\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-full-patient-id-list\"\
        , dest=\"file_path_full_patient_id_list\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--file-path-full-patient-class-labels\", dest=\"file_path_full_patient_class_labels\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --param-test-set-size\", dest=\"param_test_set_size\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\"\
        , dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--outfile-features-df\", dest=\"outfile_features_df\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--outfile-patient-id-list\", dest=\"outfile_patient_id_list\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--outfile-class-labels-pts-with-images\", dest=\"outfile_class_labels_pts_with_images\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-csv-train-indexes\", dest=\"output_csv_train_indexes\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-csv-test-indexes\", dest=\"output_csv_test_indexes\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = construct_features_images_histomicstk(**_parsed_args)\n"
      image: python:3.9
    inputs:
      parameters:
      - {name: param_random_seed}
      - {name: test_set_size}
      artifacts:
      - {name: get-dummies-output_csv_target, path: /tmp/inputs/file_path_full_patient_class_labels/data}
      - {name: get-dummies-output_csv_patient_list_filtered, path: /tmp/inputs/file_path_full_patient_id_list/data}
      - {name: download-images-outfile_images_json, path: /tmp/inputs/file_path_images/data}
    outputs:
      artifacts:
      - {name: construct-features-images-histomicstk-outfile_class_labels_pts_with_images,
        path: /tmp/outputs/outfile_class_labels_pts_with_images/data}
      - {name: construct-features-images-histomicstk-outfile_features_df, path: /tmp/outputs/outfile_features_df/data}
      - {name: construct-features-images-histomicstk-outfile_patient_id_list, path: /tmp/outputs/outfile_patient_id_list/data}
      - {name: construct-features-images-histomicstk-output_csv_test_indexes, path: /tmp/outputs/output_csv_test_indexes/data}
      - {name: construct-features-images-histomicstk-output_csv_train_indexes, path: /tmp/outputs/output_csv_train_indexes/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Feature
          construction from images.", "implementation": {"container": {"args": ["--file-path-images",
          {"inputPath": "file_path_images"}, "--file-path-full-patient-id-list", {"inputPath":
          "file_path_full_patient_id_list"}, "--file-path-full-patient-class-labels",
          {"inputPath": "file_path_full_patient_class_labels"}, "--param-test-set-size",
          {"inputValue": "param_test_set_size"}, "--param-random-seed", {"inputValue":
          "param_random_seed"}, "--outfile-features-df", {"outputPath": "outfile_features_df"},
          "--outfile-patient-id-list", {"outputPath": "outfile_patient_id_list"},
          "--outfile-class-labels-pts-with-images", {"outputPath": "outfile_class_labels_pts_with_images"},
          "--output-csv-train-indexes", {"outputPath": "output_csv_train_indexes"},
          "--output-csv-test-indexes", {"outputPath": "output_csv_test_indexes"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''cytoolz==0.11.2'' ''conda==4.3.16''
          ''large_image==1.14.3'' ''numpy==1.22.3'' ''pandas==1.4.2'' ''scikit-image==0.19.2''
          ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''cytoolz==0.11.2'' ''conda==4.3.16''
          ''large_image==1.14.3'' ''numpy==1.22.3'' ''pandas==1.4.2'' ''scikit-image==0.19.2''
          ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef construct_features_images_histomicstk(file_path_images,\n                                file_path_full_patient_id_list,\n                                file_path_full_patient_class_labels,\n                                param_test_set_size,\n                                param_random_seed,\n                                outfile_features_df,\n                                outfile_patient_id_list,\n                                outfile_class_labels_pts_with_images,\n                                output_csv_train_indexes,\n                                output_csv_test_indexes):\n    \"\"\"Feature
          construction from images. \n\n    Args:\n        file_path: A string containing
          path to images (3-channel) in JSON obj.\n    \"\"\"\n\n    #####\n    import
          os\n    import pip\n    import subprocess\n    import sys\n    def install(package):\n        subprocess.check_call([sys.executable,
          \"-m\", \"pip\", \"install\", package])\n    def uninstall(package):\n        subprocess.check_call([sys.executable,
          \"-m\", \"pip\", \"uninstall\", \"-y\", package])            \n\n    import
          conda.cli\n\n    subprocess.check_call([sys.executable, \"-m\", \"pip\",
          \"install\", \"histomicstk==1.2.0\",\n                           \"--find-links\",
          \"https://girder.github.io/large_image_wheels\"]) ## install with find-links\n\n    ##################################################################################\n    #\n    #
          IMPT NOTE: need global import for packages installed programatically\n    #
          https://stackoverflow.com/questions/59308781/python-module-not-importing-after-installing-programmatically\n    #\n    #          import
          importlib\n    #          globals()[package] = importlib.import_module(package)\n    #
          Note: python does not add it to path automatically, so need to add sys.path.append(site.getusersitepackages())\n    #    https://kubeflow-pipelines.readthedocs.io/en/stable/_modules/kfp/components/_python_op.html\n    #   \n    ##################################################################################\n\n    import
          site\n    sys.path.append(site.getusersitepackages())\n    sys.path.append(''/root/.local/lib/python3.10/site-packages'')\n    sys.path.append(''/usr/local/lib/python3.10/site-packages'')\n    sys.path.append(''/root/.local/lib/python3.9/site-packages'')\n    sys.path.append(''/usr/local/lib/python3.9/site-packages'')\n    sys.path.append(''/root/.local/lib/python3.8/site-packages'')\n    sys.path.append(''/usr/local/lib/python3.8/site-packages'')\n    sys.path.append(''/root/.local/lib/python3.7/site-packages'')\n    sys.path.append(''/usr/local/lib/python3.7/site-packages'')\n    sys.path.append(''/usr/local'')\n    sys.path.append(''/root/.local'')\n    sys.path.append(''/usr/local/bin'')\n    sys.path.append(''/root/.local/bin'')\n    print(sys.path)\n    print(os.listdir(''/usr/local/lib/python3.9/site-packages/histomicstk/segmentation/label/''))\n    import
          histomicstk.segmentation.positive_pixel_count as ppc\n\n    import json\n    from
          collections import OrderedDict\n    import numpy as np\n    import pandas
          as pd\n\n    import large_image\n    import skimage.io\n    from skimage.filters
          import prewitt_h,prewitt_v\n    from sklearn.model_selection import train_test_split\n\n    #
          open images\n\n    with open(file_path_images, \"r\") as read_file:\n        d_images_from_pl
          = json.load(read_file)\n\n    # compute histomicstk features\n\n    # make
          d_features and add all ppc-related features\n    d_features = OrderedDict()\n    d_features[''IntensitySumWeakPositive'']
          = []\n    d_features[''IntensitySumPositive''] = []\n    d_features[''IntensitySumStrongPositive'']
          = []\n    d_features[''IntensityAverage''] = []\n    d_features[''RatioStrongToTotal'']
          = []\n    d_features[''IntensityAverageWeakAndPositive''] = []\n\n    ##
          set template params\n    template_params = ppc.Parameters(\n    hue_value=0.05,\n    hue_width=9999,\n    saturation_minimum=0.05,\n    intensity_upper_limit=0.95,\n    intensity_weak_threshold=0.65,\n    intensity_strong_threshold=0.35,\n    intensity_lower_limit=0.05)\n\n    image_url
          = (''https://data.kitware.com/api/v1/file/''\n                 ''598b71ee8d777f7d33e9c1d4/download'')  #
          DAB.png\n    img_input_sample = skimage.io.imread(image_url)\n\n    ## compute
          for each patient''s image\n    l_ptID = np.sort(list(d_images_from_pl.keys()))\n    for
          ptID in l_ptID:\n        img = np.array(\n                d_images_from_pl[\n                    ptID]\n                        [0]
          # use the first image for the pt\n                )\n        # feature extraction  \n        try:\n            stats,
          label_image = ppc.count_image(img, template_params)\n        except: ##
          Placeholder - use sample image\n            stats, label_image = ppc.count_image(img_input_sample,
          template_params)\n\n        d_features[''IntensitySumWeakPositive''].append(stats.IntensitySumWeakPositive)\n        d_features[''IntensitySumPositive''].append(stats.IntensitySumPositive)\n        d_features[''IntensitySumStrongPositive''].append(stats.IntensitySumStrongPositive)\n        d_features[''IntensityAverage''].append(stats.IntensityAverage)\n        d_features[''RatioStrongToTotal''].append(stats.RatioStrongToTotal)\n        d_features[''IntensityAverageWeakAndPositive''].append(stats.IntensityAverageWeakAndPositive)\n\n    ##########################################\n\n    #
          compute features and append to d_features - skimage\n    d_features[\"mean_weight_raw_img\"]
          = []\n    d_features[\"mean_edge_weight_horizontal\"] = []\n    d_features[\"mean_edge_weight_vertical\"]
          = []\n\n    l_ptID = np.sort(list(d_images_from_pl.keys()))\n    for ptID
          in l_ptID:\n        img = np.array(\n                d_images_from_pl[\n                    ptID]\n                        [0]
          # use the first image for the pt\n                )\n        # feature extraction\n\n        ##
          calculating horizontal edges using prewitt kernel\n        edges_prewitt_horizontal
          = prewitt_h(img[:,:,0])\n        ## calculating vertical edges using prewitt
          kernel\n        edges_prewitt_vertical = prewitt_v(img[:,:,0])\n\n        ##
          Feature: mean values \n        mean_weight_raw_img = np.mean(img)\n        d_features[\"mean_weight_raw_img\"].append(mean_weight_raw_img)\n\n        ##
          Feature: mean values \n        mean_edge_weight_horizontal = np.mean(edges_prewitt_horizontal)\n        d_features[\"mean_edge_weight_horizontal\"].append(mean_edge_weight_horizontal)\n\n        ##
          Feature: mean values \n        mean_edge_weight_vertical = np.mean(edges_prewitt_vertical)\n        d_features[\"mean_edge_weight_vertical\"].append(mean_edge_weight_vertical)\n\n    df_features_combined
          = pd.DataFrame([])\n    df_features_combined[''bcr_patient_barcode''] =
          l_ptID\n    for feature_name in d_features.keys():\n        df_features_combined[feature_name]
          = d_features[feature_name]\n\n    # output patient ID list\n    df_patient_id_list
          = df_features_combined[[''bcr_patient_barcode'']]\n    df_patient_id_list.to_csv(outfile_patient_id_list,
          header = True, index = False)\n\n    # output features\n    del df_features_combined[''bcr_patient_barcode'']\n    df_features_combined.to_csv(outfile_features_df,
          header = True, index = False)\n\n    # output target class labels for this
          cohort of patients (not all patients in full\n    #  cohort will have images)\n    df_full_patient_id_list
          = pd.read_csv(file_path_full_patient_id_list)\n    df_full_patient_class_labels
          = pd.read_csv(file_path_full_patient_class_labels)\n    ## isolate patients
          that have images\n    df_slice_full_patient_id_list_have_images = df_full_patient_id_list[df_full_patient_id_list[''bcr_patient_barcode''].isin(df_patient_id_list[''bcr_patient_barcode''])]\n    idx_in_full_patient_id_list_have_images
          = df_slice_full_patient_id_list_have_images.index\n    df_class_labels_pts_with_images
          = df_full_patient_class_labels.loc[idx_in_full_patient_id_list_have_images]\n    ##
          write outputs for class labels (y) and patient id list (just for these pts
          that have images)\n    df_class_labels_pts_with_images.to_csv(outfile_class_labels_pts_with_images,
          header=True, index=False)\n    df_slice_full_patient_id_list_have_images.to_csv(outfile_patient_id_list,
          header=True, index=False)\n\n    ## create train test and store indexes,
          using these clinical features\n    param_test_set_size_float = np.float64(param_test_set_size)\n    ###
          set a valid value of 0.25 for test set size if needed\n    if param_test_set_size_float
          < 0 or param_test_set_size_float > 1:\n        param_test_set_size_float
          = 0.25    \n\n    param_random_seed_int = int(np.float64(param_random_seed))\n    X_train,
          X_test, y_train, y_test = train_test_split(df_features_combined, df_class_labels_pts_with_images,
          \n                                                    test_size=param_test_set_size_float,
          \n                                                    random_state=param_random_seed_int)
          # IMPT: set random seed\n    df_idx_train = X_train.index.to_frame().rename(columns={0:
          \"index\"})\n    df_idx_train[''bcr_patient_barcode''] = df_slice_full_patient_id_list_have_images.iloc[X_train.index]\n    df_idx_test
          = X_test.index.to_frame().rename(columns={0: \"index\"})\n    df_idx_test[''bcr_patient_barcode'']
          = df_slice_full_patient_id_list_have_images.iloc[X_test.index]\n    ## save
          training and testing splits - to be used for instances where just these
          image data are used in expt\n    df_idx_train.to_csv(output_csv_train_indexes,
          index = False, header = True)\n    df_idx_test.to_csv(output_csv_test_indexes,
          index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Construct
          features images histomicstk'', description=''Feature construction from images.'')\n_parser.add_argument(\"--file-path-images\",
          dest=\"file_path_images\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-full-patient-id-list\",
          dest=\"file_path_full_patient_id_list\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-full-patient-class-labels\",
          dest=\"file_path_full_patient_class_labels\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-test-set-size\",
          dest=\"param_test_set_size\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--outfile-features-df\",
          dest=\"outfile_features_df\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--outfile-patient-id-list\",
          dest=\"outfile_patient_id_list\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--outfile-class-labels-pts-with-images\",
          dest=\"outfile_class_labels_pts_with_images\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-train-indexes\",
          dest=\"output_csv_train_indexes\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-test-indexes\",
          dest=\"output_csv_test_indexes\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = construct_features_images_histomicstk(**_parsed_args)\n"], "image": "python:3.9"}},
          "inputs": [{"name": "file_path_images", "type": "JSON"}, {"name": "file_path_full_patient_id_list",
          "type": "CSV"}, {"name": "file_path_full_patient_class_labels", "type":
          "CSV"}, {"name": "param_test_set_size", "type": "String"}, {"name": "param_random_seed",
          "type": "String"}], "name": "Construct features images histomicstk", "outputs":
          [{"name": "outfile_features_df", "type": "CSV"}, {"name": "outfile_patient_id_list",
          "type": "CSV"}, {"name": "outfile_class_labels_pts_with_images", "type":
          "CSV"}, {"name": "output_csv_train_indexes", "type": "CSV"}, {"name": "output_csv_test_indexes",
          "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"param_random_seed":
          "{{inputs.parameters.param_random_seed}}", "param_test_set_size": "{{inputs.parameters.test_set_size}}"}'}
  - name: create-model-inputs
    container:
      args: [--file-path-features, /tmp/inputs/file_path_features/data, --file-path-class-labels,
        /tmp/inputs/file_path_class_labels/data, --test-set-size, '{{inputs.parameters.test_set_size}}',
        --param-random-seed, '{{inputs.parameters.param_random_seed}}', --file-path-train-indexes,
        /tmp/inputs/file_path_train_indexes/data, --file-path-test-indexes, /tmp/inputs/file_path_test_indexes/data,
        --output-X-train, /tmp/outputs/output_X_train/data, --output-X-test, /tmp/outputs/output_X_test/data,
        --output-y-train, /tmp/outputs/output_y_train/data, --output-y-test, /tmp/outputs/output_y_test/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' 'numpy' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        'numpy' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def create_model_inputs(file_path_features,
                                file_path_class_labels,
                                test_set_size,
                                param_random_seed,
                                file_path_train_indexes,
                                file_path_test_indexes,
                                output_X_train,
                                output_X_test,
                                output_y_train,
                                output_y_test):
            """Create train and test sets

            Args:
                file_path_features: A string containing path / pipeline component output
                file_path_class_labels: A string containing path to data / pipeline component output
                test_set_size: [optional, default=0.25] proportion to use for the test set
            """
            import numpy as np

            import pandas as pd
            from sklearn import datasets
            from sklearn.model_selection import train_test_split

            X = pd.read_csv(filepath_or_buffer=file_path_features)
            y = pd.read_csv(filepath_or_buffer=file_path_class_labels)

            # load train/test indexes - computed with combine feature domains task
            df_train_indexes = pd.read_csv(file_path_train_indexes)
            df_test_indexes = pd.read_csv(file_path_test_indexes)
            nparr_train_indexes = np.array(df_train_indexes['index'])
            nparr_test_indexes = np.array(df_test_indexes['index'])

            X_train = X.iloc[nparr_train_indexes]
            y_train = y.iloc[nparr_train_indexes]
            X_test = X.iloc[nparr_test_indexes]
            y_test = y.iloc[nparr_test_indexes]

            X_train.to_csv(output_X_train, header=True, index=False)
            X_test.to_csv(output_X_test, header=True, index=False)
            y_train.to_csv(output_y_train, header=True, index=False)
            y_test.to_csv(output_y_test, header=True, index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Create model inputs', description='Create train and test sets')
        _parser.add_argument("--file-path-features", dest="file_path_features", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-class-labels", dest="file_path_class_labels", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-set-size", dest="test_set_size", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--param-random-seed", dest="param_random_seed", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-train-indexes", dest="file_path_train_indexes", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-test-indexes", dest="file_path_test_indexes", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-X-train", dest="output_X_train", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-X-test", dest="output_X_test", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-y-train", dest="output_y_train", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-y-test", dest="output_y_test", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = create_model_inputs(**_parsed_args)
      image: python:3.9
    inputs:
      parameters:
      - {name: param_random_seed}
      - {name: test_set_size}
      artifacts:
      - {name: get-dummies-output_csv_target, path: /tmp/inputs/file_path_class_labels/data}
      - {name: scale-df-output_csv, path: /tmp/inputs/file_path_features/data}
      - {name: get-dummies-output_csv_test_indexes, path: /tmp/inputs/file_path_test_indexes/data}
      - {name: get-dummies-output_csv_train_indexes, path: /tmp/inputs/file_path_train_indexes/data}
    outputs:
      artifacts:
      - {name: create-model-inputs-output_X_test, path: /tmp/outputs/output_X_test/data}
      - {name: create-model-inputs-output_X_train, path: /tmp/outputs/output_X_train/data}
      - {name: create-model-inputs-output_y_test, path: /tmp/outputs/output_y_test/data}
      - {name: create-model-inputs-output_y_train, path: /tmp/outputs/output_y_train/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Create
          train and test sets", "implementation": {"container": {"args": ["--file-path-features",
          {"inputPath": "file_path_features"}, "--file-path-class-labels", {"inputPath":
          "file_path_class_labels"}, "--test-set-size", {"inputValue": "test_set_size"},
          "--param-random-seed", {"inputValue": "param_random_seed"}, "--file-path-train-indexes",
          {"inputPath": "file_path_train_indexes"}, "--file-path-test-indexes", {"inputPath":
          "file_path_test_indexes"}, "--output-X-train", {"outputPath": "output_X_train"},
          "--output-X-test", {"outputPath": "output_X_test"}, "--output-y-train",
          {"outputPath": "output_y_train"}, "--output-y-test", {"outputPath": "output_y_test"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2''
          ''numpy'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''numpy''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef create_model_inputs(file_path_features,\n                        file_path_class_labels,\n                        test_set_size,\n                        param_random_seed,\n                        file_path_train_indexes,\n                        file_path_test_indexes,\n                        output_X_train,\n                        output_X_test,\n                        output_y_train,\n                        output_y_test):\n    \"\"\"Create
          train and test sets\n\n    Args:\n        file_path_features: A string containing
          path / pipeline component output\n        file_path_class_labels: A string
          containing path to data / pipeline component output\n        test_set_size:
          [optional, default=0.25] proportion to use for the test set\n    \"\"\"\n    import
          numpy as np\n\n    import pandas as pd\n    from sklearn import datasets\n    from
          sklearn.model_selection import train_test_split\n\n    X = pd.read_csv(filepath_or_buffer=file_path_features)\n    y
          = pd.read_csv(filepath_or_buffer=file_path_class_labels)\n\n    # load train/test
          indexes - computed with combine feature domains task\n    df_train_indexes
          = pd.read_csv(file_path_train_indexes)\n    df_test_indexes = pd.read_csv(file_path_test_indexes)\n    nparr_train_indexes
          = np.array(df_train_indexes[''index''])\n    nparr_test_indexes = np.array(df_test_indexes[''index''])\n\n    X_train
          = X.iloc[nparr_train_indexes]\n    y_train = y.iloc[nparr_train_indexes]\n    X_test
          = X.iloc[nparr_test_indexes]\n    y_test = y.iloc[nparr_test_indexes]\n\n    X_train.to_csv(output_X_train,
          header=True, index=False)\n    X_test.to_csv(output_X_test, header=True,
          index=False)\n    y_train.to_csv(output_y_train, header=True, index=False)\n    y_test.to_csv(output_y_test,
          header=True, index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Create
          model inputs'', description=''Create train and test sets'')\n_parser.add_argument(\"--file-path-features\",
          dest=\"file_path_features\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-class-labels\",
          dest=\"file_path_class_labels\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-set-size\",
          dest=\"test_set_size\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-train-indexes\",
          dest=\"file_path_train_indexes\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-test-indexes\",
          dest=\"file_path_test_indexes\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-X-train\",
          dest=\"output_X_train\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-X-test\", dest=\"output_X_test\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-y-train\",
          dest=\"output_y_train\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-y-test\", dest=\"output_y_test\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = create_model_inputs(**_parsed_args)\n"],
          "image": "python:3.9"}}, "inputs": [{"description": "A string containing
          path / pipeline component output", "name": "file_path_features", "type":
          "CSV"}, {"description": "A string containing path to data / pipeline component
          output", "name": "file_path_class_labels", "type": "CSV"}, {"description":
          "[optional, default=0.25] proportion to use for the test set", "name": "test_set_size",
          "type": "String"}, {"name": "param_random_seed", "type": "String"}, {"name":
          "file_path_train_indexes", "type": "CSV"}, {"name": "file_path_test_indexes",
          "type": "CSV"}], "name": "Create model inputs", "outputs": [{"name": "output_X_train",
          "type": "CSV"}, {"name": "output_X_test", "type": "CSV"}, {"name": "output_y_train",
          "type": "CSV"}, {"name": "output_y_test", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"param_random_seed": "{{inputs.parameters.param_random_seed}}",
          "test_set_size": "{{inputs.parameters.test_set_size}}"}'}
  - name: create-model-inputs-2
    container:
      args: [--file-path-features, /tmp/inputs/file_path_features/data, --file-path-class-labels,
        /tmp/inputs/file_path_class_labels/data, --test-set-size, '{{inputs.parameters.test_set_size}}',
        --param-random-seed, '{{inputs.parameters.param_random_seed}}', --file-path-train-indexes,
        /tmp/inputs/file_path_train_indexes/data, --file-path-test-indexes, /tmp/inputs/file_path_test_indexes/data,
        --output-X-train, /tmp/outputs/output_X_train/data, --output-X-test, /tmp/outputs/output_X_test/data,
        --output-y-train, /tmp/outputs/output_y_train/data, --output-y-test, /tmp/outputs/output_y_test/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' 'numpy' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        'numpy' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def create_model_inputs(file_path_features,
                                file_path_class_labels,
                                test_set_size,
                                param_random_seed,
                                file_path_train_indexes,
                                file_path_test_indexes,
                                output_X_train,
                                output_X_test,
                                output_y_train,
                                output_y_test):
            """Create train and test sets

            Args:
                file_path_features: A string containing path / pipeline component output
                file_path_class_labels: A string containing path to data / pipeline component output
                test_set_size: [optional, default=0.25] proportion to use for the test set
            """
            import numpy as np

            import pandas as pd
            from sklearn import datasets
            from sklearn.model_selection import train_test_split

            X = pd.read_csv(filepath_or_buffer=file_path_features)
            y = pd.read_csv(filepath_or_buffer=file_path_class_labels)

            # load train/test indexes - computed with combine feature domains task
            df_train_indexes = pd.read_csv(file_path_train_indexes)
            df_test_indexes = pd.read_csv(file_path_test_indexes)
            nparr_train_indexes = np.array(df_train_indexes['index'])
            nparr_test_indexes = np.array(df_test_indexes['index'])

            X_train = X.iloc[nparr_train_indexes]
            y_train = y.iloc[nparr_train_indexes]
            X_test = X.iloc[nparr_test_indexes]
            y_test = y.iloc[nparr_test_indexes]

            X_train.to_csv(output_X_train, header=True, index=False)
            X_test.to_csv(output_X_test, header=True, index=False)
            y_train.to_csv(output_y_train, header=True, index=False)
            y_test.to_csv(output_y_test, header=True, index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Create model inputs', description='Create train and test sets')
        _parser.add_argument("--file-path-features", dest="file_path_features", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-class-labels", dest="file_path_class_labels", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-set-size", dest="test_set_size", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--param-random-seed", dest="param_random_seed", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-train-indexes", dest="file_path_train_indexes", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-test-indexes", dest="file_path_test_indexes", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-X-train", dest="output_X_train", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-X-test", dest="output_X_test", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-y-train", dest="output_y_train", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-y-test", dest="output_y_test", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = create_model_inputs(**_parsed_args)
      image: python:3.9
    inputs:
      parameters:
      - {name: param_random_seed}
      - {name: test_set_size}
      artifacts:
      - {name: construct-features-images-histomicstk-outfile_class_labels_pts_with_images,
        path: /tmp/inputs/file_path_class_labels/data}
      - {name: scale-df-2-output_csv, path: /tmp/inputs/file_path_features/data}
      - {name: construct-features-images-histomicstk-output_csv_test_indexes, path: /tmp/inputs/file_path_test_indexes/data}
      - {name: construct-features-images-histomicstk-output_csv_train_indexes, path: /tmp/inputs/file_path_train_indexes/data}
    outputs:
      artifacts:
      - {name: create-model-inputs-2-output_X_test, path: /tmp/outputs/output_X_test/data}
      - {name: create-model-inputs-2-output_X_train, path: /tmp/outputs/output_X_train/data}
      - {name: create-model-inputs-2-output_y_test, path: /tmp/outputs/output_y_test/data}
      - {name: create-model-inputs-2-output_y_train, path: /tmp/outputs/output_y_train/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Create
          train and test sets", "implementation": {"container": {"args": ["--file-path-features",
          {"inputPath": "file_path_features"}, "--file-path-class-labels", {"inputPath":
          "file_path_class_labels"}, "--test-set-size", {"inputValue": "test_set_size"},
          "--param-random-seed", {"inputValue": "param_random_seed"}, "--file-path-train-indexes",
          {"inputPath": "file_path_train_indexes"}, "--file-path-test-indexes", {"inputPath":
          "file_path_test_indexes"}, "--output-X-train", {"outputPath": "output_X_train"},
          "--output-X-test", {"outputPath": "output_X_test"}, "--output-y-train",
          {"outputPath": "output_y_train"}, "--output-y-test", {"outputPath": "output_y_test"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2''
          ''numpy'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''numpy''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef create_model_inputs(file_path_features,\n                        file_path_class_labels,\n                        test_set_size,\n                        param_random_seed,\n                        file_path_train_indexes,\n                        file_path_test_indexes,\n                        output_X_train,\n                        output_X_test,\n                        output_y_train,\n                        output_y_test):\n    \"\"\"Create
          train and test sets\n\n    Args:\n        file_path_features: A string containing
          path / pipeline component output\n        file_path_class_labels: A string
          containing path to data / pipeline component output\n        test_set_size:
          [optional, default=0.25] proportion to use for the test set\n    \"\"\"\n    import
          numpy as np\n\n    import pandas as pd\n    from sklearn import datasets\n    from
          sklearn.model_selection import train_test_split\n\n    X = pd.read_csv(filepath_or_buffer=file_path_features)\n    y
          = pd.read_csv(filepath_or_buffer=file_path_class_labels)\n\n    # load train/test
          indexes - computed with combine feature domains task\n    df_train_indexes
          = pd.read_csv(file_path_train_indexes)\n    df_test_indexes = pd.read_csv(file_path_test_indexes)\n    nparr_train_indexes
          = np.array(df_train_indexes[''index''])\n    nparr_test_indexes = np.array(df_test_indexes[''index''])\n\n    X_train
          = X.iloc[nparr_train_indexes]\n    y_train = y.iloc[nparr_train_indexes]\n    X_test
          = X.iloc[nparr_test_indexes]\n    y_test = y.iloc[nparr_test_indexes]\n\n    X_train.to_csv(output_X_train,
          header=True, index=False)\n    X_test.to_csv(output_X_test, header=True,
          index=False)\n    y_train.to_csv(output_y_train, header=True, index=False)\n    y_test.to_csv(output_y_test,
          header=True, index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Create
          model inputs'', description=''Create train and test sets'')\n_parser.add_argument(\"--file-path-features\",
          dest=\"file_path_features\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-class-labels\",
          dest=\"file_path_class_labels\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-set-size\",
          dest=\"test_set_size\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-train-indexes\",
          dest=\"file_path_train_indexes\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-test-indexes\",
          dest=\"file_path_test_indexes\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-X-train\",
          dest=\"output_X_train\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-X-test\", dest=\"output_X_test\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-y-train\",
          dest=\"output_y_train\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-y-test\", dest=\"output_y_test\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = create_model_inputs(**_parsed_args)\n"],
          "image": "python:3.9"}}, "inputs": [{"description": "A string containing
          path / pipeline component output", "name": "file_path_features", "type":
          "CSV"}, {"description": "A string containing path to data / pipeline component
          output", "name": "file_path_class_labels", "type": "CSV"}, {"description":
          "[optional, default=0.25] proportion to use for the test set", "name": "test_set_size",
          "type": "String"}, {"name": "param_random_seed", "type": "String"}, {"name":
          "file_path_train_indexes", "type": "CSV"}, {"name": "file_path_test_indexes",
          "type": "CSV"}], "name": "Create model inputs", "outputs": [{"name": "output_X_train",
          "type": "CSV"}, {"name": "output_X_test", "type": "CSV"}, {"name": "output_y_train",
          "type": "CSV"}, {"name": "output_y_test", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"param_random_seed": "{{inputs.parameters.param_random_seed}}",
          "test_set_size": "{{inputs.parameters.test_set_size}}"}'}
  - name: create-model-inputs-3
    container:
      args: [--file-path-features, /tmp/inputs/file_path_features/data, --file-path-class-labels,
        /tmp/inputs/file_path_class_labels/data, --test-set-size, '{{inputs.parameters.test_set_size}}',
        --param-random-seed, '{{inputs.parameters.param_random_seed}}', --file-path-train-indexes,
        /tmp/inputs/file_path_train_indexes/data, --file-path-test-indexes, /tmp/inputs/file_path_test_indexes/data,
        --output-X-train, /tmp/outputs/output_X_train/data, --output-X-test, /tmp/outputs/output_X_test/data,
        --output-y-train, /tmp/outputs/output_y_train/data, --output-y-test, /tmp/outputs/output_y_test/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' 'numpy' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        'numpy' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def create_model_inputs(file_path_features,
                                file_path_class_labels,
                                test_set_size,
                                param_random_seed,
                                file_path_train_indexes,
                                file_path_test_indexes,
                                output_X_train,
                                output_X_test,
                                output_y_train,
                                output_y_test):
            """Create train and test sets

            Args:
                file_path_features: A string containing path / pipeline component output
                file_path_class_labels: A string containing path to data / pipeline component output
                test_set_size: [optional, default=0.25] proportion to use for the test set
            """
            import numpy as np

            import pandas as pd
            from sklearn import datasets
            from sklearn.model_selection import train_test_split

            X = pd.read_csv(filepath_or_buffer=file_path_features)
            y = pd.read_csv(filepath_or_buffer=file_path_class_labels)

            # load train/test indexes - computed with combine feature domains task
            df_train_indexes = pd.read_csv(file_path_train_indexes)
            df_test_indexes = pd.read_csv(file_path_test_indexes)
            nparr_train_indexes = np.array(df_train_indexes['index'])
            nparr_test_indexes = np.array(df_test_indexes['index'])

            X_train = X.iloc[nparr_train_indexes]
            y_train = y.iloc[nparr_train_indexes]
            X_test = X.iloc[nparr_test_indexes]
            y_test = y.iloc[nparr_test_indexes]

            X_train.to_csv(output_X_train, header=True, index=False)
            X_test.to_csv(output_X_test, header=True, index=False)
            y_train.to_csv(output_y_train, header=True, index=False)
            y_test.to_csv(output_y_test, header=True, index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Create model inputs', description='Create train and test sets')
        _parser.add_argument("--file-path-features", dest="file_path_features", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-class-labels", dest="file_path_class_labels", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-set-size", dest="test_set_size", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--param-random-seed", dest="param_random_seed", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-train-indexes", dest="file_path_train_indexes", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--file-path-test-indexes", dest="file_path_test_indexes", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-X-train", dest="output_X_train", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-X-test", dest="output_X_test", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-y-train", dest="output_y_train", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-y-test", dest="output_y_test", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = create_model_inputs(**_parsed_args)
      image: python:3.9
    inputs:
      parameters:
      - {name: param_random_seed}
      - {name: test_set_size}
      artifacts:
      - {name: get-dummies-output_csv_target, path: /tmp/inputs/file_path_class_labels/data}
      - {name: scale-df-3-output_csv, path: /tmp/inputs/file_path_features/data}
      - {name: combine-feature-domains-output_csv_test_indexes, path: /tmp/inputs/file_path_test_indexes/data}
      - {name: combine-feature-domains-output_csv_train_indexes, path: /tmp/inputs/file_path_train_indexes/data}
    outputs:
      artifacts:
      - {name: create-model-inputs-3-output_X_test, path: /tmp/outputs/output_X_test/data}
      - {name: create-model-inputs-3-output_X_train, path: /tmp/outputs/output_X_train/data}
      - {name: create-model-inputs-3-output_y_test, path: /tmp/outputs/output_y_test/data}
      - {name: create-model-inputs-3-output_y_train, path: /tmp/outputs/output_y_train/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Create
          train and test sets", "implementation": {"container": {"args": ["--file-path-features",
          {"inputPath": "file_path_features"}, "--file-path-class-labels", {"inputPath":
          "file_path_class_labels"}, "--test-set-size", {"inputValue": "test_set_size"},
          "--param-random-seed", {"inputValue": "param_random_seed"}, "--file-path-train-indexes",
          {"inputPath": "file_path_train_indexes"}, "--file-path-test-indexes", {"inputPath":
          "file_path_test_indexes"}, "--output-X-train", {"outputPath": "output_X_train"},
          "--output-X-test", {"outputPath": "output_X_test"}, "--output-y-train",
          {"outputPath": "output_y_train"}, "--output-y-test", {"outputPath": "output_y_test"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2''
          ''numpy'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''numpy''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef create_model_inputs(file_path_features,\n                        file_path_class_labels,\n                        test_set_size,\n                        param_random_seed,\n                        file_path_train_indexes,\n                        file_path_test_indexes,\n                        output_X_train,\n                        output_X_test,\n                        output_y_train,\n                        output_y_test):\n    \"\"\"Create
          train and test sets\n\n    Args:\n        file_path_features: A string containing
          path / pipeline component output\n        file_path_class_labels: A string
          containing path to data / pipeline component output\n        test_set_size:
          [optional, default=0.25] proportion to use for the test set\n    \"\"\"\n    import
          numpy as np\n\n    import pandas as pd\n    from sklearn import datasets\n    from
          sklearn.model_selection import train_test_split\n\n    X = pd.read_csv(filepath_or_buffer=file_path_features)\n    y
          = pd.read_csv(filepath_or_buffer=file_path_class_labels)\n\n    # load train/test
          indexes - computed with combine feature domains task\n    df_train_indexes
          = pd.read_csv(file_path_train_indexes)\n    df_test_indexes = pd.read_csv(file_path_test_indexes)\n    nparr_train_indexes
          = np.array(df_train_indexes[''index''])\n    nparr_test_indexes = np.array(df_test_indexes[''index''])\n\n    X_train
          = X.iloc[nparr_train_indexes]\n    y_train = y.iloc[nparr_train_indexes]\n    X_test
          = X.iloc[nparr_test_indexes]\n    y_test = y.iloc[nparr_test_indexes]\n\n    X_train.to_csv(output_X_train,
          header=True, index=False)\n    X_test.to_csv(output_X_test, header=True,
          index=False)\n    y_train.to_csv(output_y_train, header=True, index=False)\n    y_test.to_csv(output_y_test,
          header=True, index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Create
          model inputs'', description=''Create train and test sets'')\n_parser.add_argument(\"--file-path-features\",
          dest=\"file_path_features\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-class-labels\",
          dest=\"file_path_class_labels\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-set-size\",
          dest=\"test_set_size\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-train-indexes\",
          dest=\"file_path_train_indexes\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-test-indexes\",
          dest=\"file_path_test_indexes\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-X-train\",
          dest=\"output_X_train\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-X-test\", dest=\"output_X_test\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-y-train\",
          dest=\"output_y_train\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-y-test\", dest=\"output_y_test\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = create_model_inputs(**_parsed_args)\n"],
          "image": "python:3.9"}}, "inputs": [{"description": "A string containing
          path / pipeline component output", "name": "file_path_features", "type":
          "CSV"}, {"description": "A string containing path to data / pipeline component
          output", "name": "file_path_class_labels", "type": "CSV"}, {"description":
          "[optional, default=0.25] proportion to use for the test set", "name": "test_set_size",
          "type": "String"}, {"name": "param_random_seed", "type": "String"}, {"name":
          "file_path_train_indexes", "type": "CSV"}, {"name": "file_path_test_indexes",
          "type": "CSV"}], "name": "Create model inputs", "outputs": [{"name": "output_X_train",
          "type": "CSV"}, {"name": "output_X_test", "type": "CSV"}, {"name": "output_y_train",
          "type": "CSV"}, {"name": "output_y_test", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"param_random_seed": "{{inputs.parameters.param_random_seed}}",
          "test_set_size": "{{inputs.parameters.test_set_size}}"}'}
  - name: download-data-kfp-sdk-v2
    container:
      args: []
      command:
      - sh
      - -exc
      - |
        url="$0"
        output_path="$1"
        curl_options="$2"
        mkdir -p "$(dirname "$output_path")"
        curl --get "$url" --output "$output_path" $curl_options
      - https://wiki.cancerimagingarchive.net/download/attachments/1966258/gdc_download_clinical_gbm.tar.gz
      - /tmp/outputs/Data/data
      - --location
      image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
    outputs:
      artifacts:
      - {name: download-data-kfp-sdk-v2-Data, path: /tmp/outputs/Data/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml',
        pipelines.kubeflow.org/component_spec: '{"description": "Downloads data from
          the specified URL. (Updated for KFP SDK v2.)", "implementation": {"container":
          {"command": ["sh", "-exc", "url=\"$0\"\noutput_path=\"$1\"\ncurl_options=\"$2\"\nmkdir
          -p \"$(dirname \"$output_path\")\"\ncurl --get \"$url\" --output \"$output_path\"
          $curl_options\n", {"inputValue": "Url"}, {"outputPath": "Data"}, {"inputValue":
          "curl options"}], "image": "byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342"}},
          "inputs": [{"name": "Url", "type": "String"}, {"default": "--location",
          "description": "Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html",
          "name": "curl options", "type": "String"}], "metadata": {"annotations":
          {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>", "canonical_location":
          "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml"}},
          "name": "Download data (KFP SDK v2)", "outputs": [{"name": "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "3eafccb9f368ff7282d1670d655c2e1b3ee8fdd241dd6b3d0e1d9de4b6da7c9b", "url":
          "./component-sdk-v2.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Url":
          "https://wiki.cancerimagingarchive.net/download/attachments/1966258/gdc_download_clinical_gbm.tar.gz",
          "curl options": "--location"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: download-images
    container:
      args: [--file-path-patient-list, /tmp/inputs/file_path_patient_list/data, --outfile-images-json,
        /tmp/outputs/outfile_images_json/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'girder_client==3.1.8' 'numpy==1.21.2' 'opencv-python-headless==4.5.5.62'
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'girder_client==3.1.8' 'numpy==1.21.2' 'opencv-python-headless==4.5.5.62'
        'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef download_images(file_path_patient_list,\n                    outfile_images_json):\n\
        \    \"\"\"Acquisition of images associated with patients listed\n       \
        \ in master `patient` table. \n\n    Args:\n        file_path: A string containing\
        \ path to the tarball.\n    \"\"\"\n    import json \n    from json import\
        \ JSONEncoder\n    import cv2\n    import girder_client\n    import numpy\
        \ as np\n    import pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n\
        \        def default(self, obj):\n            if isinstance(obj, np.ndarray):\n\
        \                return obj.tolist()\n            return JSONEncoder.default(self,\
        \ obj)\n\n    gc = girder_client.GirderClient(apiUrl=\"https://api.digitalslidearchive.org/api/v1\"\
        )\n\n    # list of patients; header col name = \"bcr_patient_barcode\"\n \
        \   df_patient_list = pd.read_csv(file_path_patient_list)\n\n    ## FYI: image\
        \ name format:\n    ##    <patient_id>-01Z-00-DX1.<string>.svs\n    ##   \
        \ TCGA-02-0038-01Z-00-DX1.5E369837-371E-4845-AD78-84BB48E1A082.svs\n\n   \
        \ # retrieve images by ptID in list\n    l_imagelist_ptID = []\n    l_imagelist_imgID\
        \ = []\n    l_imagelist_imgContent = []\n    for ptID in df_patient_list['bcr_patient_barcode']:\n\
        \n        # get case metadata for this patient\n        caseMetadata = gc.get('tcga/case/label/%s'\
        \ % ptID)\n        # get caseId for this patient\n        caseId =  caseMetadata['tcga']['caseId']\n\
        \n        # Get images for this case ID..\n        imageData = gc.get(\"/tcga/case/%s/images\"\
        \ % caseId)\n\n        for i in imageData['data']:\n            if i['name'].split('.')[0].split('-')[-1]\
        \ == 'DX1':\n                print(i['name'],i['_id'])\n                l_imagelist_ptID.append(ptID)\n\
        \                l_imagelist_imgID.append(i['_id'])\n\n    for i in range(len(l_imagelist_imgID)):\n\
        \        # retrieve image content with image ID\n        imgID = l_imagelist_imgID[i]\n\
        \        imageThumb = gc.get(\"item/%s/tiles/thumbnail\" % imgID,jsonResp=False)\n\
        \        img_array = np.frombuffer(imageThumb.content, dtype=np.uint8)\n \
        \       img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n        l_imagelist_imgContent.append(img)\n\
        \n    # image shape\n    img_shape = l_imagelist_imgContent[0].shape # assume\
        \ all images are same shape\n    df_img_shape = pd.DataFrame([])\n    df_img_shape['img_shape']\
        \ = img_shape\n\n    # JSON of all images\n    d_images = dict()\n    for\
        \ i in range(len(l_imagelist_ptID)):\n        ptID = l_imagelist_ptID[i]\n\
        \        if ptID not in d_images:\n            d_images[ptID] = []\n     \
        \   d_images[ptID].append(l_imagelist_imgContent[i])\n    json_string_output\
        \ = json.dumps(d_images, cls=NumpyArrayEncoder)\n\n    # write outputs\n \
        \   ## write json file\n    with open(outfile_images_json, 'w') as outfile:\n\
        \        outfile.write(json_string_output)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Download\
        \ images', description='Acquisition of images associated with patients listed')\n\
        _parser.add_argument(\"--file-path-patient-list\", dest=\"file_path_patient_list\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --outfile-images-json\", dest=\"outfile_images_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = download_images(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: process-data-tarball-output_patient_id_list, path: /tmp/inputs/file_path_patient_list/data}
    outputs:
      artifacts:
      - {name: download-images-outfile_images_json, path: /tmp/outputs/outfile_images_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Acquisition
          of images associated with patients listed", "implementation": {"container":
          {"args": ["--file-path-patient-list", {"inputPath": "file_path_patient_list"},
          "--outfile-images-json", {"outputPath": "outfile_images_json"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''girder_client==3.1.8'' ''numpy==1.21.2'' ''opencv-python-headless==4.5.5.62''
          ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''girder_client==3.1.8'' ''numpy==1.21.2''
          ''opencv-python-headless==4.5.5.62'' ''pandas==1.1.4'' --user) && \"$0\"
          \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef download_images(file_path_patient_list,\n                    outfile_images_json):\n    \"\"\"Acquisition
          of images associated with patients listed\n        in master `patient` table.
          \n\n    Args:\n        file_path: A string containing path to the tarball.\n    \"\"\"\n    import
          json \n    from json import JSONEncoder\n    import cv2\n    import girder_client\n    import
          numpy as np\n    import pandas as pd\n\n    class NumpyArrayEncoder(JSONEncoder):\n        def
          default(self, obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return JSONEncoder.default(self, obj)\n\n    gc
          = girder_client.GirderClient(apiUrl=\"https://api.digitalslidearchive.org/api/v1\")\n\n    #
          list of patients; header col name = \"bcr_patient_barcode\"\n    df_patient_list
          = pd.read_csv(file_path_patient_list)\n\n    ## FYI: image name format:\n    ##    <patient_id>-01Z-00-DX1.<string>.svs\n    ##    TCGA-02-0038-01Z-00-DX1.5E369837-371E-4845-AD78-84BB48E1A082.svs\n\n    #
          retrieve images by ptID in list\n    l_imagelist_ptID = []\n    l_imagelist_imgID
          = []\n    l_imagelist_imgContent = []\n    for ptID in df_patient_list[''bcr_patient_barcode'']:\n\n        #
          get case metadata for this patient\n        caseMetadata = gc.get(''tcga/case/label/%s''
          % ptID)\n        # get caseId for this patient\n        caseId =  caseMetadata[''tcga''][''caseId'']\n\n        #
          Get images for this case ID..\n        imageData = gc.get(\"/tcga/case/%s/images\"
          % caseId)\n\n        for i in imageData[''data'']:\n            if i[''name''].split(''.'')[0].split(''-'')[-1]
          == ''DX1'':\n                print(i[''name''],i[''_id''])\n                l_imagelist_ptID.append(ptID)\n                l_imagelist_imgID.append(i[''_id''])\n\n    for
          i in range(len(l_imagelist_imgID)):\n        # retrieve image content with
          image ID\n        imgID = l_imagelist_imgID[i]\n        imageThumb = gc.get(\"item/%s/tiles/thumbnail\"
          % imgID,jsonResp=False)\n        img_array = np.frombuffer(imageThumb.content,
          dtype=np.uint8)\n        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n        l_imagelist_imgContent.append(img)\n\n    #
          image shape\n    img_shape = l_imagelist_imgContent[0].shape # assume all
          images are same shape\n    df_img_shape = pd.DataFrame([])\n    df_img_shape[''img_shape'']
          = img_shape\n\n    # JSON of all images\n    d_images = dict()\n    for
          i in range(len(l_imagelist_ptID)):\n        ptID = l_imagelist_ptID[i]\n        if
          ptID not in d_images:\n            d_images[ptID] = []\n        d_images[ptID].append(l_imagelist_imgContent[i])\n    json_string_output
          = json.dumps(d_images, cls=NumpyArrayEncoder)\n\n    # write outputs\n    ##
          write json file\n    with open(outfile_images_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Download images'', description=''Acquisition
          of images associated with patients listed'')\n_parser.add_argument(\"--file-path-patient-list\",
          dest=\"file_path_patient_list\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--outfile-images-json\",
          dest=\"outfile_images_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = download_images(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "file_path_patient_list", "type": "CSV"}], "name": "Download images",
          "outputs": [{"name": "outfile_images_json", "type": "JSON"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: get-dummies
    container:
      args: [--file, /tmp/inputs/file/data, --file-path-patient-list, /tmp/inputs/file_path_patient_list/data,
        --class-label-colname, '{{inputs.parameters.user_input_class_label_column_name}}',
        --s-colnames-to-exclude, '{{inputs.parameters.s_colnames_to_exclude}}', --param-test-set-size,
        '{{inputs.parameters.test_set_size}}', --param-random-seed, '{{inputs.parameters.param_random_seed}}',
        --output-csv-features, /tmp/outputs/output_csv_features/data, --output-csv-target,
        /tmp/outputs/output_csv_target/data, --output-csv-target-class-labels, /tmp/outputs/output_csv_target_class_labels/data,
        --output-csv-feature-list, /tmp/outputs/output_csv_feature_list/data, --output-csv-patient-list-filtered,
        /tmp/outputs/output_csv_patient_list_filtered/data, --output-csv-train-indexes,
        /tmp/outputs/output_csv_train_indexes/data, --output-csv-test-indexes, /tmp/outputs/output_csv_test_indexes/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.1.4' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef get_dummies(file_path,\n                file_path_patient_list,\n  \
        \              class_label_colname,\n                s_colnames_to_exclude,\n\
        \                param_test_set_size,\n                param_random_seed,\n\
        \                output_csv_features,\n                output_csv_target,\n\
        \                output_csv_target_class_labels,\n                output_csv_feature_list,\n\
        \                output_csv_patient_list_filtered,\n                output_csv_train_indexes,\n\
        \                output_csv_test_indexes):\n    \"\"\"Distribute categorical\
        \ features into separate features.\n        Input: CSV with categorical (and\
        \ numeric) features. Assume last \n            feature is target label. \n\
        \        Output: CSV with categorical features separated into dummies.\n\n\
        \    Args:\n        file_path: A string containing path to input data.\n \
        \       output_csv: A string containing path to processed data.\n    \"\"\"\
        \n    import glob\n    import numpy as np\n    import pandas as pd\n    from\
        \ sklearn import preprocessing\n    from sklearn.model_selection import train_test_split\n\
        \n    df = pd.read_csv(filepath_or_buffer=file_path)\n    l_col_names = list(df.columns)\n\
        \n    # isolate column for target class label; if null, use last column\n\
        \    target_class_label = class_label_colname if str(class_label_colname)\
        \ != '' else l_col_names[-1]\n    ## remove all rows where class label col\
        \ is NaN\n    df = df.dropna(subset=[target_class_label])\n    ## remove patients\
        \ from master list whose data got removed because class label col was NaN\n\
        \    df_patient_list = pd.read_csv(file_path_patient_list)\n    df_patient_list_filtered\
        \ = df_patient_list.loc[df.index]\n\n    ## extract target class column\n\
        \    df_target_raw = df[target_class_label] # from input parameter\n    lb\
        \ = preprocessing.LabelBinarizer()\n    lb.fit(df_target_raw.astype(str))\
        \ # fit to data for target class to find all classes\n    target_class_label_names\
        \ = lb.classes_ # store array of all the classes\n#    df_target = pd.DataFrame(np.array(lb.fit_transform(df_target_raw)))\n\
        \n    df_target_class_labels = pd.DataFrame({'class': target_class_label_names})\n\
        \    d_target_class_label_idx = dict(zip(list(df_target_class_labels['class']),\
        \ list(df_target_class_labels.index)))\n\n    l_target_column = [d_target_class_label_idx[x]\
        \ for x in df[target_class_label]]\n    df_target = pd.DataFrame([])\n   \
        \ df_target[target_class_label] = l_target_column# 1-column of multiple classes\n\
        \n    # create dummies for every col except class label col\n    df_features\
        \ = df[[x for x in l_col_names if x != class_label_colname]] # features are\
        \ all colnames except target class\n\n    # exclude any other colnames (other\
        \ than target class column), if specified; \n    ## performed on feature matrix\
        \ after dummies are removed\n    if str(s_colnames_to_exclude) != '':\n  \
        \      l_colnames_to_exclude = s_colnames_to_exclude.split(',')\n        l_colnames_in_this_df_to_exclude\
        \ = [x for x in l_colnames_to_exclude if x in df.columns]\n        df_features\
        \ = df_features.drop(l_colnames_to_exclude, axis = 1).reset_index() # reset_index\
        \ becauase need to get indexes for train/test later on data with rows filtered\
        \ out\n\n    # get dummies\n    df_features_dummies = pd.get_dummies(df_features)\n\
        \n    # feature list\n    l_features = df_features_dummies.columns\n    df_feature_list\
        \ = pd.DataFrame({'feature': l_features})\n\n    # create train test and store\
        \ indexes, using these clinical features\n    param_test_set_size_float =\
        \ np.float64(param_test_set_size)\n\n    ## set a valid value of 0.25 for\
        \ test set size if needed\n    if param_test_set_size_float < 0 or param_test_set_size_float\
        \ > 1:\n        param_test_set_size_float = 0.25    \n\n    param_random_seed_int\
        \ = int(np.float64(param_random_seed))\n    X_train, X_test, y_train, y_test\
        \ = train_test_split(df_features_dummies, df_target, \n                  \
        \                                  test_size=param_test_set_size_float, \n\
        \                                                    random_state=param_random_seed_int)\
        \ # IMPT: set random seed\n    df_idx_train = X_train.index.to_frame().rename(columns={0:\
        \ \"index\"})\n    df_idx_train['bcr_patient_barcode'] = df_patient_list_filtered.iloc[X_train.index]\n\
        \    df_idx_test = X_test.index.to_frame().rename(columns={0: \"index\"})\n\
        \    df_idx_test['bcr_patient_barcode'] = df_patient_list_filtered.iloc[X_test.index]\n\
        \n    # write outputs\n    df_features_dummies.to_csv(output_csv_features,\
        \ index = False, header = True)\n    df_target.to_csv(output_csv_target, index\
        \ = False, header = True)\n    df_target_class_labels.to_csv(output_csv_target_class_labels,\
        \ index = True, header = False)\n    df_feature_list.to_csv(output_csv_feature_list,\
        \ index = True, header = False)\n    ### save new filtered patient list\n\
        \    df_patient_list_filtered.to_csv(output_csv_patient_list_filtered, header=True,\
        \ index=False)\n    ### save training and testing splits - to use with all\
        \ fused feature matrices from now on\n    df_idx_train.to_csv(output_csv_train_indexes,\
        \ index = False, header = True)\n    df_idx_test.to_csv(output_csv_test_indexes,\
        \ index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Get\
        \ dummies', description='Distribute categorical features into separate features.')\n\
        _parser.add_argument(\"--file\", dest=\"file_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-patient-list\"\
        , dest=\"file_path_patient_list\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--class-label-colname\", dest=\"class_label_colname\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --s-colnames-to-exclude\", dest=\"s_colnames_to_exclude\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-test-set-size\"\
        , dest=\"param_test_set_size\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--param-random-seed\", dest=\"param_random_seed\",\
        \ type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --output-csv-features\", dest=\"output_csv_features\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-target\"\
        , dest=\"output_csv_target\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-target-class-labels\"\
        , dest=\"output_csv_target_class_labels\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-feature-list\"\
        , dest=\"output_csv_feature_list\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-patient-list-filtered\"\
        , dest=\"output_csv_patient_list_filtered\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-train-indexes\"\
        , dest=\"output_csv_train_indexes\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-test-indexes\"\
        , dest=\"output_csv_test_indexes\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = get_dummies(**_parsed_args)\n"
      image: python:3.7
    inputs:
      parameters:
      - {name: param_random_seed}
      - {name: s_colnames_to_exclude}
      - {name: test_set_size}
      - {name: user_input_class_label_column_name}
      artifacts:
      - {name: process-data-tarball-output_master_df, path: /tmp/inputs/file/data}
      - {name: process-data-tarball-output_patient_id_list, path: /tmp/inputs/file_path_patient_list/data}
    outputs:
      artifacts:
      - {name: get-dummies-output_csv_feature_list, path: /tmp/outputs/output_csv_feature_list/data}
      - {name: get-dummies-output_csv_features, path: /tmp/outputs/output_csv_features/data}
      - {name: get-dummies-output_csv_patient_list_filtered, path: /tmp/outputs/output_csv_patient_list_filtered/data}
      - {name: get-dummies-output_csv_target, path: /tmp/outputs/output_csv_target/data}
      - {name: get-dummies-output_csv_target_class_labels, path: /tmp/outputs/output_csv_target_class_labels/data}
      - {name: get-dummies-output_csv_test_indexes, path: /tmp/outputs/output_csv_test_indexes/data}
      - {name: get-dummies-output_csv_train_indexes, path: /tmp/outputs/output_csv_train_indexes/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Distribute
          categorical features into separate features.", "implementation": {"container":
          {"args": ["--file", {"inputPath": "file"}, "--file-path-patient-list", {"inputPath":
          "file_path_patient_list"}, "--class-label-colname", {"inputValue": "class_label_colname"},
          "--s-colnames-to-exclude", {"inputValue": "s_colnames_to_exclude"}, "--param-test-set-size",
          {"inputValue": "param_test_set_size"}, "--param-random-seed", {"inputValue":
          "param_random_seed"}, "--output-csv-features", {"outputPath": "output_csv_features"},
          "--output-csv-target", {"outputPath": "output_csv_target"}, "--output-csv-target-class-labels",
          {"outputPath": "output_csv_target_class_labels"}, "--output-csv-feature-list",
          {"outputPath": "output_csv_feature_list"}, "--output-csv-patient-list-filtered",
          {"outputPath": "output_csv_patient_list_filtered"}, "--output-csv-train-indexes",
          {"outputPath": "output_csv_train_indexes"}, "--output-csv-test-indexes",
          {"outputPath": "output_csv_test_indexes"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.1.4'' ''scikit-learn==1.0.2''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef get_dummies(file_path,\n                file_path_patient_list,\n                class_label_colname,\n                s_colnames_to_exclude,\n                param_test_set_size,\n                param_random_seed,\n                output_csv_features,\n                output_csv_target,\n                output_csv_target_class_labels,\n                output_csv_feature_list,\n                output_csv_patient_list_filtered,\n                output_csv_train_indexes,\n                output_csv_test_indexes):\n    \"\"\"Distribute
          categorical features into separate features.\n        Input: CSV with categorical
          (and numeric) features. Assume last \n            feature is target label.
          \n        Output: CSV with categorical features separated into dummies.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import glob\n    import numpy as
          np\n    import pandas as pd\n    from sklearn import preprocessing\n    from
          sklearn.model_selection import train_test_split\n\n    df = pd.read_csv(filepath_or_buffer=file_path)\n    l_col_names
          = list(df.columns)\n\n    # isolate column for target class label; if null,
          use last column\n    target_class_label = class_label_colname if str(class_label_colname)
          != '''' else l_col_names[-1]\n    ## remove all rows where class label col
          is NaN\n    df = df.dropna(subset=[target_class_label])\n    ## remove patients
          from master list whose data got removed because class label col was NaN\n    df_patient_list
          = pd.read_csv(file_path_patient_list)\n    df_patient_list_filtered = df_patient_list.loc[df.index]\n\n    ##
          extract target class column\n    df_target_raw = df[target_class_label]
          # from input parameter\n    lb = preprocessing.LabelBinarizer()\n    lb.fit(df_target_raw.astype(str))
          # fit to data for target class to find all classes\n    target_class_label_names
          = lb.classes_ # store array of all the classes\n#    df_target = pd.DataFrame(np.array(lb.fit_transform(df_target_raw)))\n\n    df_target_class_labels
          = pd.DataFrame({''class'': target_class_label_names})\n    d_target_class_label_idx
          = dict(zip(list(df_target_class_labels[''class'']), list(df_target_class_labels.index)))\n\n    l_target_column
          = [d_target_class_label_idx[x] for x in df[target_class_label]]\n    df_target
          = pd.DataFrame([])\n    df_target[target_class_label] = l_target_column#
          1-column of multiple classes\n\n    # create dummies for every col except
          class label col\n    df_features = df[[x for x in l_col_names if x != class_label_colname]]
          # features are all colnames except target class\n\n    # exclude any other
          colnames (other than target class column), if specified; \n    ## performed
          on feature matrix after dummies are removed\n    if str(s_colnames_to_exclude)
          != '''':\n        l_colnames_to_exclude = s_colnames_to_exclude.split('','')\n        l_colnames_in_this_df_to_exclude
          = [x for x in l_colnames_to_exclude if x in df.columns]\n        df_features
          = df_features.drop(l_colnames_to_exclude, axis = 1).reset_index() # reset_index
          becauase need to get indexes for train/test later on data with rows filtered
          out\n\n    # get dummies\n    df_features_dummies = pd.get_dummies(df_features)\n\n    #
          feature list\n    l_features = df_features_dummies.columns\n    df_feature_list
          = pd.DataFrame({''feature'': l_features})\n\n    # create train test and
          store indexes, using these clinical features\n    param_test_set_size_float
          = np.float64(param_test_set_size)\n\n    ## set a valid value of 0.25 for
          test set size if needed\n    if param_test_set_size_float < 0 or param_test_set_size_float
          > 1:\n        param_test_set_size_float = 0.25    \n\n    param_random_seed_int
          = int(np.float64(param_random_seed))\n    X_train, X_test, y_train, y_test
          = train_test_split(df_features_dummies, df_target, \n                                                    test_size=param_test_set_size_float,
          \n                                                    random_state=param_random_seed_int)
          # IMPT: set random seed\n    df_idx_train = X_train.index.to_frame().rename(columns={0:
          \"index\"})\n    df_idx_train[''bcr_patient_barcode''] = df_patient_list_filtered.iloc[X_train.index]\n    df_idx_test
          = X_test.index.to_frame().rename(columns={0: \"index\"})\n    df_idx_test[''bcr_patient_barcode'']
          = df_patient_list_filtered.iloc[X_test.index]\n\n    # write outputs\n    df_features_dummies.to_csv(output_csv_features,
          index = False, header = True)\n    df_target.to_csv(output_csv_target, index
          = False, header = True)\n    df_target_class_labels.to_csv(output_csv_target_class_labels,
          index = True, header = False)\n    df_feature_list.to_csv(output_csv_feature_list,
          index = True, header = False)\n    ### save new filtered patient list\n    df_patient_list_filtered.to_csv(output_csv_patient_list_filtered,
          header=True, index=False)\n    ### save training and testing splits - to
          use with all fused feature matrices from now on\n    df_idx_train.to_csv(output_csv_train_indexes,
          index = False, header = True)\n    df_idx_test.to_csv(output_csv_test_indexes,
          index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Get
          dummies'', description=''Distribute categorical features into separate features.'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-patient-list\",
          dest=\"file_path_patient_list\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--class-label-colname\",
          dest=\"class_label_colname\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s-colnames-to-exclude\",
          dest=\"s_colnames_to_exclude\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-test-set-size\",
          dest=\"param_test_set_size\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-features\",
          dest=\"output_csv_features\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-target\",
          dest=\"output_csv_target\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-target-class-labels\",
          dest=\"output_csv_target_class_labels\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-feature-list\",
          dest=\"output_csv_feature_list\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-patient-list-filtered\",
          dest=\"output_csv_patient_list_filtered\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-train-indexes\",
          dest=\"output_csv_train_indexes\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv-test-indexes\",
          dest=\"output_csv_test_indexes\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = get_dummies(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"description":
          "A string containing path to input data.", "name": "file", "type": "CSV"},
          {"name": "file_path_patient_list", "type": "CSV"}, {"name": "class_label_colname",
          "type": "String"}, {"name": "s_colnames_to_exclude", "type": "String"},
          {"name": "param_test_set_size", "type": "String"}, {"name": "param_random_seed",
          "type": "String"}], "name": "Get dummies", "outputs": [{"name": "output_csv_features",
          "type": "CSV"}, {"name": "output_csv_target", "type": "CSV"}, {"name": "output_csv_target_class_labels",
          "type": "CSV"}, {"name": "output_csv_feature_list", "type": "CSV"}, {"name":
          "output_csv_patient_list_filtered", "type": "CSV"}, {"name": "output_csv_train_indexes",
          "type": "CSV"}, {"name": "output_csv_test_indexes", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"class_label_colname": "{{inputs.parameters.user_input_class_label_column_name}}",
          "param_random_seed": "{{inputs.parameters.param_random_seed}}", "param_test_set_size":
          "{{inputs.parameters.test_set_size}}", "s_colnames_to_exclude": "{{inputs.parameters.s_colnames_to_exclude}}"}'}
  - name: impute-unknown
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def impute_unknown(file_path,
                           output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import numpy as np
            import pandas as pd
            from sklearn.impute import SimpleImputer

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # Impute: most common
            imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
            imp_most_frequent.fit(df)
            nparr_imputed = imp_most_frequent.transform(df)
            df_imputed = pd.DataFrame(nparr_imputed)
            df_imputed.columns = df.columns

            # Output to CSV
            df_imputed.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Impute unknown', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = impute_unknown(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: get-dummies-output_csv_features, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: impute-unknown-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef impute_unknown(file_path,\n                   output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import numpy as np\n    import
          pandas as pd\n    from sklearn.impute import SimpleImputer\n\n    # Read
          in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    # Impute:
          most common\n    imp_most_frequent = SimpleImputer(missing_values=np.nan,
          strategy=''most_frequent'')\n    imp_most_frequent.fit(df)\n    nparr_imputed
          = imp_most_frequent.transform(df)\n    df_imputed = pd.DataFrame(nparr_imputed)\n    df_imputed.columns
          = df.columns\n\n    # Output to CSV\n    df_imputed.to_csv(output_csv, index
          = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Impute
          unknown'', description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = impute_unknown(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs":
          [{"description": "A string containing path to input data.", "name": "file",
          "type": "CSV"}], "name": "Impute unknown", "outputs": [{"description": "A
          string containing path to processed data.", "name": "output_csv", "type":
          "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: impute-unknown-2
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def impute_unknown(file_path,
                           output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import numpy as np
            import pandas as pd
            from sklearn.impute import SimpleImputer

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # Impute: most common
            imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
            imp_most_frequent.fit(df)
            nparr_imputed = imp_most_frequent.transform(df)
            df_imputed = pd.DataFrame(nparr_imputed)
            df_imputed.columns = df.columns

            # Output to CSV
            df_imputed.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Impute unknown', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = impute_unknown(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: construct-features-images-histomicstk-outfile_features_df, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: impute-unknown-2-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef impute_unknown(file_path,\n                   output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import numpy as np\n    import
          pandas as pd\n    from sklearn.impute import SimpleImputer\n\n    # Read
          in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    # Impute:
          most common\n    imp_most_frequent = SimpleImputer(missing_values=np.nan,
          strategy=''most_frequent'')\n    imp_most_frequent.fit(df)\n    nparr_imputed
          = imp_most_frequent.transform(df)\n    df_imputed = pd.DataFrame(nparr_imputed)\n    df_imputed.columns
          = df.columns\n\n    # Output to CSV\n    df_imputed.to_csv(output_csv, index
          = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Impute
          unknown'', description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = impute_unknown(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs":
          [{"description": "A string containing path to input data.", "name": "file",
          "type": "CSV"}], "name": "Impute unknown", "outputs": [{"description": "A
          string containing path to processed data.", "name": "output_csv", "type":
          "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: impute-unknown-3
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def impute_unknown(file_path,
                           output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import numpy as np
            import pandas as pd
            from sklearn.impute import SimpleImputer

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # Impute: most common
            imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
            imp_most_frequent.fit(df)
            nparr_imputed = imp_most_frequent.transform(df)
            df_imputed = pd.DataFrame(nparr_imputed)
            df_imputed.columns = df.columns

            # Output to CSV
            df_imputed.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Impute unknown', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = impute_unknown(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: combine-feature-domains-outfile_master_features_df, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: impute-unknown-3-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef impute_unknown(file_path,\n                   output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import numpy as np\n    import
          pandas as pd\n    from sklearn.impute import SimpleImputer\n\n    # Read
          in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    # Impute:
          most common\n    imp_most_frequent = SimpleImputer(missing_values=np.nan,
          strategy=''most_frequent'')\n    imp_most_frequent.fit(df)\n    nparr_imputed
          = imp_most_frequent.transform(df)\n    df_imputed = pd.DataFrame(nparr_imputed)\n    df_imputed.columns
          = df.columns\n\n    # Output to CSV\n    df_imputed.to_csv(output_csv, index
          = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Impute
          unknown'', description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = impute_unknown(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs":
          [{"description": "A string containing path to input data.", "name": "file",
          "type": "CSV"}], "name": "Impute unknown", "outputs": [{"description": "A
          string containing path to processed data.", "name": "output_csv", "type":
          "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: pl-gbm-v3-early-fusion-clinical-images
    inputs:
      parameters:
      - {name: param_random_seed}
      - {name: s_colnames_to_exclude}
      - {name: test_set_size}
      - {name: user_input_class_label_column_name}
    dag:
      tasks:
      - name: combine-feature-domains
        template: combine-feature-domains
        dependencies: [construct-features-images-histomicstk, get-dummies]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          - {name: test_set_size, value: '{{inputs.parameters.test_set_size}}'}
          artifacts:
          - {name: construct-features-images-histomicstk-outfile_features_df, from: '{{tasks.construct-features-images-histomicstk.outputs.artifacts.construct-features-images-histomicstk-outfile_features_df}}'}
          - {name: construct-features-images-histomicstk-outfile_patient_id_list,
            from: '{{tasks.construct-features-images-histomicstk.outputs.artifacts.construct-features-images-histomicstk-outfile_patient_id_list}}'}
          - {name: get-dummies-output_csv_features, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_features}}'}
          - {name: get-dummies-output_csv_patient_list_filtered, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_patient_list_filtered}}'}
      - name: construct-features-images-histomicstk
        template: construct-features-images-histomicstk
        dependencies: [download-images, get-dummies]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          - {name: test_set_size, value: '{{inputs.parameters.test_set_size}}'}
          artifacts:
          - {name: download-images-outfile_images_json, from: '{{tasks.download-images.outputs.artifacts.download-images-outfile_images_json}}'}
          - {name: get-dummies-output_csv_patient_list_filtered, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_patient_list_filtered}}'}
          - {name: get-dummies-output_csv_target, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_target}}'}
      - name: create-model-inputs
        template: create-model-inputs
        dependencies: [get-dummies, scale-df]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          - {name: test_set_size, value: '{{inputs.parameters.test_set_size}}'}
          artifacts:
          - {name: get-dummies-output_csv_target, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_target}}'}
          - {name: get-dummies-output_csv_test_indexes, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_test_indexes}}'}
          - {name: get-dummies-output_csv_train_indexes, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_train_indexes}}'}
          - {name: scale-df-output_csv, from: '{{tasks.scale-df.outputs.artifacts.scale-df-output_csv}}'}
      - name: create-model-inputs-2
        template: create-model-inputs-2
        dependencies: [construct-features-images-histomicstk, scale-df-2]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          - {name: test_set_size, value: '{{inputs.parameters.test_set_size}}'}
          artifacts:
          - {name: construct-features-images-histomicstk-outfile_class_labels_pts_with_images,
            from: '{{tasks.construct-features-images-histomicstk.outputs.artifacts.construct-features-images-histomicstk-outfile_class_labels_pts_with_images}}'}
          - {name: construct-features-images-histomicstk-output_csv_test_indexes,
            from: '{{tasks.construct-features-images-histomicstk.outputs.artifacts.construct-features-images-histomicstk-output_csv_test_indexes}}'}
          - {name: construct-features-images-histomicstk-output_csv_train_indexes,
            from: '{{tasks.construct-features-images-histomicstk.outputs.artifacts.construct-features-images-histomicstk-output_csv_train_indexes}}'}
          - {name: scale-df-2-output_csv, from: '{{tasks.scale-df-2.outputs.artifacts.scale-df-2-output_csv}}'}
      - name: create-model-inputs-3
        template: create-model-inputs-3
        dependencies: [combine-feature-domains, get-dummies, scale-df-3]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          - {name: test_set_size, value: '{{inputs.parameters.test_set_size}}'}
          artifacts:
          - {name: combine-feature-domains-output_csv_test_indexes, from: '{{tasks.combine-feature-domains.outputs.artifacts.combine-feature-domains-output_csv_test_indexes}}'}
          - {name: combine-feature-domains-output_csv_train_indexes, from: '{{tasks.combine-feature-domains.outputs.artifacts.combine-feature-domains-output_csv_train_indexes}}'}
          - {name: get-dummies-output_csv_target, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_target}}'}
          - {name: scale-df-3-output_csv, from: '{{tasks.scale-df-3.outputs.artifacts.scale-df-3-output_csv}}'}
      - {name: download-data-kfp-sdk-v2, template: download-data-kfp-sdk-v2}
      - name: download-images
        template: download-images
        dependencies: [process-data-tarball]
        arguments:
          artifacts:
          - {name: process-data-tarball-output_patient_id_list, from: '{{tasks.process-data-tarball.outputs.artifacts.process-data-tarball-output_patient_id_list}}'}
      - name: get-dummies
        template: get-dummies
        dependencies: [process-data-tarball]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          - {name: s_colnames_to_exclude, value: '{{inputs.parameters.s_colnames_to_exclude}}'}
          - {name: test_set_size, value: '{{inputs.parameters.test_set_size}}'}
          - {name: user_input_class_label_column_name, value: '{{inputs.parameters.user_input_class_label_column_name}}'}
          artifacts:
          - {name: process-data-tarball-output_master_df, from: '{{tasks.process-data-tarball.outputs.artifacts.process-data-tarball-output_master_df}}'}
          - {name: process-data-tarball-output_patient_id_list, from: '{{tasks.process-data-tarball.outputs.artifacts.process-data-tarball-output_patient_id_list}}'}
      - name: impute-unknown
        template: impute-unknown
        dependencies: [get-dummies]
        arguments:
          artifacts:
          - {name: get-dummies-output_csv_features, from: '{{tasks.get-dummies.outputs.artifacts.get-dummies-output_csv_features}}'}
      - name: impute-unknown-2
        template: impute-unknown-2
        dependencies: [construct-features-images-histomicstk]
        arguments:
          artifacts:
          - {name: construct-features-images-histomicstk-outfile_features_df, from: '{{tasks.construct-features-images-histomicstk.outputs.artifacts.construct-features-images-histomicstk-outfile_features_df}}'}
      - name: impute-unknown-3
        template: impute-unknown-3
        dependencies: [combine-feature-domains]
        arguments:
          artifacts:
          - {name: combine-feature-domains-outfile_master_features_df, from: '{{tasks.combine-feature-domains.outputs.artifacts.combine-feature-domains-outfile_master_features_df}}'}
      - name: process-data-tarball
        template: process-data-tarball
        dependencies: [download-data-kfp-sdk-v2]
        arguments:
          artifacts:
          - {name: download-data-kfp-sdk-v2-Data, from: '{{tasks.download-data-kfp-sdk-v2.outputs.artifacts.download-data-kfp-sdk-v2-Data}}'}
      - name: scale-df
        template: scale-df
        dependencies: [impute-unknown]
        arguments:
          artifacts:
          - {name: impute-unknown-output_csv, from: '{{tasks.impute-unknown.outputs.artifacts.impute-unknown-output_csv}}'}
      - name: scale-df-2
        template: scale-df-2
        dependencies: [impute-unknown-2]
        arguments:
          artifacts:
          - {name: impute-unknown-2-output_csv, from: '{{tasks.impute-unknown-2.outputs.artifacts.impute-unknown-2-output_csv}}'}
      - name: scale-df-3
        template: scale-df-3
        dependencies: [impute-unknown-3]
        arguments:
          artifacts:
          - {name: impute-unknown-3-output_csv, from: '{{tasks.impute-unknown-3.outputs.artifacts.impute-unknown-3-output_csv}}'}
      - name: test-model
        template: test-model
        dependencies: [create-model-inputs, train-model]
        arguments:
          artifacts:
          - {name: create-model-inputs-output_X_test, from: '{{tasks.create-model-inputs.outputs.artifacts.create-model-inputs-output_X_test}}'}
          - {name: create-model-inputs-output_y_test, from: '{{tasks.create-model-inputs.outputs.artifacts.create-model-inputs-output_y_test}}'}
          - {name: train-model-output_model, from: '{{tasks.train-model.outputs.artifacts.train-model-output_model}}'}
      - name: test-model-2
        template: test-model-2
        dependencies: [create-model-inputs-2, train-model-2]
        arguments:
          artifacts:
          - {name: create-model-inputs-2-output_X_test, from: '{{tasks.create-model-inputs-2.outputs.artifacts.create-model-inputs-2-output_X_test}}'}
          - {name: create-model-inputs-2-output_y_test, from: '{{tasks.create-model-inputs-2.outputs.artifacts.create-model-inputs-2-output_y_test}}'}
          - {name: train-model-2-output_model, from: '{{tasks.train-model-2.outputs.artifacts.train-model-2-output_model}}'}
      - name: test-model-3
        template: test-model-3
        dependencies: [create-model-inputs-3, train-model-3]
        arguments:
          artifacts:
          - {name: create-model-inputs-3-output_X_test, from: '{{tasks.create-model-inputs-3.outputs.artifacts.create-model-inputs-3-output_X_test}}'}
          - {name: create-model-inputs-3-output_y_test, from: '{{tasks.create-model-inputs-3.outputs.artifacts.create-model-inputs-3-output_y_test}}'}
          - {name: train-model-3-output_model, from: '{{tasks.train-model-3.outputs.artifacts.train-model-3-output_model}}'}
      - name: train-model
        template: train-model
        dependencies: [create-model-inputs]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          artifacts:
          - {name: create-model-inputs-output_X_train, from: '{{tasks.create-model-inputs.outputs.artifacts.create-model-inputs-output_X_train}}'}
          - {name: create-model-inputs-output_y_train, from: '{{tasks.create-model-inputs.outputs.artifacts.create-model-inputs-output_y_train}}'}
      - name: train-model-2
        template: train-model-2
        dependencies: [create-model-inputs-2]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          artifacts:
          - {name: create-model-inputs-2-output_X_train, from: '{{tasks.create-model-inputs-2.outputs.artifacts.create-model-inputs-2-output_X_train}}'}
          - {name: create-model-inputs-2-output_y_train, from: '{{tasks.create-model-inputs-2.outputs.artifacts.create-model-inputs-2-output_y_train}}'}
      - name: train-model-3
        template: train-model-3
        dependencies: [create-model-inputs-3]
        arguments:
          parameters:
          - {name: param_random_seed, value: '{{inputs.parameters.param_random_seed}}'}
          artifacts:
          - {name: create-model-inputs-3-output_X_train, from: '{{tasks.create-model-inputs-3.outputs.artifacts.create-model-inputs-3-output_X_train}}'}
          - {name: create-model-inputs-3-output_y_train, from: '{{tasks.create-model-inputs-3.outputs.artifacts.create-model-inputs-3-output_y_train}}'}
  - name: process-data-tarball
    container:
      args: [--file, /tmp/inputs/file/data, --output-patient-id-list, /tmp/outputs/output_patient_id_list/data,
        --output-master-df, /tmp/outputs/output_master_df/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef process_data_tarball(file_path,\n                         output_patient_id_list,\n\
        \                         output_master_df):\n    \"\"\"Specific data processing\
        \ of tarball downloaded.\n        - Hard code tarball content names. \n  \
        \      - Assume there is MANIFEST.txt\n        - Output: \n            - output_MANIFEST:\
        \ MANIFEST.txt as df\n            - output_DATA: tables in dict (keys: name\
        \ as in MANIFEST.txt, values: table)\n\n    Args:\n        file_path: A string\
        \ containing path to the tarball.\n    \"\"\"\n    from functools import reduce\n\
        \    import glob\n    import json \n    from json import JSONEncoder\n   \
        \ import numpy as np\n    import tarfile\n\n    import pandas as pd\n\n  \
        \  tarfile.open(name=file_path, mode=\"r|gz\").extractall('data_extracted')\n\
        \    l_tarball_contents = tarfile.open(name=file_path, mode=\"r|gz\").getnames()\n\
        \n    # all dataframes\n    d_df_data = {}\n\n    for name in l_tarball_contents:\n\
        \        archive_filename = 'data_extracted/' + name\n        if name == 'MANIFEST.txt':\n\
        \            df_manifest = pd.read_csv(glob.glob(archive_filename)[0], sep\
        \ = \"\\t\")\n        else:\n            df = pd.read_csv(glob.glob(archive_filename)[0],\
        \ sep = \"\\t\")\n            df.to_csv(index=False, header=True)\n      \
        \      d_df_data[name] = df    \n\n    filename_patient_table = '57683e22-a8ea-4eca-bfcf-f708cf459546/nationwidechildrens.org_clinical_patient_gbm.txt'\n\
        \    filename_follow_up_table = 'c9cdbc76-105d-429b-9fce-f000819716f9/nationwidechildrens.org_clinical_follow_up_v1.0_gbm.txt'\n\
        \n    # 1 remove un-needed header rows for tables\n    df_patient_raw = d_df_data[filename_patient_table]\n\
        \    df_follow_up_raw = d_df_data[filename_follow_up_table]\n\n    ## use\
        \ the 2nd row as the column name (in raw data, first 3 rows are column labels)\n\
        \    ### patient table\n    df_patient = df_patient_raw.drop([1])\n    df_patient\
        \ = df_patient.tail(len(df_patient) -1 )\n    df_patient = df_patient.reset_index().drop(['index'],\
        \ axis = 1)\n    ### follow_up table\n    df_follow_up = df_follow_up_raw.drop([1])\n\
        \    df_follow_up = df_follow_up.tail(len(df_follow_up) -1 )\n    df_follow_up\
        \ = df_follow_up.reset_index().drop(['index'], axis = 1)\n\n    # 2 data processing\n\
        \    ## 2a: data processing: clnical data\n\n    ### patient table\n    missing_value_flags\
        \ = ['[Not Available]',\n                       '[Discrepancy]',\n       \
        \                '[Unknown]',\n                       '[Not Applicable]',\n\
        \                       '[Not Evaluated]'\n                      ]\n    ####\
        \ replace flags with np.nan\n    for flag in missing_value_flags:\n      \
        \  df_patient = df_patient.replace(flag, np.nan)\n\n    #### drop un-needed\
        \ columns - identifiers\n    df_patient = df_patient.drop(['bcr_patient_uuid',\
        \ 'form_completion_date', 'patient_id'], axis=1)\n\n    #### parse numerical\
        \ columns w/ dates\n    l_numerical_cols = [\n        'last_contact_days_to',\n\
        \        'death_days_to',\n        'age_at_initial_pathologic_diagnosis'\n\
        \    ]\n    for col in l_numerical_cols:\n        df_patient[col] = [float(x)\
        \ for x in df_patient[col]]\n\n    df_patient['survival_time_yrs'] = np.abs(df_patient['death_days_to'])\
        \ / 365\n    df_patient['days_since_last_contact'] = np.abs(df_patient['last_contact_days_to'])\
        \ / 365\n\n    #### drop columns parsed / not needed anymore\n    l_columns_not_needed\
        \ = [\n        'last_contact_days_to',\n        'birth_days_to',\n       \
        \ 'death_days_to',\n\n        # uninformative cols (all missing/not evaluated\
        \ etc):\n        'anatomic_neoplasm_subdivision',\n        'disease_code',\n\
        \        'project_code',\n        'days_to_initial_pathologic_diagnosis',\n\
        \        'icd_10',\n        'icd_o_3_histology',\n        'icd_o_3_site',\n\
        \        'informed_consent_verified',\n        'initial_pathologic_dx_year'\n\
        \n    ]\n    df_patient = df_patient.drop(l_columns_not_needed, axis = 1)\n\
        \n    ### follow_up table\n    #### replace missing value flags with np.nan\n\
        \    missing_value_flags = ['[Not Available]',\n                         \
        \  '[Discrepancy]',\n                           '[Unknown]',\n           \
        \                '[Not Applicable]',\n                           '[Not Evaluated]'\n\
        \                          ]\n    for flag in missing_value_flags:\n     \
        \   df_follow_up = df_follow_up.replace(flag, np.nan)\n\n    #### drop un-needed\
        \ columns - identifiers\n    df_follow_up = df_follow_up.drop(['bcr_patient_uuid',\
        \ 'bcr_followup_uuid', 'form_completion_date',\n                         \
        \            'followup_reason', 'followup_lost_to'], axis=1)\n\n    #### parse\
        \ numerical columns w/ dates\n    l_numerical_cols = [\n        'last_contact_days_to',\n\
        \        'death_days_to'\n    ]\n    for col in l_numerical_cols:\n      \
        \  df_follow_up[col] = [float(x) for x in df_follow_up[col]]\n\n    #### isolate\
        \ the LAST followup (sorted by barcode) --> get one row per patient\n    df_follow_up\
        \ = df_follow_up.groupby(['bcr_patient_barcode']).tail(1)\n\n    #### drop\
        \ columns parsed / not needed anymore\n    l_columns_not_needed = [\n    \
        \    'last_contact_days_to',\n        'death_days_to',\n        'bcr_followup_barcode'\n\
        \    ]\n    df_follow_up = df_follow_up.drop(l_columns_not_needed, axis =\
        \ 1)\n\n    # 3 Merge tables by patient identifier (create master table)\n\
        \n    ## merge patient and followup table\n    ### find common cols: \n  \
        \  l_cols_table_1 = set(df_patient.columns)\n    l_cols_table_2 = set(df_follow_up.columns)\n\
        \    intersect_cols = l_cols_table_1.intersection(l_cols_table_2)\n    l_cols_to_remove_table_1\
        \ = [x for x in intersect_cols if x != 'bcr_patient_barcode']\n\n    ### drop\
        \ common cols from left table (`patient`) before join\n    df_patient = df_patient.drop(l_cols_to_remove_table_1,\
        \ axis = 1)\n    ### join\n    df_master = df_patient.merge(df_follow_up,\
        \ \n                                 left_on = 'bcr_patient_barcode', \n \
        \                                right_on = 'bcr_patient_barcode', \n    \
        \                             how = 'inner')\n    ### drop patient ID identifier\
        \ from master table (used as identifier, not treated as a feature)\n    df_patient_id_list\
        \ = df_master[['bcr_patient_barcode']]\n    df_master = df_master.drop(['bcr_patient_barcode'],\
        \ axis = 1)    \n\n    # pl output\n    df_patient_id_list.to_csv(output_patient_id_list,\
        \ index=False, header=True)\n    df_master.to_csv(output_master_df, index=False,\
        \ header=True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Process\
        \ data tarball', description='Specific data processing of tarball downloaded.')\n\
        _parser.add_argument(\"--file\", dest=\"file_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-patient-id-list\"\
        , dest=\"output_patient_id_list\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-master-df\"\
        , dest=\"output_master_df\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = process_data_tarball(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: download-data-kfp-sdk-v2-Data, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: process-data-tarball-output_master_df, path: /tmp/outputs/output_master_df/data}
      - {name: process-data-tarball-output_patient_id_list, path: /tmp/outputs/output_patient_id_list/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Specific
          data processing of tarball downloaded.", "implementation": {"container":
          {"args": ["--file", {"inputPath": "file"}, "--output-patient-id-list", {"outputPath":
          "output_patient_id_list"}, "--output-master-df", {"outputPath": "output_master_df"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef process_data_tarball(file_path,\n                         output_patient_id_list,\n                         output_master_df):\n    \"\"\"Specific
          data processing of tarball downloaded.\n        - Hard code tarball content
          names. \n        - Assume there is MANIFEST.txt\n        - Output: \n            -
          output_MANIFEST: MANIFEST.txt as df\n            - output_DATA: tables in
          dict (keys: name as in MANIFEST.txt, values: table)\n\n    Args:\n        file_path:
          A string containing path to the tarball.\n    \"\"\"\n    from functools
          import reduce\n    import glob\n    import json \n    from json import JSONEncoder\n    import
          numpy as np\n    import tarfile\n\n    import pandas as pd\n\n    tarfile.open(name=file_path,
          mode=\"r|gz\").extractall(''data_extracted'')\n    l_tarball_contents =
          tarfile.open(name=file_path, mode=\"r|gz\").getnames()\n\n    # all dataframes\n    d_df_data
          = {}\n\n    for name in l_tarball_contents:\n        archive_filename =
          ''data_extracted/'' + name\n        if name == ''MANIFEST.txt'':\n            df_manifest
          = pd.read_csv(glob.glob(archive_filename)[0], sep = \"\\t\")\n        else:\n            df
          = pd.read_csv(glob.glob(archive_filename)[0], sep = \"\\t\")\n            df.to_csv(index=False,
          header=True)\n            d_df_data[name] = df    \n\n    filename_patient_table
          = ''57683e22-a8ea-4eca-bfcf-f708cf459546/nationwidechildrens.org_clinical_patient_gbm.txt''\n    filename_follow_up_table
          = ''c9cdbc76-105d-429b-9fce-f000819716f9/nationwidechildrens.org_clinical_follow_up_v1.0_gbm.txt''\n\n    #
          1 remove un-needed header rows for tables\n    df_patient_raw = d_df_data[filename_patient_table]\n    df_follow_up_raw
          = d_df_data[filename_follow_up_table]\n\n    ## use the 2nd row as the column
          name (in raw data, first 3 rows are column labels)\n    ### patient table\n    df_patient
          = df_patient_raw.drop([1])\n    df_patient = df_patient.tail(len(df_patient)
          -1 )\n    df_patient = df_patient.reset_index().drop([''index''], axis =
          1)\n    ### follow_up table\n    df_follow_up = df_follow_up_raw.drop([1])\n    df_follow_up
          = df_follow_up.tail(len(df_follow_up) -1 )\n    df_follow_up = df_follow_up.reset_index().drop([''index''],
          axis = 1)\n\n    # 2 data processing\n    ## 2a: data processing: clnical
          data\n\n    ### patient table\n    missing_value_flags = [''[Not Available]'',\n                       ''[Discrepancy]'',\n                       ''[Unknown]'',\n                       ''[Not
          Applicable]'',\n                       ''[Not Evaluated]''\n                      ]\n    ####
          replace flags with np.nan\n    for flag in missing_value_flags:\n        df_patient
          = df_patient.replace(flag, np.nan)\n\n    #### drop un-needed columns -
          identifiers\n    df_patient = df_patient.drop([''bcr_patient_uuid'', ''form_completion_date'',
          ''patient_id''], axis=1)\n\n    #### parse numerical columns w/ dates\n    l_numerical_cols
          = [\n        ''last_contact_days_to'',\n        ''death_days_to'',\n        ''age_at_initial_pathologic_diagnosis''\n    ]\n    for
          col in l_numerical_cols:\n        df_patient[col] = [float(x) for x in df_patient[col]]\n\n    df_patient[''survival_time_yrs'']
          = np.abs(df_patient[''death_days_to'']) / 365\n    df_patient[''days_since_last_contact'']
          = np.abs(df_patient[''last_contact_days_to'']) / 365\n\n    #### drop columns
          parsed / not needed anymore\n    l_columns_not_needed = [\n        ''last_contact_days_to'',\n        ''birth_days_to'',\n        ''death_days_to'',\n\n        #
          uninformative cols (all missing/not evaluated etc):\n        ''anatomic_neoplasm_subdivision'',\n        ''disease_code'',\n        ''project_code'',\n        ''days_to_initial_pathologic_diagnosis'',\n        ''icd_10'',\n        ''icd_o_3_histology'',\n        ''icd_o_3_site'',\n        ''informed_consent_verified'',\n        ''initial_pathologic_dx_year''\n\n    ]\n    df_patient
          = df_patient.drop(l_columns_not_needed, axis = 1)\n\n    ### follow_up table\n    ####
          replace missing value flags with np.nan\n    missing_value_flags = [''[Not
          Available]'',\n                           ''[Discrepancy]'',\n                           ''[Unknown]'',\n                           ''[Not
          Applicable]'',\n                           ''[Not Evaluated]''\n                          ]\n    for
          flag in missing_value_flags:\n        df_follow_up = df_follow_up.replace(flag,
          np.nan)\n\n    #### drop un-needed columns - identifiers\n    df_follow_up
          = df_follow_up.drop([''bcr_patient_uuid'', ''bcr_followup_uuid'', ''form_completion_date'',\n                                     ''followup_reason'',
          ''followup_lost_to''], axis=1)\n\n    #### parse numerical columns w/ dates\n    l_numerical_cols
          = [\n        ''last_contact_days_to'',\n        ''death_days_to''\n    ]\n    for
          col in l_numerical_cols:\n        df_follow_up[col] = [float(x) for x in
          df_follow_up[col]]\n\n    #### isolate the LAST followup (sorted by barcode)
          --> get one row per patient\n    df_follow_up = df_follow_up.groupby([''bcr_patient_barcode'']).tail(1)\n\n    ####
          drop columns parsed / not needed anymore\n    l_columns_not_needed = [\n        ''last_contact_days_to'',\n        ''death_days_to'',\n        ''bcr_followup_barcode''\n    ]\n    df_follow_up
          = df_follow_up.drop(l_columns_not_needed, axis = 1)\n\n    # 3 Merge tables
          by patient identifier (create master table)\n\n    ## merge patient and
          followup table\n    ### find common cols: \n    l_cols_table_1 = set(df_patient.columns)\n    l_cols_table_2
          = set(df_follow_up.columns)\n    intersect_cols = l_cols_table_1.intersection(l_cols_table_2)\n    l_cols_to_remove_table_1
          = [x for x in intersect_cols if x != ''bcr_patient_barcode'']\n\n    ###
          drop common cols from left table (`patient`) before join\n    df_patient
          = df_patient.drop(l_cols_to_remove_table_1, axis = 1)\n    ### join\n    df_master
          = df_patient.merge(df_follow_up, \n                                 left_on
          = ''bcr_patient_barcode'', \n                                 right_on =
          ''bcr_patient_barcode'', \n                                 how = ''inner'')\n    ###
          drop patient ID identifier from master table (used as identifier, not treated
          as a feature)\n    df_patient_id_list = df_master[[''bcr_patient_barcode'']]\n    df_master
          = df_master.drop([''bcr_patient_barcode''], axis = 1)    \n\n    # pl output\n    df_patient_id_list.to_csv(output_patient_id_list,
          index=False, header=True)\n    df_master.to_csv(output_master_df, index=False,
          header=True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Process
          data tarball'', description=''Specific data processing of tarball downloaded.'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-patient-id-list\",
          dest=\"output_patient_id_list\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-master-df\",
          dest=\"output_master_df\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = process_data_tarball(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"description": "A string containing path to the tarball.", "name": "file",
          "type": "Tarball"}], "name": "Process data tarball", "outputs": [{"name":
          "output_patient_id_list", "type": "CSV"}, {"name": "output_master_df", "type":
          "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: scale-df
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def scale_df(file_path,
                     output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import pandas as pd
            from sklearn.preprocessing import StandardScaler

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # scaler
            scaler = StandardScaler()
            scaler.fit(df)
            nparr_scaled_data = scaler.transform(df)

            df_scaled = pd.DataFrame(nparr_scaled_data)
            df_scaled.columns = df.columns

            # Output to CSV
            df_scaled.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Scale df', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = scale_df(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: impute-unknown-output_csv, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: scale-df-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef scale_df(file_path,\n             output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import pandas as pd\n    from sklearn.preprocessing
          import StandardScaler\n\n    # Read in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    #
          scaler\n    scaler = StandardScaler()\n    scaler.fit(df)\n    nparr_scaled_data
          = scaler.transform(df)\n\n    df_scaled = pd.DataFrame(nparr_scaled_data)\n    df_scaled.columns
          = df.columns\n\n    # Output to CSV\n    df_scaled.to_csv(output_csv, index
          = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Scale
          df'', description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = scale_df(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs": [{"description":
          "A string containing path to input data.", "name": "file", "type": "CSV"}],
          "name": "Scale df", "outputs": [{"description": "A string containing path
          to processed data.", "name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: scale-df-2
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def scale_df(file_path,
                     output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import pandas as pd
            from sklearn.preprocessing import StandardScaler

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # scaler
            scaler = StandardScaler()
            scaler.fit(df)
            nparr_scaled_data = scaler.transform(df)

            df_scaled = pd.DataFrame(nparr_scaled_data)
            df_scaled.columns = df.columns

            # Output to CSV
            df_scaled.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Scale df', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = scale_df(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: impute-unknown-2-output_csv, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: scale-df-2-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef scale_df(file_path,\n             output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import pandas as pd\n    from sklearn.preprocessing
          import StandardScaler\n\n    # Read in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    #
          scaler\n    scaler = StandardScaler()\n    scaler.fit(df)\n    nparr_scaled_data
          = scaler.transform(df)\n\n    df_scaled = pd.DataFrame(nparr_scaled_data)\n    df_scaled.columns
          = df.columns\n\n    # Output to CSV\n    df_scaled.to_csv(output_csv, index
          = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Scale
          df'', description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = scale_df(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs": [{"description":
          "A string containing path to input data.", "name": "file", "type": "CSV"}],
          "name": "Scale df", "outputs": [{"description": "A string containing path
          to processed data.", "name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: scale-df-3
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.4.2' 'scikit-learn==1.0.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas==1.4.2' 'scikit-learn==1.0.2'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def scale_df(file_path,
                     output_csv):
            """Impute unknown values (nan).
                Input: CSV.
                Output: CSV.

            Args:
                file_path: A string containing path to input data.
                output_csv: A string containing path to processed data.
            """
            import pandas as pd
            from sklearn.preprocessing import StandardScaler

            # Read in CSV
            df = pd.read_csv(filepath_or_buffer=file_path)

            # scaler
            scaler = StandardScaler()
            scaler.fit(df)
            nparr_scaled_data = scaler.transform(df)

            df_scaled = pd.DataFrame(nparr_scaled_data)
            df_scaled.columns = df.columns

            # Output to CSV
            df_scaled.to_csv(output_csv, index = False, header = True)

        import argparse
        _parser = argparse.ArgumentParser(prog='Scale df', description='Impute unknown values (nan).')
        _parser.add_argument("--file", dest="file_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-csv", dest="output_csv", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = scale_df(**_parsed_args)
      image: python:3.9
    inputs:
      artifacts:
      - {name: impute-unknown-3-output_csv, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: scale-df-3-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Impute
          unknown values (nan).", "implementation": {"container": {"args": ["--file",
          {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas==1.4.2'' ''scikit-learn==1.0.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef scale_df(file_path,\n             output_csv):\n    \"\"\"Impute
          unknown values (nan).\n        Input: CSV.\n        Output: CSV.\n\n    Args:\n        file_path:
          A string containing path to input data.\n        output_csv: A string containing
          path to processed data.\n    \"\"\"\n    import pandas as pd\n    from sklearn.preprocessing
          import StandardScaler\n\n    # Read in CSV\n    df = pd.read_csv(filepath_or_buffer=file_path)\n\n    #
          scaler\n    scaler = StandardScaler()\n    scaler.fit(df)\n    nparr_scaled_data
          = scaler.transform(df)\n\n    df_scaled = pd.DataFrame(nparr_scaled_data)\n    df_scaled.columns
          = df.columns\n\n    # Output to CSV\n    df_scaled.to_csv(output_csv, index
          = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Scale
          df'', description=''Impute unknown values (nan).'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = scale_df(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs": [{"description":
          "A string containing path to input data.", "name": "file", "type": "CSV"}],
          "name": "Scale df", "outputs": [{"description": "A string containing path
          to processed data.", "name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: test-model
    container:
      args: [--file-path-x-test, /tmp/inputs/file_path_x_test/data, --file-path-y-test,
        /tmp/inputs/file_path_y_test/data, --file-path-model, /tmp/inputs/file_path_model/data,
        --output-json, /tmp/outputs/output_json/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow==1.24.0' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow==1.24.0' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef test_model(file_path_x_test,\n               file_path_y_test,\n   \
        \            file_path_model,\n               output_json,\n             \
        \  output_csv):\n    import joblib\n    import json\n    import mlflow.sklearn\n\
        \    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model\
        \ import LogisticRegression\n    from sklearn.metrics import accuracy_score\n\
        \    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics\
        \ import roc_auc_score\n    from sklearn.preprocessing import LabelBinarizer\n\
        \n    class NumpyArrayEncoder(json.JSONEncoder):\n        def default(self,\
        \ obj):\n            if isinstance(obj, np.ndarray):\n                return\
        \ obj.tolist()\n            return json.JSONEncoder.default(self, obj)\n\n\
        \    trained_model = mlflow.sklearn.load_model(file_path_model)\n\n    X_test\
        \ = pd.read_csv(file_path_x_test)\n    y_test = np.array(pd.read_csv(file_path_y_test))\n\
        \n    # Get predictions \n    y_pred = trained_model.predict(X_test)\n\n \
        \   #############################################################\n\n    #\
        \ Get accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Confusion\
        \ matrix - use labels\n    cm = confusion_matrix(y_test, y_pred)\n\n    #\
        \ AUC score\n    sum_y_test_axis_0 = np.sum(y_test, axis=0)\n    col_idx_y_test_nnz\
        \ = np.where(sum_y_test_axis_0 > 0)[0]\n\n    # compute AUC \n\n    ## check\
        \ if multi-class\n    num_classes = np.max([len(np.unique(y_pred)), len(np.unique(y_test))])\n\
        \    if num_classes > 2: # one-hot encode\n        lb = LabelBinarizer()\n\
        \        lb.fit(np.concatenate((np.array(y_test).ravel(), np.array(y_pred).ravel())))\n\
        \        y_test_onehot = lb.fit_transform(y_test)\n        y_pred_onehot =\
        \ lb.fit_transform(y_pred)\n    else:\n        y_test_onehot = y_test\n  \
        \      y_pred_onehot = y_pred\n\n    auc_test = roc_auc_score(y_test_onehot,\n\
        \                             y_pred_onehot,\n                           \
        \  multi_class=trained_model.multi_class) # specify multi-class method\n\n\
        \    # output: JSON\n    d_output = {}\n    ## add in model results\n    d_output['model_results']\
        \ = {}\n    d_output['model_results']['model'] = trained_model.get_params()\n\
        \    d_output['model_results']['y_test'] = np.array(y_test).ravel()\n    d_output['model_results']['y_pred']\
        \ = y_pred\n    d_output['model_results']['accuracy'] = accuracy\n    d_output['model_results']['auc_test']\
        \ = auc_test\n    d_output['model_results']['cm'] = cm\n\n    json_string_output\
        \ = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json output:\
        \ results\n    with open(output_json, 'w') as outfile:\n        outfile.write(json_string_output)\n\
        \n    ## write csv output: results\n    df_output_csv = pd.DataFrame({'metric':\
        \ d_output['model_results'].keys(),\n                                  'value':\
        \  d_output['model_results'].values()})\n    df_output_csv.to_csv(output_csv,\
        \ index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Test\
        \ model', description='')\n_parser.add_argument(\"--file-path-x-test\", dest=\"\
        file_path_x_test\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--file-path-y-test\", dest=\"file_path_y_test\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-model\"\
        , dest=\"file_path_model\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-json\", dest=\"output_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\"\
        , dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = test_model(**_parsed_args)\n"
      image: python:3.9
    inputs:
      artifacts:
      - {name: train-model-output_model, path: /tmp/inputs/file_path_model/data}
      - {name: create-model-inputs-output_X_test, path: /tmp/inputs/file_path_x_test/data}
      - {name: create-model-inputs-output_y_test, path: /tmp/inputs/file_path_y_test/data}
    outputs:
      artifacts:
      - {name: test-model-output_csv, path: /tmp/outputs/output_csv/data}
      - {name: test-model-output_json, path: /tmp/outputs/output_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file-path-x-test", {"inputPath": "file_path_x_test"}, "--file-path-y-test",
          {"inputPath": "file_path_y_test"}, "--file-path-model", {"inputPath": "file_path_model"},
          "--output-json", {"outputPath": "output_json"}, "--output-csv", {"outputPath":
          "output_csv"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''joblib==1.1.0''
          ''mlflow==1.24.0'' ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''protobuf==3.19.0''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''joblib==1.1.0'' ''mlflow==1.24.0'' ''pandas==1.4.2'' ''scikit-learn==1.0.2''
          ''protobuf==3.19.0'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef test_model(file_path_x_test,\n               file_path_y_test,\n               file_path_model,\n               output_json,\n               output_csv):\n    import
          joblib\n    import json\n    import mlflow.sklearn\n    import numpy as
          np\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from
          sklearn.metrics import accuracy_score\n    from sklearn.metrics import confusion_matrix\n    from
          sklearn.metrics import roc_auc_score\n    from sklearn.preprocessing import
          LabelBinarizer\n\n    class NumpyArrayEncoder(json.JSONEncoder):\n        def
          default(self, obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return json.JSONEncoder.default(self, obj)\n\n    trained_model
          = mlflow.sklearn.load_model(file_path_model)\n\n    X_test = pd.read_csv(file_path_x_test)\n    y_test
          = np.array(pd.read_csv(file_path_y_test))\n\n    # Get predictions \n    y_pred
          = trained_model.predict(X_test)\n\n    #############################################################\n\n    #
          Get accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Confusion
          matrix - use labels\n    cm = confusion_matrix(y_test, y_pred)\n\n    #
          AUC score\n    sum_y_test_axis_0 = np.sum(y_test, axis=0)\n    col_idx_y_test_nnz
          = np.where(sum_y_test_axis_0 > 0)[0]\n\n    # compute AUC \n\n    ## check
          if multi-class\n    num_classes = np.max([len(np.unique(y_pred)), len(np.unique(y_test))])\n    if
          num_classes > 2: # one-hot encode\n        lb = LabelBinarizer()\n        lb.fit(np.concatenate((np.array(y_test).ravel(),
          np.array(y_pred).ravel())))\n        y_test_onehot = lb.fit_transform(y_test)\n        y_pred_onehot
          = lb.fit_transform(y_pred)\n    else:\n        y_test_onehot = y_test\n        y_pred_onehot
          = y_pred\n\n    auc_test = roc_auc_score(y_test_onehot,\n                             y_pred_onehot,\n                             multi_class=trained_model.multi_class)
          # specify multi-class method\n\n    # output: JSON\n    d_output = {}\n    ##
          add in model results\n    d_output[''model_results''] = {}\n    d_output[''model_results''][''model'']
          = trained_model.get_params()\n    d_output[''model_results''][''y_test'']
          = np.array(y_test).ravel()\n    d_output[''model_results''][''y_pred'']
          = y_pred\n    d_output[''model_results''][''accuracy''] = accuracy\n    d_output[''model_results''][''auc_test'']
          = auc_test\n    d_output[''model_results''][''cm''] = cm\n\n    json_string_output
          = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json output:
          results\n    with open(output_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\n    ##
          write csv output: results\n    df_output_csv = pd.DataFrame({''metric'':
          d_output[''model_results''].keys(),\n                                  ''value'':  d_output[''model_results''].values()})\n    df_output_csv.to_csv(output_csv,
          index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Test
          model'', description='''')\n_parser.add_argument(\"--file-path-x-test\",
          dest=\"file_path_x_test\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-y-test\",
          dest=\"file_path_y_test\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-model\",
          dest=\"file_path_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\",
          dest=\"output_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\", dest=\"output_csv\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = test_model(**_parsed_args)\n"],
          "image": "python:3.9"}}, "inputs": [{"name": "file_path_x_test", "type":
          "CSV"}, {"name": "file_path_y_test", "type": "CSV"}, {"name": "file_path_model",
          "type": "Model"}], "name": "Test model", "outputs": [{"name": "output_json",
          "type": "JSON"}, {"name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: test-model-2
    container:
      args: [--file-path-x-test, /tmp/inputs/file_path_x_test/data, --file-path-y-test,
        /tmp/inputs/file_path_y_test/data, --file-path-model, /tmp/inputs/file_path_model/data,
        --output-json, /tmp/outputs/output_json/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow==1.24.0' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow==1.24.0' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef test_model(file_path_x_test,\n               file_path_y_test,\n   \
        \            file_path_model,\n               output_json,\n             \
        \  output_csv):\n    import joblib\n    import json\n    import mlflow.sklearn\n\
        \    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model\
        \ import LogisticRegression\n    from sklearn.metrics import accuracy_score\n\
        \    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics\
        \ import roc_auc_score\n    from sklearn.preprocessing import LabelBinarizer\n\
        \n    class NumpyArrayEncoder(json.JSONEncoder):\n        def default(self,\
        \ obj):\n            if isinstance(obj, np.ndarray):\n                return\
        \ obj.tolist()\n            return json.JSONEncoder.default(self, obj)\n\n\
        \    trained_model = mlflow.sklearn.load_model(file_path_model)\n\n    X_test\
        \ = pd.read_csv(file_path_x_test)\n    y_test = np.array(pd.read_csv(file_path_y_test))\n\
        \n    # Get predictions \n    y_pred = trained_model.predict(X_test)\n\n \
        \   #############################################################\n\n    #\
        \ Get accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Confusion\
        \ matrix - use labels\n    cm = confusion_matrix(y_test, y_pred)\n\n    #\
        \ AUC score\n    sum_y_test_axis_0 = np.sum(y_test, axis=0)\n    col_idx_y_test_nnz\
        \ = np.where(sum_y_test_axis_0 > 0)[0]\n\n    # compute AUC \n\n    ## check\
        \ if multi-class\n    num_classes = np.max([len(np.unique(y_pred)), len(np.unique(y_test))])\n\
        \    if num_classes > 2: # one-hot encode\n        lb = LabelBinarizer()\n\
        \        lb.fit(np.concatenate((np.array(y_test).ravel(), np.array(y_pred).ravel())))\n\
        \        y_test_onehot = lb.fit_transform(y_test)\n        y_pred_onehot =\
        \ lb.fit_transform(y_pred)\n    else:\n        y_test_onehot = y_test\n  \
        \      y_pred_onehot = y_pred\n\n    auc_test = roc_auc_score(y_test_onehot,\n\
        \                             y_pred_onehot,\n                           \
        \  multi_class=trained_model.multi_class) # specify multi-class method\n\n\
        \    # output: JSON\n    d_output = {}\n    ## add in model results\n    d_output['model_results']\
        \ = {}\n    d_output['model_results']['model'] = trained_model.get_params()\n\
        \    d_output['model_results']['y_test'] = np.array(y_test).ravel()\n    d_output['model_results']['y_pred']\
        \ = y_pred\n    d_output['model_results']['accuracy'] = accuracy\n    d_output['model_results']['auc_test']\
        \ = auc_test\n    d_output['model_results']['cm'] = cm\n\n    json_string_output\
        \ = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json output:\
        \ results\n    with open(output_json, 'w') as outfile:\n        outfile.write(json_string_output)\n\
        \n    ## write csv output: results\n    df_output_csv = pd.DataFrame({'metric':\
        \ d_output['model_results'].keys(),\n                                  'value':\
        \  d_output['model_results'].values()})\n    df_output_csv.to_csv(output_csv,\
        \ index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Test\
        \ model', description='')\n_parser.add_argument(\"--file-path-x-test\", dest=\"\
        file_path_x_test\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--file-path-y-test\", dest=\"file_path_y_test\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-model\"\
        , dest=\"file_path_model\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-json\", dest=\"output_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\"\
        , dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = test_model(**_parsed_args)\n"
      image: python:3.9
    inputs:
      artifacts:
      - {name: train-model-2-output_model, path: /tmp/inputs/file_path_model/data}
      - {name: create-model-inputs-2-output_X_test, path: /tmp/inputs/file_path_x_test/data}
      - {name: create-model-inputs-2-output_y_test, path: /tmp/inputs/file_path_y_test/data}
    outputs:
      artifacts:
      - {name: test-model-2-output_csv, path: /tmp/outputs/output_csv/data}
      - {name: test-model-2-output_json, path: /tmp/outputs/output_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file-path-x-test", {"inputPath": "file_path_x_test"}, "--file-path-y-test",
          {"inputPath": "file_path_y_test"}, "--file-path-model", {"inputPath": "file_path_model"},
          "--output-json", {"outputPath": "output_json"}, "--output-csv", {"outputPath":
          "output_csv"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''joblib==1.1.0''
          ''mlflow==1.24.0'' ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''protobuf==3.19.0''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''joblib==1.1.0'' ''mlflow==1.24.0'' ''pandas==1.4.2'' ''scikit-learn==1.0.2''
          ''protobuf==3.19.0'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef test_model(file_path_x_test,\n               file_path_y_test,\n               file_path_model,\n               output_json,\n               output_csv):\n    import
          joblib\n    import json\n    import mlflow.sklearn\n    import numpy as
          np\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from
          sklearn.metrics import accuracy_score\n    from sklearn.metrics import confusion_matrix\n    from
          sklearn.metrics import roc_auc_score\n    from sklearn.preprocessing import
          LabelBinarizer\n\n    class NumpyArrayEncoder(json.JSONEncoder):\n        def
          default(self, obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return json.JSONEncoder.default(self, obj)\n\n    trained_model
          = mlflow.sklearn.load_model(file_path_model)\n\n    X_test = pd.read_csv(file_path_x_test)\n    y_test
          = np.array(pd.read_csv(file_path_y_test))\n\n    # Get predictions \n    y_pred
          = trained_model.predict(X_test)\n\n    #############################################################\n\n    #
          Get accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Confusion
          matrix - use labels\n    cm = confusion_matrix(y_test, y_pred)\n\n    #
          AUC score\n    sum_y_test_axis_0 = np.sum(y_test, axis=0)\n    col_idx_y_test_nnz
          = np.where(sum_y_test_axis_0 > 0)[0]\n\n    # compute AUC \n\n    ## check
          if multi-class\n    num_classes = np.max([len(np.unique(y_pred)), len(np.unique(y_test))])\n    if
          num_classes > 2: # one-hot encode\n        lb = LabelBinarizer()\n        lb.fit(np.concatenate((np.array(y_test).ravel(),
          np.array(y_pred).ravel())))\n        y_test_onehot = lb.fit_transform(y_test)\n        y_pred_onehot
          = lb.fit_transform(y_pred)\n    else:\n        y_test_onehot = y_test\n        y_pred_onehot
          = y_pred\n\n    auc_test = roc_auc_score(y_test_onehot,\n                             y_pred_onehot,\n                             multi_class=trained_model.multi_class)
          # specify multi-class method\n\n    # output: JSON\n    d_output = {}\n    ##
          add in model results\n    d_output[''model_results''] = {}\n    d_output[''model_results''][''model'']
          = trained_model.get_params()\n    d_output[''model_results''][''y_test'']
          = np.array(y_test).ravel()\n    d_output[''model_results''][''y_pred'']
          = y_pred\n    d_output[''model_results''][''accuracy''] = accuracy\n    d_output[''model_results''][''auc_test'']
          = auc_test\n    d_output[''model_results''][''cm''] = cm\n\n    json_string_output
          = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json output:
          results\n    with open(output_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\n    ##
          write csv output: results\n    df_output_csv = pd.DataFrame({''metric'':
          d_output[''model_results''].keys(),\n                                  ''value'':  d_output[''model_results''].values()})\n    df_output_csv.to_csv(output_csv,
          index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Test
          model'', description='''')\n_parser.add_argument(\"--file-path-x-test\",
          dest=\"file_path_x_test\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-y-test\",
          dest=\"file_path_y_test\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-model\",
          dest=\"file_path_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\",
          dest=\"output_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\", dest=\"output_csv\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = test_model(**_parsed_args)\n"],
          "image": "python:3.9"}}, "inputs": [{"name": "file_path_x_test", "type":
          "CSV"}, {"name": "file_path_y_test", "type": "CSV"}, {"name": "file_path_model",
          "type": "Model"}], "name": "Test model", "outputs": [{"name": "output_json",
          "type": "JSON"}, {"name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: test-model-3
    container:
      args: [--file-path-x-test, /tmp/inputs/file_path_x_test/data, --file-path-y-test,
        /tmp/inputs/file_path_y_test/data, --file-path-model, /tmp/inputs/file_path_model/data,
        --output-json, /tmp/outputs/output_json/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow==1.24.0' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow==1.24.0' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef test_model(file_path_x_test,\n               file_path_y_test,\n   \
        \            file_path_model,\n               output_json,\n             \
        \  output_csv):\n    import joblib\n    import json\n    import mlflow.sklearn\n\
        \    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model\
        \ import LogisticRegression\n    from sklearn.metrics import accuracy_score\n\
        \    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics\
        \ import roc_auc_score\n    from sklearn.preprocessing import LabelBinarizer\n\
        \n    class NumpyArrayEncoder(json.JSONEncoder):\n        def default(self,\
        \ obj):\n            if isinstance(obj, np.ndarray):\n                return\
        \ obj.tolist()\n            return json.JSONEncoder.default(self, obj)\n\n\
        \    trained_model = mlflow.sklearn.load_model(file_path_model)\n\n    X_test\
        \ = pd.read_csv(file_path_x_test)\n    y_test = np.array(pd.read_csv(file_path_y_test))\n\
        \n    # Get predictions \n    y_pred = trained_model.predict(X_test)\n\n \
        \   #############################################################\n\n    #\
        \ Get accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Confusion\
        \ matrix - use labels\n    cm = confusion_matrix(y_test, y_pred)\n\n    #\
        \ AUC score\n    sum_y_test_axis_0 = np.sum(y_test, axis=0)\n    col_idx_y_test_nnz\
        \ = np.where(sum_y_test_axis_0 > 0)[0]\n\n    # compute AUC \n\n    ## check\
        \ if multi-class\n    num_classes = np.max([len(np.unique(y_pred)), len(np.unique(y_test))])\n\
        \    if num_classes > 2: # one-hot encode\n        lb = LabelBinarizer()\n\
        \        lb.fit(np.concatenate((np.array(y_test).ravel(), np.array(y_pred).ravel())))\n\
        \        y_test_onehot = lb.fit_transform(y_test)\n        y_pred_onehot =\
        \ lb.fit_transform(y_pred)\n    else:\n        y_test_onehot = y_test\n  \
        \      y_pred_onehot = y_pred\n\n    auc_test = roc_auc_score(y_test_onehot,\n\
        \                             y_pred_onehot,\n                           \
        \  multi_class=trained_model.multi_class) # specify multi-class method\n\n\
        \    # output: JSON\n    d_output = {}\n    ## add in model results\n    d_output['model_results']\
        \ = {}\n    d_output['model_results']['model'] = trained_model.get_params()\n\
        \    d_output['model_results']['y_test'] = np.array(y_test).ravel()\n    d_output['model_results']['y_pred']\
        \ = y_pred\n    d_output['model_results']['accuracy'] = accuracy\n    d_output['model_results']['auc_test']\
        \ = auc_test\n    d_output['model_results']['cm'] = cm\n\n    json_string_output\
        \ = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json output:\
        \ results\n    with open(output_json, 'w') as outfile:\n        outfile.write(json_string_output)\n\
        \n    ## write csv output: results\n    df_output_csv = pd.DataFrame({'metric':\
        \ d_output['model_results'].keys(),\n                                  'value':\
        \  d_output['model_results'].values()})\n    df_output_csv.to_csv(output_csv,\
        \ index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Test\
        \ model', description='')\n_parser.add_argument(\"--file-path-x-test\", dest=\"\
        file_path_x_test\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--file-path-y-test\", dest=\"file_path_y_test\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-model\"\
        , dest=\"file_path_model\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-json\", dest=\"output_json\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\"\
        , dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = test_model(**_parsed_args)\n"
      image: python:3.9
    inputs:
      artifacts:
      - {name: train-model-3-output_model, path: /tmp/inputs/file_path_model/data}
      - {name: create-model-inputs-3-output_X_test, path: /tmp/inputs/file_path_x_test/data}
      - {name: create-model-inputs-3-output_y_test, path: /tmp/inputs/file_path_y_test/data}
    outputs:
      artifacts:
      - {name: test-model-3-output_csv, path: /tmp/outputs/output_csv/data}
      - {name: test-model-3-output_json, path: /tmp/outputs/output_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file-path-x-test", {"inputPath": "file_path_x_test"}, "--file-path-y-test",
          {"inputPath": "file_path_y_test"}, "--file-path-model", {"inputPath": "file_path_model"},
          "--output-json", {"outputPath": "output_json"}, "--output-csv", {"outputPath":
          "output_csv"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''joblib==1.1.0''
          ''mlflow==1.24.0'' ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''protobuf==3.19.0''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''joblib==1.1.0'' ''mlflow==1.24.0'' ''pandas==1.4.2'' ''scikit-learn==1.0.2''
          ''protobuf==3.19.0'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef test_model(file_path_x_test,\n               file_path_y_test,\n               file_path_model,\n               output_json,\n               output_csv):\n    import
          joblib\n    import json\n    import mlflow.sklearn\n    import numpy as
          np\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from
          sklearn.metrics import accuracy_score\n    from sklearn.metrics import confusion_matrix\n    from
          sklearn.metrics import roc_auc_score\n    from sklearn.preprocessing import
          LabelBinarizer\n\n    class NumpyArrayEncoder(json.JSONEncoder):\n        def
          default(self, obj):\n            if isinstance(obj, np.ndarray):\n                return
          obj.tolist()\n            return json.JSONEncoder.default(self, obj)\n\n    trained_model
          = mlflow.sklearn.load_model(file_path_model)\n\n    X_test = pd.read_csv(file_path_x_test)\n    y_test
          = np.array(pd.read_csv(file_path_y_test))\n\n    # Get predictions \n    y_pred
          = trained_model.predict(X_test)\n\n    #############################################################\n\n    #
          Get accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Confusion
          matrix - use labels\n    cm = confusion_matrix(y_test, y_pred)\n\n    #
          AUC score\n    sum_y_test_axis_0 = np.sum(y_test, axis=0)\n    col_idx_y_test_nnz
          = np.where(sum_y_test_axis_0 > 0)[0]\n\n    # compute AUC \n\n    ## check
          if multi-class\n    num_classes = np.max([len(np.unique(y_pred)), len(np.unique(y_test))])\n    if
          num_classes > 2: # one-hot encode\n        lb = LabelBinarizer()\n        lb.fit(np.concatenate((np.array(y_test).ravel(),
          np.array(y_pred).ravel())))\n        y_test_onehot = lb.fit_transform(y_test)\n        y_pred_onehot
          = lb.fit_transform(y_pred)\n    else:\n        y_test_onehot = y_test\n        y_pred_onehot
          = y_pred\n\n    auc_test = roc_auc_score(y_test_onehot,\n                             y_pred_onehot,\n                             multi_class=trained_model.multi_class)
          # specify multi-class method\n\n    # output: JSON\n    d_output = {}\n    ##
          add in model results\n    d_output[''model_results''] = {}\n    d_output[''model_results''][''model'']
          = trained_model.get_params()\n    d_output[''model_results''][''y_test'']
          = np.array(y_test).ravel()\n    d_output[''model_results''][''y_pred'']
          = y_pred\n    d_output[''model_results''][''accuracy''] = accuracy\n    d_output[''model_results''][''auc_test'']
          = auc_test\n    d_output[''model_results''][''cm''] = cm\n\n    json_string_output
          = json.dumps(d_output, cls=NumpyArrayEncoder)\n\n    ## write json output:
          results\n    with open(output_json, ''w'') as outfile:\n        outfile.write(json_string_output)\n\n    ##
          write csv output: results\n    df_output_csv = pd.DataFrame({''metric'':
          d_output[''model_results''].keys(),\n                                  ''value'':  d_output[''model_results''].values()})\n    df_output_csv.to_csv(output_csv,
          index = False, header = True)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Test
          model'', description='''')\n_parser.add_argument(\"--file-path-x-test\",
          dest=\"file_path_x_test\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-y-test\",
          dest=\"file_path_y_test\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-model\",
          dest=\"file_path_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\",
          dest=\"output_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\", dest=\"output_csv\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = test_model(**_parsed_args)\n"],
          "image": "python:3.9"}}, "inputs": [{"name": "file_path_x_test", "type":
          "CSV"}, {"name": "file_path_y_test", "type": "CSV"}, {"name": "file_path_model",
          "type": "Model"}], "name": "Test model", "outputs": [{"name": "output_json",
          "type": "JSON"}, {"name": "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-model
    container:
      args: [--file-path-x-train, /tmp/inputs/file_path_x_train/data, --file-path-y-train,
        /tmp/inputs/file_path_y_train/data, --param-random-seed, '{{inputs.parameters.param_random_seed}}',
        --output-model, /tmp/outputs/output_model/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        'numpy' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'joblib==1.1.0' 'mlflow' 'pandas==1.4.2' 'scikit-learn==1.0.2'
        'protobuf==3.19.0' 'numpy' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef train_model(file_path_x_train, \n                file_path_y_train,\n\
        \                param_random_seed,\n                output_model):\n    import\
        \ joblib\n    import numpy as np\n    import mlflow.sklearn\n    import pandas\
        \ as pd\n    from sklearn.linear_model import LogisticRegression\n    from\
        \ sklearn import preprocessing\n\n    # set the seed\n    np.random.seed(int(np.float64(param_random_seed)))\n\
        \n    X_train = pd.read_csv(file_path_x_train)\n    y_train_raw = pd.read_csv(file_path_y_train)\n\
        \n#     lb = preprocessing.LabelBinarizer()\n#     lb.fit_transform(y_train_raw.astype(str))\n\
        #     y_class_labels = lb.classes_ #\n#     print('y_class_labels: ', y_class_labels)\
        \ #\n#     y_train = lb.fit_transform(y_train_raw.astype(str))    \n\n   \
        \ y_train=y_train_raw\n\n    model = LogisticRegression(verbose=1,\n     \
        \                          penalty='l2',\n                               tol=1e-4,\n\
        \                               C=1.0,\n                               class_weight='balanced',\n\
        \                               solver='lbfgs',\n                        \
        \       multi_class='ovr')\n    model.fit(X_train, y_train)\n\n    mlflow.sklearn.save_model(model,\
        \ output_model,\n                          serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n\
        \n    # log model\n    mlflow.sklearn.log_model(model, \"sklearn_models\"\
        )\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train model',\
        \ description='')\n_parser.add_argument(\"--file-path-x-train\", dest=\"file_path_x_train\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --file-path-y-train\", dest=\"file_path_y_train\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\"\
        , dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-model\", dest=\"output_model\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = train_model(**_parsed_args)\n"
      image: python:3.9
    inputs:
      parameters:
      - {name: param_random_seed}
      artifacts:
      - {name: create-model-inputs-output_X_train, path: /tmp/inputs/file_path_x_train/data}
      - {name: create-model-inputs-output_y_train, path: /tmp/inputs/file_path_y_train/data}
    outputs:
      artifacts:
      - {name: train-model-output_model, path: /tmp/outputs/output_model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file-path-x-train", {"inputPath": "file_path_x_train"}, "--file-path-y-train",
          {"inputPath": "file_path_y_train"}, "--param-random-seed", {"inputValue":
          "param_random_seed"}, "--output-model", {"outputPath": "output_model"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''joblib==1.1.0'' ''mlflow'' ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' ''protobuf==3.19.0'' ''numpy'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''joblib==1.1.0''
          ''mlflow'' ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''protobuf==3.19.0''
          ''numpy'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_model(file_path_x_train,
          \n                file_path_y_train,\n                param_random_seed,\n                output_model):\n    import
          joblib\n    import numpy as np\n    import mlflow.sklearn\n    import pandas
          as pd\n    from sklearn.linear_model import LogisticRegression\n    from
          sklearn import preprocessing\n\n    # set the seed\n    np.random.seed(int(np.float64(param_random_seed)))\n\n    X_train
          = pd.read_csv(file_path_x_train)\n    y_train_raw = pd.read_csv(file_path_y_train)\n\n#     lb
          = preprocessing.LabelBinarizer()\n#     lb.fit_transform(y_train_raw.astype(str))\n#     y_class_labels
          = lb.classes_ #\n#     print(''y_class_labels: '', y_class_labels) #\n#     y_train
          = lb.fit_transform(y_train_raw.astype(str))    \n\n    y_train=y_train_raw\n\n    model
          = LogisticRegression(verbose=1,\n                               penalty=''l2'',\n                               tol=1e-4,\n                               C=1.0,\n                               class_weight=''balanced'',\n                               solver=''lbfgs'',\n                               multi_class=''ovr'')\n    model.fit(X_train,
          y_train)\n\n    mlflow.sklearn.save_model(model, output_model,\n                          serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n\n    #
          log model\n    mlflow.sklearn.log_model(model, \"sklearn_models\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train model'', description='''')\n_parser.add_argument(\"--file-path-x-train\",
          dest=\"file_path_x_train\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-y-train\",
          dest=\"file_path_y_train\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-model\",
          dest=\"output_model\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs": [{"name":
          "file_path_x_train", "type": "CSV"}, {"name": "file_path_y_train", "type":
          "CSV"}, {"name": "param_random_seed", "type": "String"}], "name": "Train
          model", "outputs": [{"name": "output_model", "type": "Model"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"param_random_seed": "{{inputs.parameters.param_random_seed}}"}'}
  - name: train-model-2
    container:
      args: [--file-path-x-train, /tmp/inputs/file_path_x_train/data, --file-path-y-train,
        /tmp/inputs/file_path_y_train/data, --param-random-seed, '{{inputs.parameters.param_random_seed}}',
        --output-model, /tmp/outputs/output_model/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        'numpy' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'joblib==1.1.0' 'mlflow' 'pandas==1.4.2' 'scikit-learn==1.0.2'
        'protobuf==3.19.0' 'numpy' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef train_model(file_path_x_train, \n                file_path_y_train,\n\
        \                param_random_seed,\n                output_model):\n    import\
        \ joblib\n    import numpy as np\n    import mlflow.sklearn\n    import pandas\
        \ as pd\n    from sklearn.linear_model import LogisticRegression\n    from\
        \ sklearn import preprocessing\n\n    # set the seed\n    np.random.seed(int(np.float64(param_random_seed)))\n\
        \n    X_train = pd.read_csv(file_path_x_train)\n    y_train_raw = pd.read_csv(file_path_y_train)\n\
        \n#     lb = preprocessing.LabelBinarizer()\n#     lb.fit_transform(y_train_raw.astype(str))\n\
        #     y_class_labels = lb.classes_ #\n#     print('y_class_labels: ', y_class_labels)\
        \ #\n#     y_train = lb.fit_transform(y_train_raw.astype(str))    \n\n   \
        \ y_train=y_train_raw\n\n    model = LogisticRegression(verbose=1,\n     \
        \                          penalty='l2',\n                               tol=1e-4,\n\
        \                               C=1.0,\n                               class_weight='balanced',\n\
        \                               solver='lbfgs',\n                        \
        \       multi_class='ovr')\n    model.fit(X_train, y_train)\n\n    mlflow.sklearn.save_model(model,\
        \ output_model,\n                          serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n\
        \n    # log model\n    mlflow.sklearn.log_model(model, \"sklearn_models\"\
        )\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train model',\
        \ description='')\n_parser.add_argument(\"--file-path-x-train\", dest=\"file_path_x_train\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --file-path-y-train\", dest=\"file_path_y_train\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\"\
        , dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-model\", dest=\"output_model\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = train_model(**_parsed_args)\n"
      image: python:3.9
    inputs:
      parameters:
      - {name: param_random_seed}
      artifacts:
      - {name: create-model-inputs-2-output_X_train, path: /tmp/inputs/file_path_x_train/data}
      - {name: create-model-inputs-2-output_y_train, path: /tmp/inputs/file_path_y_train/data}
    outputs:
      artifacts:
      - {name: train-model-2-output_model, path: /tmp/outputs/output_model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file-path-x-train", {"inputPath": "file_path_x_train"}, "--file-path-y-train",
          {"inputPath": "file_path_y_train"}, "--param-random-seed", {"inputValue":
          "param_random_seed"}, "--output-model", {"outputPath": "output_model"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''joblib==1.1.0'' ''mlflow'' ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' ''protobuf==3.19.0'' ''numpy'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''joblib==1.1.0''
          ''mlflow'' ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''protobuf==3.19.0''
          ''numpy'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_model(file_path_x_train,
          \n                file_path_y_train,\n                param_random_seed,\n                output_model):\n    import
          joblib\n    import numpy as np\n    import mlflow.sklearn\n    import pandas
          as pd\n    from sklearn.linear_model import LogisticRegression\n    from
          sklearn import preprocessing\n\n    # set the seed\n    np.random.seed(int(np.float64(param_random_seed)))\n\n    X_train
          = pd.read_csv(file_path_x_train)\n    y_train_raw = pd.read_csv(file_path_y_train)\n\n#     lb
          = preprocessing.LabelBinarizer()\n#     lb.fit_transform(y_train_raw.astype(str))\n#     y_class_labels
          = lb.classes_ #\n#     print(''y_class_labels: '', y_class_labels) #\n#     y_train
          = lb.fit_transform(y_train_raw.astype(str))    \n\n    y_train=y_train_raw\n\n    model
          = LogisticRegression(verbose=1,\n                               penalty=''l2'',\n                               tol=1e-4,\n                               C=1.0,\n                               class_weight=''balanced'',\n                               solver=''lbfgs'',\n                               multi_class=''ovr'')\n    model.fit(X_train,
          y_train)\n\n    mlflow.sklearn.save_model(model, output_model,\n                          serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n\n    #
          log model\n    mlflow.sklearn.log_model(model, \"sklearn_models\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train model'', description='''')\n_parser.add_argument(\"--file-path-x-train\",
          dest=\"file_path_x_train\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-y-train\",
          dest=\"file_path_y_train\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-model\",
          dest=\"output_model\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs": [{"name":
          "file_path_x_train", "type": "CSV"}, {"name": "file_path_y_train", "type":
          "CSV"}, {"name": "param_random_seed", "type": "String"}], "name": "Train
          model", "outputs": [{"name": "output_model", "type": "Model"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"param_random_seed": "{{inputs.parameters.param_random_seed}}"}'}
  - name: train-model-3
    container:
      args: [--file-path-x-train, /tmp/inputs/file_path_x_train/data, --file-path-y-train,
        /tmp/inputs/file_path_y_train/data, --param-random-seed, '{{inputs.parameters.param_random_seed}}',
        --output-model, /tmp/outputs/output_model/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'joblib==1.1.0' 'mlflow' 'pandas==1.4.2' 'scikit-learn==1.0.2' 'protobuf==3.19.0'
        'numpy' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'joblib==1.1.0' 'mlflow' 'pandas==1.4.2' 'scikit-learn==1.0.2'
        'protobuf==3.19.0' 'numpy' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef train_model(file_path_x_train, \n                file_path_y_train,\n\
        \                param_random_seed,\n                output_model):\n    import\
        \ joblib\n    import numpy as np\n    import mlflow.sklearn\n    import pandas\
        \ as pd\n    from sklearn.linear_model import LogisticRegression\n    from\
        \ sklearn import preprocessing\n\n    # set the seed\n    np.random.seed(int(np.float64(param_random_seed)))\n\
        \n    X_train = pd.read_csv(file_path_x_train)\n    y_train_raw = pd.read_csv(file_path_y_train)\n\
        \n#     lb = preprocessing.LabelBinarizer()\n#     lb.fit_transform(y_train_raw.astype(str))\n\
        #     y_class_labels = lb.classes_ #\n#     print('y_class_labels: ', y_class_labels)\
        \ #\n#     y_train = lb.fit_transform(y_train_raw.astype(str))    \n\n   \
        \ y_train=y_train_raw\n\n    model = LogisticRegression(verbose=1,\n     \
        \                          penalty='l2',\n                               tol=1e-4,\n\
        \                               C=1.0,\n                               class_weight='balanced',\n\
        \                               solver='lbfgs',\n                        \
        \       multi_class='ovr')\n    model.fit(X_train, y_train)\n\n    mlflow.sklearn.save_model(model,\
        \ output_model,\n                          serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n\
        \n    # log model\n    mlflow.sklearn.log_model(model, \"sklearn_models\"\
        )\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train model',\
        \ description='')\n_parser.add_argument(\"--file-path-x-train\", dest=\"file_path_x_train\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --file-path-y-train\", dest=\"file_path_y_train\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\"\
        , dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-model\", dest=\"output_model\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = train_model(**_parsed_args)\n"
      image: python:3.9
    inputs:
      parameters:
      - {name: param_random_seed}
      artifacts:
      - {name: create-model-inputs-3-output_X_train, path: /tmp/inputs/file_path_x_train/data}
      - {name: create-model-inputs-3-output_y_train, path: /tmp/inputs/file_path_y_train/data}
    outputs:
      artifacts:
      - {name: train-model-3-output_model, path: /tmp/outputs/output_model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--file-path-x-train", {"inputPath": "file_path_x_train"}, "--file-path-y-train",
          {"inputPath": "file_path_y_train"}, "--param-random-seed", {"inputValue":
          "param_random_seed"}, "--output-model", {"outputPath": "output_model"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''joblib==1.1.0'' ''mlflow'' ''pandas==1.4.2''
          ''scikit-learn==1.0.2'' ''protobuf==3.19.0'' ''numpy'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''joblib==1.1.0''
          ''mlflow'' ''pandas==1.4.2'' ''scikit-learn==1.0.2'' ''protobuf==3.19.0''
          ''numpy'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_model(file_path_x_train,
          \n                file_path_y_train,\n                param_random_seed,\n                output_model):\n    import
          joblib\n    import numpy as np\n    import mlflow.sklearn\n    import pandas
          as pd\n    from sklearn.linear_model import LogisticRegression\n    from
          sklearn import preprocessing\n\n    # set the seed\n    np.random.seed(int(np.float64(param_random_seed)))\n\n    X_train
          = pd.read_csv(file_path_x_train)\n    y_train_raw = pd.read_csv(file_path_y_train)\n\n#     lb
          = preprocessing.LabelBinarizer()\n#     lb.fit_transform(y_train_raw.astype(str))\n#     y_class_labels
          = lb.classes_ #\n#     print(''y_class_labels: '', y_class_labels) #\n#     y_train
          = lb.fit_transform(y_train_raw.astype(str))    \n\n    y_train=y_train_raw\n\n    model
          = LogisticRegression(verbose=1,\n                               penalty=''l2'',\n                               tol=1e-4,\n                               C=1.0,\n                               class_weight=''balanced'',\n                               solver=''lbfgs'',\n                               multi_class=''ovr'')\n    model.fit(X_train,
          y_train)\n\n    mlflow.sklearn.save_model(model, output_model,\n                          serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n\n    #
          log model\n    mlflow.sklearn.log_model(model, \"sklearn_models\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train model'', description='''')\n_parser.add_argument(\"--file-path-x-train\",
          dest=\"file_path_x_train\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--file-path-y-train\",
          dest=\"file_path_y_train\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--param-random-seed\",
          dest=\"param_random_seed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-model\",
          dest=\"output_model\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs": [{"name":
          "file_path_x_train", "type": "CSV"}, {"name": "file_path_y_train", "type":
          "CSV"}, {"name": "param_random_seed", "type": "String"}], "name": "Train
          model", "outputs": [{"name": "output_model", "type": "Model"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"param_random_seed": "{{inputs.parameters.param_random_seed}}"}'}
  arguments:
    parameters:
    - {name: user_input_class_label_column_name}
    - {name: s_colnames_to_exclude}
    - {name: test_set_size}
    - {name: param_random_seed}
  serviceAccountName: pipeline-runner
