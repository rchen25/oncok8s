{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a863fe05",
   "metadata": {},
   "source": [
    "# build kubeflow pipeline with python SDK\n",
    "\n",
    "\n",
    "* Leverage python SDK \n",
    "* Create components and pipelines (YAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37adbfdf",
   "metadata": {},
   "source": [
    "## Required installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2717c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /Users/user1/miniconda3/lib/python3.9/site-packages (1.8.11)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (3.0.1)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.13 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.1.13)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (2.0.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (8.0.3)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (18.20.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.35.0)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.1.10)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.44.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (3.19.4)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.9.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.8.1)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.2.13)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.0.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.13)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp) (1.12.11)\n",
      "Requirement already satisfied: six in /Users/user1/miniconda3/lib/python3.9/site-packages (from absl-py<2,>=0.9->kfp) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/user1/miniconda3/lib/python3.9/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /Users/user1/miniconda3/lib/python3.9/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (2.7.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.20.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (1.55.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (58.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (4.8)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.2.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/user1/miniconda3/lib/python3.9/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp) (3.0.6)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: certifi in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.15 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.8)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.2.3)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/user1/miniconda3/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/user1/miniconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/user1/miniconda3/lib/python3.9/site-packages (from pydantic<2,>=1.8.2->kfp) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user1/miniconda3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2.0.10)\n",
      "Requirement already satisfied: wheel in /Users/user1/miniconda3/lib/python3.9/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/user1/miniconda3/lib/python3.9/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf0a30",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad238789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244350a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f38ee704",
   "metadata": {},
   "source": [
    "## 1 Data Acquisition: Download data\n",
    "\n",
    "Download data file from web. \n",
    "Use kfp component developed by kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f2ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_downloader_op = kfp.components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component-sdk-v2.yaml')\n",
    "\n",
    "## load component from yaml\n",
    "download_file_component = kfp.components.load_component_from_file(\"./component-sdk-v2.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e7c6b",
   "metadata": {},
   "source": [
    "### Pipeline: Download a single CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2b8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "url_for_data_download = 'https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff'\n",
    "\n",
    "# Define a pipeline and create a task from a component:\n",
    "def pl01_web_download(url):\n",
    "    web_downloader_task = download_file_component(url)\n",
    "\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl01_web_download,\n",
    "    package_path='pl01_web_download.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc59636",
   "metadata": {},
   "source": [
    "## 1 Data Acquisition: Merge CSV\n",
    "\n",
    "If tarball with multiple CSV file is downloaded, merge them together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de30d6",
   "metadata": {},
   "source": [
    "### Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d6c314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(file_path: comp.InputPath('Tarball'),\n",
    "              output_csv: comp.OutputPath('CSV')):\n",
    "    \"\"\"Merge multiple CSV files inside tarball.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the tarball.\n",
    "                    e.g., 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import tarfile\n",
    "\n",
    "    tarfile.open(name=file_path, mode=\"r|gz\").extractall('data')\n",
    "    df = pd.concat(\n",
    "        [pd.read_csv(csv_file, header=None) \n",
    "            for csv_file in glob.glob('data/*.csv')])\n",
    "    df.to_csv(output_csv, index=False, header=False)\n",
    "    \n",
    "    \n",
    "create_step_merge_csv = kfp.components.create_component_from_func(\n",
    "    func=merge_csv,\n",
    "    output_component_file='component_merge_csv.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068f67d",
   "metadata": {},
   "source": [
    "### Pipeline: Download tarball of multiple CSV and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "016d27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline and create a task from a component:\n",
    "def pl01a_web_download_merge_csv_tarball(url):\n",
    "    \"\"\"Pipeline: Download and merge multiple CSV files inside tarball.\n",
    "        1. download tarball\n",
    "        2. unpack and merge multiple CSVs (assuming same header)\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to tarball.\n",
    "                    e.g., 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n",
    "    \"\"\"\n",
    "    web_downloader_task = download_file_component(url)\n",
    "    merge_csv_task = create_step_merge_csv(file=web_downloader_task.outputs['data'])\n",
    "    # The outputs of the merge_csv_task can be referenced using the\n",
    "    # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl01a_web_download_merge_csv_tarball,\n",
    "    package_path='pl01a_web_download_merge_csv_tarball.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b04e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f8bd666",
   "metadata": {},
   "source": [
    "## 2 Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089545ad",
   "metadata": {},
   "source": [
    "### Component: get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "008454f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(file_path: comp.InputPath('CSV'),\n",
    "                output_csv_features: comp.OutputPath('CSV'),\n",
    "                output_csv_target: comp.OutputPath('CSV')):\n",
    "    \"\"\"Distribute categorical features into separate features.\n",
    "        Input: CSV with categorical (and numeric) features. Assume last \n",
    "            feature is target label. \n",
    "        Output: CSV with categorical features separated into dummies.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.read_csv(filepath_or_buffer=file_path)\n",
    "    l_col_names = list(df.columns)\n",
    "    \n",
    "    # assume last col is target\n",
    "    df_target = df[l_col_names[-1]]\n",
    "    \n",
    "    # create dummies for every col except last\n",
    "    df_features = df[l_col_names[:len(l_col_names)-1]]\n",
    "    df_features_dummies = pd.get_dummies(df_features)\n",
    "    \n",
    "    # write outputs\n",
    "    df_features_dummies.to_csv(output_csv_features, index = False, header = True)\n",
    "    df_target.to_csv(output_csv_target, index = False, header = True)\n",
    "\n",
    "create_step_dp_get_dummies = kfp.components.create_component_from_func(\n",
    "    func=get_dummies,\n",
    "    output_component_file='component_dp_get_dummies.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767283a",
   "metadata": {},
   "source": [
    "### Pipeline 02a\n",
    "\n",
    "1. download file\n",
    "2. data processing: a) get dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d8da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl02a_web_download_dp_dummies(url):\n",
    "    \"\"\"Pipeline: Download data, data processing\n",
    "        1. download data\n",
    "        2. data processing\n",
    "            a. get dummies\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the CSV.\n",
    "                    e.g., 'https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff'\n",
    "    \"\"\"\n",
    "    web_downloader_task = download_file_component(url=url_for_data_download)\n",
    "    dp_get_dummies_task = create_step_dp_get_dummies(file=web_downloader_task.outputs['data'])\n",
    "\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl02a_web_download_dp_dummies,\n",
    "    package_path='pl02a_web_download_dp_dummies.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0bbc8",
   "metadata": {},
   "source": [
    "### Component: Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a3a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_unknown(file_path: comp.InputPath('CSV'),\n",
    "                output_csv: comp.OutputPath('CSV')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read in CSV\n",
    "    df = pd.read_csv(filepath_or_buffer=file_path)\n",
    "    \n",
    "    # Output to CSV\n",
    "    df.to_csv(output_csv, index = False, header = True)\n",
    "\n",
    "create_step_dp_impute_unknown = kfp.components.create_component_from_func(\n",
    "    func=impute_unknown,\n",
    "    output_component_file='component_dp_impute_unknown.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefae17",
   "metadata": {},
   "source": [
    "### Component: Scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aed425ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df(file_path: comp.InputPath('CSV'),\n",
    "                output_csv: comp.OutputPath('CSV')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.\n",
    "        output_csv: A string containing path to processed data.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read in CSV\n",
    "    df = pd.read_csv(filepath_or_buffer=file_path)\n",
    "    \n",
    "    # Output to CSV\n",
    "    df.to_csv(output_csv, index = False, header = True)\n",
    "\n",
    "create_step_dp_scale_df = kfp.components.create_component_from_func(\n",
    "    func=scale_df,\n",
    "    output_component_file='component_dp_scale_df.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599379de",
   "metadata": {},
   "source": [
    "### Pipeline 02b\n",
    "\n",
    "1. download file\n",
    "2. data processing: \n",
    "    \n",
    "    - a) get dummies\n",
    "    - b) impute\n",
    "    - c) scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d0df011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl02b_web_download_dp_dummies_impute_scale(url):\n",
    "    \"\"\"Pipeline: Download data, data processing\n",
    "        1. download data\n",
    "        2. data processing\n",
    "            a. get dummies\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the CSV.\n",
    "                    e.g., 'https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff'\n",
    "    \"\"\"\n",
    "    web_downloader_task = download_file_component(url=url_for_data_download)\n",
    "    dp_get_dummies_task = create_step_dp_get_dummies(file=web_downloader_task.outputs['data'])\n",
    "    dp_impute_task = create_step_dp_impute_unknown(file=dp_get_dummies_task.outputs['output_csv_features'])\n",
    "    dp_scale_task = create_step_dp_scale_df(file=dp_impute_task.outputs['output_csv'])\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl02b_web_download_dp_dummies_impute_scale,\n",
    "    package_path='pl02b_web_download_dp_dummies_impute_scale.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af64f0",
   "metadata": {},
   "source": [
    "### Component: create model inputs - feature matrix, target, feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fc446a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(file_path_features: comp.InputPath('CSV'),\n",
    "                        file_path_target: comp.InputPath('CSV'),\n",
    "                            output_json: comp.OutputPath('JSON')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.        \n",
    "    \"\"\"\n",
    "    \n",
    "    import json \n",
    "    from json import JSONEncoder\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "    class NumpyArrayEncoder(JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return JSONEncoder.default(self, obj)\n",
    "    \n",
    "    \n",
    "    # Read in feature df and make X\n",
    "    \n",
    "    df_feature_matrix = pd.read_csv(filepath_or_buffer=file_path_features)\n",
    "    df_X = df_feature_matrix\n",
    "    X = np.array(df_X)\n",
    "    feature_names = np.array(df_feature_matrix.columns)\n",
    "    \n",
    "    \n",
    "    # Read in target df and make y\n",
    "    df_target = pd.read_csv(filepath_or_buffer=file_path_target) # one-column df\n",
    "    df_y = df_target # one-column df\n",
    "    y = np.array(df_y)\n",
    "    \n",
    "\n",
    "    # If needed, binarize one-vs-all\n",
    "    ## https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(y)\n",
    "    target_class_names = lb.classes_\n",
    "    y_binarized = lb.transform(y) # can be multi-dimensional array if there are more than 2 classes\n",
    "    \n",
    "    \n",
    "    # Train and test split - with labels for y\n",
    "    X_train, X_test, y_train_labels, y_test_labels = train_test_split(\n",
    "                                            X, y, test_size=0.25, random_state=42) \n",
    "    \n",
    "    # Train and test split - one-hot encoded y\n",
    "    X_train, X_test, y_train_one_hot, y_test_one_hot = train_test_split(\n",
    "                                            X, y_binarized, test_size=0.25, random_state=42) \n",
    "    \n",
    "    ## output: JSON\n",
    "    d_output = {\"X\": np.array(X),\n",
    "                \"y\": y,\n",
    "                \"X_train\": X_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_train_labels\": y_train_labels,\n",
    "                \"y_test_labels\": y_test_labels,\n",
    "                \"y_train_one_hot\": y_train_one_hot,\n",
    "                \"y_test_one_hot\": y_test_one_hot,\n",
    "                \"X_feature_names\": feature_names,\n",
    "                \"y_target_class_names\": target_class_names}\n",
    "\n",
    "    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n",
    "    \n",
    "    ### write json file\n",
    "    with open(output_json, 'w') as outfile:\n",
    "        outfile.write(json_string_output)\n",
    "\n",
    "create_step_create_model_inputs = kfp.components.create_component_from_func(\n",
    "    func=create_model_inputs,\n",
    "    output_component_file='component_dp_create_model_inputs.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abaf19c",
   "metadata": {},
   "source": [
    "### Pipeline 02c\n",
    "\n",
    "1. download file\n",
    "2. data processing: \n",
    "    \n",
    "    - a) get dummies\n",
    "    - b) impute\n",
    "    - c) scale\n",
    "    \n",
    "    - Then, create model inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cde9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl02c_web_download_dp(url):\n",
    "    \"\"\"Pipeline: Download data, data processing\n",
    "        1. download data\n",
    "        2. data processing\n",
    "            a. get dummies\n",
    "            b. imputation\n",
    "            c. scaling\n",
    "            d. create model inputs\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the CSV.\n",
    "                    e.g., 'https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff'\n",
    "    \"\"\"\n",
    "    web_downloader_task = download_file_component(url=url_for_data_download)\n",
    "    dp_get_dummies_task = create_step_dp_get_dummies(file=web_downloader_task.outputs['data'])\n",
    "    dp_impute_task = create_step_dp_impute_unknown(file=dp_get_dummies_task.outputs['output_csv_features'])\n",
    "    dp_scale_task = create_step_dp_scale_df(file=dp_impute_task.outputs['output_csv'])\n",
    "    dp_create_model_inputs_task = create_step_create_model_inputs(file_path_features=dp_scale_task.outputs['output_csv'],\n",
    "                                                                  file_path_target=dp_get_dummies_task.outputs['output_csv_target'])\n",
    "    \n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl02c_web_download_dp,\n",
    "    package_path='pl02c_web_download_dp.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634635f",
   "metadata": {},
   "source": [
    "## 3 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fce38e",
   "metadata": {},
   "source": [
    "### Component: feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a153084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(file_path: comp.InputPath('JSON'),\n",
    "                            output_json: comp.OutputPath('JSON')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.        \n",
    "    \"\"\"\n",
    "    \n",
    "    import json \n",
    "    from json import JSONEncoder\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    \n",
    "    class NumpyArrayEncoder(JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return JSONEncoder.default(self, obj)\n",
    "    \n",
    "    # read in JSON\n",
    "    with open(file_path, \"r\") as read_file:\n",
    "        json_input_data = json.load(read_file)\n",
    "    \n",
    "    X = np.array(json_input_data['X'])\n",
    "    y = np.array(json_input_data['y'])\n",
    "    X_feature_names = json_input_data['X_feature_names']\n",
    "    y_target_class_names = json_input_data['y_target_class_names']\n",
    "            \n",
    "        \n",
    "    # output: JSON\n",
    "    d_output = json_input_data\n",
    "\n",
    "    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n",
    "    \n",
    "    ## write json file\n",
    "    with open(output_json, 'w') as outfile:\n",
    "        outfile.write(json_string_output)\n",
    "\n",
    "create_step_feature_selection = kfp.components.create_component_from_func(\n",
    "    func=feature_selection,\n",
    "    output_component_file='component_feature_selection.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                        'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ff11d",
   "metadata": {},
   "source": [
    "### Pipeline 03\n",
    "\n",
    "1. download file\n",
    "2. data processing: \n",
    "    \n",
    "    - a) get dummies\n",
    "    - b) impute\n",
    "    - c) scale\n",
    "    - d) create JSON object for inputs\n",
    "3. feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffb6bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl03_webdl_dp_fs(url):\n",
    "    \"\"\"Pipeline: Download data, data processing\n",
    "        1. download data\n",
    "        2. data processing\n",
    "            a. get dummies\n",
    "            b. imputation\n",
    "            c. scaling\n",
    "            d. create model inputs\n",
    "        3. feature selection\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the CSV.\n",
    "                    e.g., 'https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff'\n",
    "    \"\"\"\n",
    "    web_downloader_task = download_file_component(url=url_for_data_download)\n",
    "    dp_get_dummies_task = create_step_dp_get_dummies(file=web_downloader_task.outputs['data'])\n",
    "    dp_impute_task = create_step_dp_impute_unknown(file=dp_get_dummies_task.outputs['output_csv_features'])\n",
    "    dp_scale_task = create_step_dp_scale_df(file=dp_impute_task.outputs['output_csv'])\n",
    "    dp_create_model_inputs_task = create_step_create_model_inputs(file_path_features=dp_scale_task.outputs['output_csv'],\n",
    "                                                                  file_path_target=dp_get_dummies_task.outputs['output_csv_target'])\n",
    "    dp_feature_selection_task = create_step_feature_selection(file=dp_create_model_inputs_task.outputs['output_json'])\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl03_webdl_dp_fs,\n",
    "    package_path='pl03_webdl_dp_fs.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7be3b",
   "metadata": {},
   "source": [
    "## 4 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f06aaf7",
   "metadata": {},
   "source": [
    "### Component: decision tree model\n",
    "\n",
    "* source: https://towardsdatascience.com/kubeflow-pipelines-how-to-build-your-first-kubeflow-pipeline-from-scratch-2424227f7e5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77d38a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree model: https://towardsdatascience.com/kubeflow-pipelines-how-to-build-your-first-kubeflow-pipeline-from-scratch-2424227f7e5\n",
    "\n",
    "\n",
    "def decision_tree(file_path: comp.InputPath('JSON'),\n",
    "                            output_json: comp.OutputPath('JSON')):\n",
    "    \"\"\"Impute unknown values (nan).\n",
    "        Input: CSV.\n",
    "        Output: CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to input data.        \n",
    "    \"\"\"\n",
    "    \n",
    "    import json \n",
    "    from json import JSONEncoder\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    \n",
    "    class NumpyArrayEncoder(JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return JSONEncoder.default(self, obj)\n",
    "    \n",
    "    # read in JSON\n",
    "    with open(file_path, \"r\") as read_file:\n",
    "        json_input_data = json.load(read_file)\n",
    "    \n",
    "    X_train = np.array(json_input_data['X_train']) # JSON has list of lists - cast to np array\n",
    "    y_train_labels = np.array(json_input_data['y_train_labels'])\n",
    "    y_train_one_hot = np.array(json_input_data['y_train_one_hot'])\n",
    "    X_test = np.array(json_input_data['X_test'])\n",
    "    y_test_labels = np.array(json_input_data['y_test_labels'])\n",
    "    y_test_one_hot = np.array(json_input_data['y_test_one_hot'])\n",
    "    X_feature_names = json_input_data['X_feature_names']\n",
    "    y_target_class_names = json_input_data['y_target_class_names']\n",
    "            \n",
    "\n",
    "    # Initialize and train the model - labels\n",
    "    model_labels = DecisionTreeClassifier(max_depth=3)\n",
    "    model_labels.fit(X_train, y_train_labels)        \n",
    "\n",
    "    # Initialize and train the model - one-hot\n",
    "    model_one_hot = DecisionTreeClassifier(max_depth=3)\n",
    "    model_one_hot.fit(X_train, y_train_one_hot)   \n",
    "    \n",
    "    # Get predictions - labels (use for confusion matrix computation)\n",
    "    y_pred_labels = model_labels.predict(X_test)\n",
    "    \n",
    "    # Get predictions - one-hot (use for AUC computation)\n",
    "    y_pred_one_hot = model_one_hot.predict(X_test)\n",
    "    \n",
    "    # Get accuracy - use labels\n",
    "    accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "    \n",
    "    # Get accuracy - use one-hot\n",
    "\n",
    "    # Confusion matrix - use labels\n",
    "    cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "    \n",
    "    \n",
    "    # AUC score - use one-hot\n",
    "    sum_y_test_axis_0 = np.sum(y_test_one_hot, axis=0)\n",
    "    col_idx_y_test_nnz = np.where(sum_y_test_axis_0 > 0)[0]\n",
    "    y_test_classes_more_than_one_label = y_test_one_hot[:, col_idx_y_test_nnz]\n",
    "    y_pred_classes_more_than_one_label = y_pred_one_hot[:, col_idx_y_test_nnz]\n",
    "    auc_test = roc_auc_score(y_test_classes_more_than_one_label, y_pred_classes_more_than_one_label, multi_class='ovr') # AUC score true y_test vs predicted\n",
    "\n",
    "\n",
    "    # output: JSON\n",
    "    d_output = json_input_data\n",
    "    ## add in model results\n",
    "    d_output['model_results'] = {}\n",
    "#    d_output['model_results']['model'] = model\n",
    "    d_output['model_results']['y_pred_labels'] = y_pred_labels\n",
    "    d_output['model_results']['y_pred_one_hot'] = y_pred_one_hot\n",
    "    d_output['model_results']['accuracy'] = accuracy\n",
    "    d_output['model_results']['auc_test'] = auc_test\n",
    "\n",
    "    json_string_output = json.dumps(d_output, cls=NumpyArrayEncoder)\n",
    "    \n",
    "    ## write json file\n",
    "    with open(output_json, 'w') as outfile:\n",
    "        outfile.write(json_string_output)\n",
    "\n",
    "create_step_classif_decision_tree = kfp.components.create_component_from_func(\n",
    "    func=decision_tree,\n",
    "    output_component_file='component_classif_decision_tree.yaml', # save the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4',\n",
    "                         'scikit-learn==1.0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148dba37",
   "metadata": {},
   "source": [
    "### Pipeline 04\n",
    "\n",
    "1. download file\n",
    "2. data processing: \n",
    "    \n",
    "    - a) get dummies\n",
    "    - b) impute\n",
    "    - c) scale\n",
    "    - d) create JSON object for inputs\n",
    "3. feature selection\n",
    "4. classification\n",
    "    - decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a5dd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl04_webdl_dp_fs_classif_decisiontree(url):\n",
    "    \"\"\"Pipeline: Download data, data processing\n",
    "        1. download data\n",
    "        2. data processing\n",
    "            a. get dummies\n",
    "            b. imputation\n",
    "            c. scaling\n",
    "            d. create model inputs\n",
    "        3. feature selection\n",
    "        4. classification\n",
    "            a. decision tree\n",
    "    \n",
    "    Args:\n",
    "        file_path: A string containing path to the CSV.\n",
    "                    e.g., 'https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff'\n",
    "    \"\"\"\n",
    "    web_downloader_task = download_file_component(url=url_for_data_download)\n",
    "    dp_get_dummies_task = create_step_dp_get_dummies(file=web_downloader_task.outputs['data'])\n",
    "    dp_impute_task = create_step_dp_impute_unknown(file=dp_get_dummies_task.outputs['output_csv_features'])\n",
    "    dp_scale_task = create_step_dp_scale_df(file=dp_impute_task.outputs['output_csv'])\n",
    "    dp_create_model_inputs_task = create_step_create_model_inputs(file_path_features=dp_scale_task.outputs['output_csv'],\n",
    "                                                                  file_path_target=dp_get_dummies_task.outputs['output_csv_target'])\n",
    "    dp_feature_selection_task = create_step_feature_selection(file=dp_create_model_inputs_task.outputs['output_json'])                                                                  \n",
    "    dp_classif_decision_tree_task = create_step_classif_decision_tree(file=dp_feature_selection_task.outputs['output_json'])\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pl04_webdl_dp_fs_classif_decisiontree,\n",
    "    package_path='pl04_webdl_dp_fs_classif_dectree.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f1275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca5960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
