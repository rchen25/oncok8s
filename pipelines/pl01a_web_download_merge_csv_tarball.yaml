apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: pl01a-web-download-merge-csv-tarball-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.11, pipelines.kubeflow.org/pipeline_compilation_time: '2022-03-17T13:07:14.281453',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Pipeline: Download and
      merge multiple CSV files inside tarball.", "inputs": [{"name": "url"}], "name":
      "Pl01a web download merge csv tarball"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.11}
spec:
  entrypoint: pl01a-web-download-merge-csv-tarball
  templates:
  - name: download-data-kfp-sdk-v2
    container:
      args: []
      command:
      - sh
      - -exc
      - |
        url="$0"
        output_path="$1"
        curl_options="$2"
        mkdir -p "$(dirname "$output_path")"
        curl --get "$url" --output "$output_path" $curl_options
      - https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff
      - /tmp/outputs/Data/data
      - --location
      image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
    outputs:
      artifacts:
      - {name: download-data-kfp-sdk-v2-Data, path: /tmp/outputs/Data/data}
    metadata:
      annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml',
        pipelines.kubeflow.org/component_spec: '{"description": "Downloads data from
          the specified URL. (Updated for KFP SDK v2.)", "implementation": {"container":
          {"command": ["sh", "-exc", "url=\"$0\"\noutput_path=\"$1\"\ncurl_options=\"$2\"\nmkdir
          -p \"$(dirname \"$output_path\")\"\ncurl --get \"$url\" --output \"$output_path\"
          $curl_options\n", {"inputValue": "Url"}, {"outputPath": "Data"}, {"inputValue":
          "curl options"}], "image": "byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342"}},
          "inputs": [{"name": "Url", "type": "String"}, {"default": "--location",
          "description": "Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html",
          "name": "curl options", "type": "String"}], "metadata": {"annotations":
          {"author": "Alexey Volkov <alexey.volkov@ark-kun.com>", "canonical_location":
          "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml"}},
          "name": "Download data (KFP SDK v2)", "outputs": [{"name": "Data"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "3eafccb9f368ff7282d1670d655c2e1b3ee8fdd241dd6b3d0e1d9de4b6da7c9b", "url":
          "./component-sdk-v2.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"Url":
          "https://www.openml.org/data/get_csv/3594/dataset_113_primary-tumor.arff",
          "curl options": "--location"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: merge-csv
    container:
      args: [--file, /tmp/inputs/file/data, --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas==1.1.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas==1.1.4' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef merge_csv(file_path,\n              output_csv):\n    \"\"\"Merge multiple\
        \ CSV files inside tarball.\n\n    Args:\n        file_path: A string containing\
        \ path to the YAML file.\n                    e.g., 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n\
        \    \"\"\"\n    import glob\n    import pandas as pd\n    import tarfile\n\
        \n    tarfile.open(name=file_path, mode=\"r|gz\").extractall('data')\n   \
        \ df = pd.concat(\n        [pd.read_csv(csv_file, header=None) \n        \
        \    for csv_file in glob.glob('data/*.csv')])\n    df.to_csv(output_csv,\
        \ index=False, header=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Merge\
        \ csv', description='Merge multiple CSV files inside tarball.')\n_parser.add_argument(\"\
        --file\", dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--output-csv\", dest=\"output_csv\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = merge_csv(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: download-data-kfp-sdk-v2-Data, path: /tmp/inputs/file/data}
    outputs:
      artifacts:
      - {name: merge-csv-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.11
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Merge
          multiple CSV files inside tarball.", "implementation": {"container": {"args":
          ["--file", {"inputPath": "file"}, "--output-csv", {"outputPath": "output_csv"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pandas==1.1.4'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas==1.1.4''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef merge_csv(file_path,\n              output_csv):\n    \"\"\"Merge
          multiple CSV files inside tarball.\n\n    Args:\n        file_path: A string
          containing path to the YAML file.\n                    e.g., ''https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz''\n    \"\"\"\n    import
          glob\n    import pandas as pd\n    import tarfile\n\n    tarfile.open(name=file_path,
          mode=\"r|gz\").extractall(''data'')\n    df = pd.concat(\n        [pd.read_csv(csv_file,
          header=None) \n            for csv_file in glob.glob(''data/*.csv'')])\n    df.to_csv(output_csv,
          index=False, header=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Merge
          csv'', description=''Merge multiple CSV files inside tarball.'')\n_parser.add_argument(\"--file\",
          dest=\"file_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = merge_csv(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"description":
          "A string containing path to the YAML file.\ne.g., ''https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz''",
          "name": "file", "type": "Tarball"}], "name": "Merge csv", "outputs": [{"name":
          "output_csv", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: pl01a-web-download-merge-csv-tarball
    dag:
      tasks:
      - {name: download-data-kfp-sdk-v2, template: download-data-kfp-sdk-v2}
      - name: merge-csv
        template: merge-csv
        dependencies: [download-data-kfp-sdk-v2]
        arguments:
          artifacts:
          - {name: download-data-kfp-sdk-v2-Data, from: '{{tasks.download-data-kfp-sdk-v2.outputs.artifacts.download-data-kfp-sdk-v2-Data}}'}
  arguments:
    parameters:
    - {name: url}
  serviceAccountName: pipeline-runner
